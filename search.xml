<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>6.824Lecture1-MapReduce</title>
    <url>/2021/12/11/6-824Lecture1/</url>
    <content><![CDATA[<h1 id="Backgorund"><a href="#Backgorund" class="headerlink" title="Backgorund"></a>Backgorund</h1><ul>
<li>big data</li>
<li>lots of kv data stuctures, like inverted index</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Abstract-view"><a href="#Abstract-view" class="headerlink" title="Abstract view"></a>Abstract view</h2><img src="/2021/12/11/6-824Lecture1/workflow.png" class="" title="workflow">
<a id="more"></a> 
<ul>
<li><strong>split</strong> files from GFS to disks</li>
<li>master分配worker执行map任务，生成k,v值，存入disk，map回传disk地址给master</li>
<li>master传递地址给reduce worker，reduce worker使用RPC读disk数据</li>
<li><strong>sort by key</strong>, 并将所有values聚合</li>
<li>reduce</li>
<li>master返回reduce结果给GFS</li>
</ul>
<h2 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault tolerance"></a>Fault tolerance</h2><ul>
<li>fail -&gt; rerun: worker rerun不会产生其他问题</li>
<li>worker fault: Master 周期性的 ping 每个 Worker，如果指定时间内没回应就是挂了。将这个 Worker 标记为失效，分配给这个失效 Worker 的任务将被重新分配给其他 Worker；</li>
<li>master fault: 中止整个 MapReduce 运算，重新执行。</li>
</ul>
<h1 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h1><ul>
<li>bandwidth: 尽量保持worker和其执行的文件在同一台机器上，会相近的机器</li>
<li>slow workers: 吊起执行快的作为backup worker，取最先执行完的worker</li>
</ul>
<h1 id="Cons"><a href="#Cons" class="headerlink" title="Cons"></a>Cons</h1><ul>
<li>实时性</li>
<li>复杂需求时需要大量相互依赖的mr逻辑 -》难开发，难调试</li>
</ul>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf">paper</a></li>
<li><a href="https://www.youtube.com/watch?v=WtZ7pcRSkOA">video</a></li>
<li><a href="https://mp.weixin.qq.com/s/I0PBo_O8sl18O5cgMvQPYA">note</a></li>
<li><a href="https://www.the-paper-trail.org/post/2014-06-25-the-elephant-was-a-trojan-horse-on-the-death-of-map-reduce-at-google/">the reason why google gives up MapReduce</a></li>
</ul>
<p><strong>作为一种范式，而非产品？</strong></p>
]]></content>
      <categories>
        <category>Learning</category>
        <category>6.824</category>
      </categories>
      <tags>
        <tag>6.824</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>6.824Lecture2-RPC&amp;Threads</title>
    <url>/2021/12/11/6-824Lecture2/</url>
    <content><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Why-Threads"><a href="#Why-Threads" class="headerlink" title="Why Threads?"></a>Why Threads?</h2><ul>
<li>I/O concurrency</li>
<li>multi core parallelism</li>
<li>convenience for, like routine</li>
</ul>
<a id="more"></a>
<h2 id="Threads-Changes"><a href="#Threads-Changes" class="headerlink" title="Threads Changes"></a>Threads Changes</h2><ul>
<li>race conditions<ul>
<li>avoid share memory</li>
<li>use locks: 并行变串行</li>
</ul>
</li>
<li>coordinations<ul>
<li>channels or condition variables</li>
</ul>
</li>
<li>dead lock</li>
</ul>
<h1 id="Go-Methods"><a href="#Go-Methods" class="headerlink" title="Go Methods"></a>Go Methods</h1><h2 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h2><ul>
<li>用来在并发中同步变量</li>
<li>pipeline with datatype, need be created, like: ch = make(chan int)</li>
<li>receivers &amp; sender.默认情况下，发送和接收操作在另一端准备好之前都会阻塞。这使得 Go 程可以在没有显式的锁或竞态变量的情况下进行同步<ul>
<li>sender: 通过close关闭信道表示没有要发送的值. <strong>向一个已经关闭的信道发送数据会引发程序panic</strong></li>
<li>receiver: v, ok := &lt;-ch, ok=false when channel is empty and is closed</li>
</ul>
</li>
</ul>
<h2 id="sync-Mutex"><a href="#sync-Mutex" class="headerlink" title="sync.Mutex"></a>sync.Mutex</h2><ul>
<li>互斥锁，代码段前后调用Lock() &amp; Unlock()实现某段代码的互斥执行，用defer保证互斥锁一定被解锁。<ul>
<li>defer func()：func会在包含defer语句的函数返回时再执行</li>
</ul>
</li>
</ul>
<h2 id="sync-Cond"><a href="#sync-Cond" class="headerlink" title="sync.Cond"></a>sync.Cond</h2><ul>
<li>Wait()/Broadcast()/Singal()</li>
</ul>
<h1 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h1><p><em>remote procedure call</em></p>
<ul>
<li>goal: rpc ≈ pc</li>
<li>pros: stub完成数据转换和解析、打开/关闭连接等细节</li>
</ul>
<h2 id="workflow"><a href="#workflow" class="headerlink" title="workflow"></a>workflow</h2><p><img src="https://pic1.zhimg.com/45366c44f775abfd0ac3b43bccc1abc3_r.jpg?source=1940ef5c" alt="workflow"></p>
<ol>
<li>客户端调用 client stub，并将调用参数 push 到栈（stack）中，这个调用是在本地的</li>
<li>client stub 将这些参数包装，并通过系统调用发送到服务端机器。打包的过程叫 marshalling（常见方式：XML、JSON、二进制编码）。</li>
<li>客户端操作系统将消息传给传输层，传输层发送信息至服务端；</li>
<li>服务端的传输层将消息传递给 server stub</li>
<li>server stub 解析信息。该过程叫 unmarshalling。</li>
<li>server stub 调用程序，并通过类似的方式返回给客户端。</li>
<li>客户端拿到数据解析后，将执行结果返回给调用者。</li>
</ol>
<h2 id="RPC-Semantics-under-failures"><a href="#RPC-Semantics-under-failures" class="headerlink" title="RPC Semantics under failures"></a>RPC Semantics under failures</h2><p><strong>server不知道client的具体情况，导致failure时难以处理</strong></p>
<ul>
<li>at-least-once: client不断重试直到ack成功。适用于幂等操作<ul>
<li>幂等操作：幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同</li>
</ul>
</li>
<li>at-most-once: <strong>最常见的</strong>，只执行0/1次。通过过滤重复项来实现。</li>
<li>exactly-once: hard</li>
</ul>
<p>go rpc: at-most-once, 由client手动决定是否重试 </p>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwODA2NjIxOA==&amp;mid=2247484193&amp;idx=1&amp;sn=693e0ff4bfcc6e02dea10ed9d639b41b&amp;chksm=970980e4a07e09f2647de63ed0bf3be98d9032a3797033af3872c692d2373f98627a63f30e22&amp;scene=178&amp;cur_album_id=1751707148520112128#rd">note</a></li>
<li><a href="https://www.bilibili.com/video/BV1e5411E7RM?p=2&amp;spm_id_from=pageDriver">video</a></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>6.824</category>
      </categories>
      <tags>
        <tag>6.824</tag>
        <tag>RPC</tag>
        <tag>Thread</tag>
      </tags>
  </entry>
  <entry>
    <title>C++-内存管理</title>
    <url>/2021/12/10/C++-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<h2 id="C-的内存分配方式"><a href="#C-的内存分配方式" class="headerlink" title="C++的内存分配方式"></a>C++的内存分配方式</h2><ul>
<li>全局/静态变量区</li>
<li>常量存储区</li>
<li>栈：在局部变量和函数参数在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是栈的内存容量有限。</li>
<li>堆：由new分配的内存块，需要手动通过delete来释放。如果没有delete，那么在程序结束后，操作系统会自动回收。</li>
<li>自由存储区：由malloc等分配的内存块，和堆是十分相似，不过它是用free来结束自己的生命的。</li>
</ul>
<a id="more"></a>
<h3 id="堆和栈"><a href="#堆和栈" class="headerlink" title="堆和栈"></a>堆和栈</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123; <span class="keyword">int</span>* p = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>]; &#125;</span><br><span class="line"><span class="keyword">delete</span> []p;</span><br></pre></td></tr></table></figure>
<ul>
<li>new了一个在堆上的数组，并在栈上创建一个指针p；用delete []p释放堆上的内存</li>
<li>未delete的话会造成内存泄漏。delete后的指针为野指针，</li>
<li><strong>new和delete时分别执行构造函数和析构函数</strong></li>
<li></li>
</ul>
<h4 id="管理方式-amp-申请后系统的响应"><a href="#管理方式-amp-申请后系统的响应" class="headerlink" title="管理方式&amp;申请后系统的响应"></a>管理方式&amp;申请后系统的响应</h4><ul>
<li>栈：由系统管理。不用手动申请，只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。  </li>
<li>堆：要new。<ul>
<li>首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序，另外，对于大多数系统，会在这块内存空间中的首地址处记录本次分配的大小，这样，代码中的delete语句才能正确的释放本内存空间。另外，由于找到的堆结点的大小不一定正好等于申请的大小，系统会自动的将多余的那部分重新放入空闲链表中。   </li>
</ul>
</li>
</ul>
<h4 id="空间大小"><a href="#空间大小" class="headerlink" title="空间大小"></a>空间大小</h4><ul>
<li>栈：在Windows下,<strong>栈是向低地址扩展</strong>的数据结构，是一块连续的内存的区域。这句话的意思是栈顶的地址和栈的最大容量是系统预先规定好的，在WINDOWS下，栈的大小是2M（或1M，总之是一个编译时就确定的常数），如果申请的空间超过栈的剩余空间时，将提示overflow。因此，能从栈获得的空间较小。   </li>
<li>堆：<strong>堆是向高地址扩展</strong>的数据结构，是不连续的内存区域。这是由于系统是用链表来存储的空闲内存地址的，自然是不连续的，而链表的遍历方向是由低地址向高地址。堆的大小受限于计算机系统中有效的虚拟内存。由此可见，堆获得的空间比较灵活，也比较大。   </li>
</ul>
<h4 id="申请效率的比较："><a href="#申请效率的比较：" class="headerlink" title="申请效率的比较："></a>申请效率的比较：</h4><ul>
<li>栈：机器系统提供的数据结构，计算机会在底层对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高。</li>
<li>堆：则是C/C++函数库提供的，它的机制是很复杂的，例如为了分配一块内存，库函数会按照一定的算法（具体的算法可以参考数据结构/操作系统）在堆内存中搜索可用的足够大小的空间，如果没有足够大小的空间（可能是由于内存碎片太多），就有可能调用系统功能去增加程序数据段的内存空间，这样就有机会分到足够大小的内存，然后进行返回。显然，堆的效率比栈要低得多。</li>
</ul>
<h4 id="堆和栈中的存储内容"><a href="#堆和栈中的存储内容" class="headerlink" title="堆和栈中的存储内容"></a>堆和栈中的存储内容</h4><ul>
<li>栈：在函数调用时，第一个进栈的是主函数中后的下一条指令（函数调用语句的下一条可执行语句）的地址，然后是函数的各个参数，在大多数的C编译器中，参数是由右往左入栈的，然后是函数中的局部变量。注意静态变量是不入栈的。当本次函数调用结束后，局部变量先出栈，然后是参数，最后栈顶指针指向开始存的地址，也就是主函数中的下一条指令，程序由该点继续运行。   </li>
<li>堆：一般是在堆的头部用一个字节存放堆的大小。堆中的具体内容由程序员安排。   </li>
</ul>
<h2 id="函数传递方式"><a href="#函数传递方式" class="headerlink" title="函数传递方式"></a>函数传递方式</h2><ul>
<li>值传递：调用了一个复制构造函数，相当于创建了一个拷贝，会造成更多的内存占用</li>
<li>引用传递：传递的是原参数的别名。节省内存，可对实参进行修改。</li>
<li><p>指针传递：形参是指向实参的指针，对形参指针的操作，会影响到实参。</p>
</li>
<li><p>指针传递和引用传递一般适用于：</p>
<ul>
<li>函数内部修改参数并且希望改动影响调用者。</li>
<li>当一个函数实际需要返回多个值，而只能显式返回一个值时，可以将另外需要返回的变量以指针/引用传递。</li>
</ul>
</li>
</ul>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul>
<li>引用为别名，不能有null引用</li>
<li>引用在创建的同时必须对其初始化</li>
<li>引用在初始化后，无法改变引用关系</li>
</ul>
<h3 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h3><ul>
<li>本质上还是值传递，传递的是地址</li>
</ul>
<p>&lt;未完&gt;</p>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>C++-自建数组及STL中的array&amp;vector</title>
    <url>/2021/12/10/C++-%E8%87%AA%E5%BB%BA%E6%95%B0%E7%BB%84%E5%8F%8ASTL%E4%B8%AD%E7%9A%84array&amp;vector/</url>
    <content><![CDATA[<h2 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h2><ul>
<li>三者皆使用连续内存，且array和vector的底层由自建数组实现</li>
<li>都可以用下标访问</li>
</ul>
<a id="more"></a>
<h2 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h2><ol>
<li>vector属于变长容器，即可以根据数据的插入删除重新构建容器容量；但array和数组属于定长容器。</li>
<li>vector提供了可以动态插入和删除元素的机制，而array和数组则无法做到，或者说array和数组需要完成该功能则需要自己实现完成。</li>
<li>由于vector的动态内存变化的机制，<strong>在插入和删除时，需要考虑迭代的是否失效的问题</strong>，vector容器增长时，不是在原有内存空间中添加，而是重新申请+分配内存。</li>
<li>vector和array提供了更好的数据访问机制，即可以使用front和back以及at访问方式，使得访问更加安全。而数组只能通过下标访问，在程序的设计过程中，更容易引发访问错误。</li>
<li>vector和array提供了更好的遍历机制，有正向迭代器和反向迭代器两种</li>
<li>vector和array提供了size和判空的获取机制，而数组只能通过遍历或者通过额外的变量记录数组的size。</li>
<li>vector和array提供了两个容器对象的内容交换，即swap的机制，而数组对于交换只能通过遍历的方式，逐个元素交换的方式使用</li>
<li>array提供了初始化所有成员的方法fill</li>
</ol>
<h2 id="自建数组"><a href="#自建数组" class="headerlink" title="自建数组"></a>自建数组</h2><h3 id="静态数组-amp-动态数组"><a href="#静态数组-amp-动态数组" class="headerlink" title="静态数组&amp;动态数组"></a>静态数组&amp;动态数组</h3><h4 id="静态数组"><a href="#静态数组" class="headerlink" title="静态数组"></a>静态数组</h4><ul>
<li><strong>在栈中</strong></li>
<li>在编译期间在栈中分配好内存的数组，在运行期间不能改变存储空间，运行后由<strong>系统自动释放</strong>。</li>
<li>必须用常量指定数组大小</li>
<li>数组名为第一个数组元素</li>
</ul>
<h4 id="动态数组"><a href="#动态数组" class="headerlink" title="动态数组"></a>动态数组</h4><ul>
<li>程序运行时才分配内存的数组</li>
<li><strong>在堆中</strong></li>
<li>数组名为指向第一个数组元素的指针</li>
</ul>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ul>
<li>int a[3]; 局部作用域中，只做了声明，未初始化，其值未定</li>
<li>int a[3]; 静态数组以及直接声明在某个namespace中（函数外部），默认初始化为全0</li>
<li>int a[3] = {}; 初始化为0</li>
<li>int a[3] = {1}; 初始化为{1, 0, 0}</li>
<li>int a[] = {1,2,3,4}; 声明大小为4的数组，并初始化为{1,2,3,4}</li>
<li>int a[] {1,2,3,4}; <strong>通用初始化</strong>方法，声明和初始化程序之间不用等号</li>
<li>int* a = new int[10]; //new 分配了一个大小为10的未初始化的int数组，并返回指向该数组第一个元素的指针，此指针初始化了指针a<ul>
<li>a = {4, -3, 5, -2, -1, 2, 6, -2, 3}; // 错误，注意这种用大括号的数组赋值只能用在声明时，此处已经不是声明，所以出错。</li>
<li>int *a = new int[10] ();  // 默认初始化，每个元素初始化为0,括号内不能写其他值，只能初始化为0</li>
<li>int* a = new int[n];// t</li>
<li>string* Dynamic_Arr4 = new string[size]{“aa”, “bb”,”cc”, “dd”, string(2, ‘e’) };      //显式的初始化</li>
<li>delete [ ] Dynamic_Arr4；//动态数组的释放</li>
</ul>
</li>
<li>维度为变量时，不能在声明时同时初始化，即int a[b]={1};是不合法的。 </li>
<li>多维数组为1维数组的特殊形式，定义多维数组时，如果不对它进行初始化，必须标明每个维度的大小；如果进行了显式的初始化，可以不标明最高维度的大小，（也就是第一个维度，当第一个维度不标明大小，则不需进行初始化）</li>
</ul>
<h2 id="array"><a href="#array" class="headerlink" title="array"></a>array</h2><ul>
<li><a href="https://www.cplusplus.com/reference/array/array/">ref</a></li>
<li>大小固定</li>
</ul>
<h2 id="vector"><a href="#vector" class="headerlink" title="vector"></a>vector</h2><ul>
<li><a href="https://www.cplusplus.com/reference/vector/vector/">ref</a></li>
<li>动态数组</li>
</ul>
<h3 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化"></a>初始化</h3><ul>
<li>构造函数<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>():创建一个空<span class="built_in">vector</span></span><br><span class="line"><span class="built_in">vector</span>(<span class="keyword">int</span> nSize):创建一个<span class="built_in">vector</span>,元素个数为nSize</span><br><span class="line"><span class="built_in">vector</span>(<span class="keyword">int</span> nSize,<span class="keyword">const</span> t&amp; t):创建一个<span class="built_in">vector</span>，元素个数为nSize,且值均为t</span><br><span class="line"><span class="built_in">vector</span>(<span class="keyword">const</span> <span class="built_in">vector</span>&amp;):复制构造函数</span><br><span class="line"><span class="built_in">vector</span>(begin,end):复制[begin,end)区间内另一个数组的元素到<span class="built_in">vector</span>中</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; first;                                <span class="comment">// empty vector of ints</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">second</span> <span class="params">(<span class="number">4</span>,<span class="number">100</span>)</span></span>;                       <span class="comment">// four ints with value 100</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">third</span> <span class="params">(second.begin(),second.end())</span></span>;  <span class="comment">// iterating through second</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">fourth</span> <span class="params">(third)</span></span>;                       <span class="comment">// a copy of third</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// the iterator constructor can also be used to construct from arrays:</span></span><br><span class="line"><span class="keyword">int</span> myints[] = &#123;<span class="number">16</span>,<span class="number">2</span>,<span class="number">77</span>,<span class="number">29</span>&#125;;</span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">fifth</span> <span class="params">(myints, myints + <span class="keyword">sizeof</span>(myints) / <span class="keyword">sizeof</span>(<span class="keyword">int</span>) )</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>二维vector若要使用A[i].push_back(3)等方式初始化，则应事先定义好第一维的大小，例如<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">ans</span><span class="params">(<span class="number">5</span>)</span></span>;</span><br><span class="line">ans[<span class="number">3</span>].push_back(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="增加"><a href="#增加" class="headerlink" title="增加"></a>增加</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">void push_back(const T&amp; x):向量尾部增加一个元素X</span><br><span class="line">iterator insert(iterator it,const T&amp; x):向量中迭代器指向元素前增加一个元素x</span><br><span class="line">iterator insert(iterator it,int n,const T&amp; x):向量中迭代器指向元素前增加n个相同的元素x</span><br><span class="line">iterator insert(iterator it,const_iterator first,const_iterator last):向量中迭代器指向元素前插入另一个相同类型向量的[first,last)间的数据</span><br></pre></td></tr></table></figure>
<h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">iterator erase(iterator it):删除向量中迭代器指向元素</span><br><span class="line">iterator erase(iterator first,iterator last):删除向量中[first,last)中元素</span><br><span class="line">void pop_back():删除向量中最后一个元素</span><br><span class="line">void clear():清空向量中所有元素</span><br></pre></td></tr></table></figure>
<h3 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">reference at(int pos):返回pos位置元素的引用</span><br><span class="line">reference front():返回首元素的引用</span><br><span class="line">reference back():返回尾元素的引用</span><br><span class="line">iterator begin():返回向量头指针，指向第一个元素</span><br><span class="line">iterator end():返回向量尾指针，指向向量最后一个元素的下一个位置</span><br><span class="line">reverse_iterator rbegin():反向迭代器，指向最后一个元素</span><br><span class="line">reverse_iterator rend():反向迭代器，指向第一个元素之前的位置</span><br></pre></td></tr></table></figure>
<h3 id="判断"><a href="#判断" class="headerlink" title="判断"></a>判断</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">bool empty() const:判断向量是否为空，若为空，则向量中无元素</span><br></pre></td></tr></table></figure>
<h3 id="大小"><a href="#大小" class="headerlink" title="大小"></a>大小</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">int size() const:返回向量中元素的个数</span><br><span class="line">int capacity() const:返回当前向量所能容纳的最大元素值</span><br><span class="line">int max_size() const:返回最大可允许的vector元素数量值</span><br></pre></td></tr></table></figure>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">void swap(vector&amp;):交换两个同类型向量的数据</span><br><span class="line">void assign(int n,const T&amp; x):设置向量中第n个元素的值为x</span><br><span class="line">void assign(const_iterator first,const_iterator last):向量中[first,last)中元素设置成当前向量元素</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec2-Traditional Methods for ML on Graphs</title>
    <url>/2021/12/12/CS224WLec2/</url>
    <content><![CDATA[<p><a href="http://web.stanford.edu/class/cs224w/slides/02-tradition-ml.pdf">ppt</a></p>
<h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h1><p>design features for nodes/link/graphs —&gt; obtain features for all data —&gt; train an ML model —&gt; apply the model<br><a id="more"></a></p>
<h1 id="Feature-design"><a href="#Feature-design" class="headerlink" title="Feature design"></a>Feature design</h1><ul>
<li><em>focus on undirected graphs</em></li>
</ul>
<h2 id="Node-Features"><a href="#Node-Features" class="headerlink" title="Node Features"></a>Node Features</h2><h3 id="centrality中心性"><a href="#centrality中心性" class="headerlink" title="centrality中心性"></a>centrality中心性</h3><ul>
<li><strong>Degree counts #(edges) that a node touches</strong></li>
<li>degree只考虑neibors数量，不考虑neibor的不同重要性</li>
</ul>
<h4 id="Eigenvector-centrality"><a href="#Eigenvector-centrality" class="headerlink" title="Eigenvector centrality"></a>Eigenvector centrality</h4><ul>
<li>定义：节点的重要性由邻居节点的重要性决定。节点v的centrality是邻居centrality的加总，N(v)为v的neibors集合<script type="math/tex; mode=display">
c_{v}=\frac{1}{\lambda} \sum_{u \in N(v)} c_{u}</script></li>
</ul>
<p>可以将其写为矩阵形式，得到 $ \lambda c=A c $ ，A为邻接矩阵，$ \lambda{max} $ 总为正且唯一，因此可以将其对应的$ c_{max} $作为eigenvector</p>
<h4 id="betweenness-centrality"><a href="#betweenness-centrality" class="headerlink" title="betweenness centrality"></a>betweenness centrality</h4><ul>
<li>如果一个节点处在很多节点对的最短路径上，那么这个节点是重要的<script type="math/tex; mode=display">
c_{v}=\sum_{s \neq v \neq t} \frac{\#(\text { shortest paths betwen } s \text { and } t \text { that contain } v)}{\#(\text { shortest paths between } s \text { and } t)}</script></li>
</ul>
<h4 id="closeness-centrality"><a href="#closeness-centrality" class="headerlink" title="closeness centrality"></a>closeness centrality</h4><ul>
<li>一个节点距其他节点之间距离最短，那么认为这个节点是重要的。分母：该节点与其他节点的最短距离之和。<script type="math/tex; mode=display">
c_{v}=\frac{1}{\sum_{u \neq v} \text { shortest path length between } u \text { and } v}</script></li>
</ul>
<h3 id="clustering-coefficient"><a href="#clustering-coefficient" class="headerlink" title="clustering coefficient"></a>clustering coefficient</h3><ul>
<li><strong>Clustering coefficient counts #(triangles) that a node touches.</strong></li>
<li>节点的<strong>neighbors</strong>两两连接的情况 —》neighbor总跟该节点连接着，neibor两两连接就能构成三角形 —》<strong>反映了该节点和其neighbors是否能聚为一类的情况</strong><script type="math/tex; mode=display">
e_{v}=\frac{\#(\text { edges among neighboring nodes })}{\left(\begin{array}{c}
k_{v} \\
2
\end{array}\right)} \in[0,1]</script></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/2021052810445748.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BvbGFyaXNSaXNpbmdXYXI=,size_16,color_FFFFFF,t_70" alt=""></p>
<p><img src="https://img-blog.csdnimg.cn/20210528105114750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BvbGFyaXNSaXNpbmdXYXI=,size_16,color_FFFFFF,t_70" alt=""></p>
<h3 id="Graphlets"><a href="#Graphlets" class="headerlink" title="Graphlets"></a>Graphlets</h3><ul>
<li>Rooted connected induced non-isomorphic subgraphs, 有根连通异构子图<ul>
<li>graphlet为给定图的子图，需要满足<strong>四个条件</strong><ul>
<li>1.rooted: 同一个结构，指定的根节点不同，属于不同的结构</li>
<li>2.connected: 连通图</li>
<li>3.induced subgraphs: 是induced得到的subgraphs, 即该子图包括的nodes在原图中的所有link都应该包括在子图中</li>
<li>4.non-isomorphic: 异构图</li>
<li>对3的解释：</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/2021/12/12/CS224WLec2/induced_subgraph.png" class="" title="induced_subgraph">
<ul>
<li>不给定图时，<strong>节点数为2-5情况下一共能产生如图所示73种graphlet。这73个graphlet的核心概念就是不同的形状，不同的位置。</strong>图中标的数字代表graphlet的id（根节点可选的位置）。例如对于$ G_0 $，两个节点是等价的（对称的），所以只有一种graphlet；对于$ G_1 $，根节点有在中间和在边上两种选择，上下两个边上的点是等价的，所以只有两种graphlet。其他的类似。<br><img src="https://img-blog.csdnimg.cn/20210528121841575.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BvbGFyaXNSaXNpbmdXYXI=,size_16,color_FFFFFF,t_70" alt=""><ul>
<li>Graphlet Degree Vector (GDV): A count vector of graphslets rooted at a given node</li>
</ul>
</li>
</ul>
<ul>
<li><strong>GDV counts #(graphlets) that a node touches</strong><br><img src="https://img-blog.csdnimg.cn/20210528123912356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BvbGFyaXNSaXNpbmdXYXI=,size_16,color_FFFFFF,t_70" alt=""></li>
<li>以v作为节点的有根连通异构子图共4个，分别为a b c d为节点的子图。a有两种情况；b一种情况；c不存在是因为graphlet是induced subgraph，c可以induce为b；d有两种情况。所以得到的GDV为[2, 1, 0, 2]</li>
<li>考虑2-5个节点的graphlets，我们得到一个长度为73个坐标coordinate（就前图所示一共73种graphlet）的向量GDV，描述该点的局部拓扑结构topology of node’s neighborhood，可以捕获距离为4 hops的互联性interconnectivities</li>
<li>相比节点度数或clustering coefficient，GDV能够描述两个节点之间更详细的节点局部拓扑结构相似性local topological similarity。</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>节点的重要性importance-based: node degree/centrality</li>
<li>节点邻域的拓扑结构structure-based: node degree/clustering cofficient/graphlet count vector</li>
</ul>
<h2 id="Link-features-amp-Link-level-prediction"><a href="#Link-features-amp-Link-level-prediction" class="headerlink" title="Link features &amp; Link-level prediction"></a>Link features &amp; Link-level prediction</h2><ul>
<li>diagram: predict <strong>new links</strong> based on the existing links. At test time, predict ranked <strong>top k</strong> node pairs</li>
<li>所以需要为<strong>pair of nodes</strong>设计特征</li>
</ul>
<h3 id="两类任务"><a href="#两类任务" class="headerlink" title="两类任务"></a>两类任务</h3><ul>
<li>predict缺失的links</li>
<li>给定t0时刻的links，预测t1时刻的links</li>
</ul>
<h3 id="Distance-based-features"><a href="#Distance-based-features" class="headerlink" title="Distance-based features"></a>Distance-based features</h3><ul>
<li>一个pair of node的最短距离、最长距离等</li>
<li><strong>无法得知其邻域的overlap</strong></li>
</ul>
<h3 id="Local-neighborhood-overlap"><a href="#Local-neighborhood-overlap" class="headerlink" title="Local neighborhood overlap"></a>Local neighborhood overlap</h3><ul>
<li>某两个节点其邻域的overlap -》<strong>两节点邻域没有交集其overlap为0</strong></li>
<li>1.common neighbors: <strong>求交集</strong>, <script type="math/tex">\left|N\left(v_{1}\right) \cap N\left(v_{2}\right)\right|</script></li>
<li>2.Jaccard’s coefficient: <strong>IoU</strong> <script type="math/tex">\frac{\left|N\left(v_{1}\right) \cap N\left(v_{2}\right)\right|}{\left|N\left(v_{1}\right) \cup N\left(v_{2}\right)\right|}</script></li>
<li>3.Adamic-Adar index: <strong>两节点交集的点的1/log(degree)之和</strong> <script type="math/tex">\sum_{u \in N\left(v_{1}\right) \cap N\left(v_{2}\right)} \frac{1}{\log \left(k_{u}\right)}</script></li>
</ul>
<h3 id="Global-neighborhood-overlap"><a href="#Global-neighborhood-overlap" class="headerlink" title="Global neighborhood overlap"></a>Global neighborhood overlap</h3><ul>
<li><strong>使用整张图的结构来表示两节点的关系</strong></li>
<li>Katz index: 计算两个节点间距离为i时有多少条通路，并穷举i，求通路数之和</li>
<li>邻接矩阵A的k次幂可以表示距离为k时通路的数量：$A_{u v}^{l}$即距离为l的通路数量</li>
<li>Katz index: <script type="math/tex">S_{v_{1} v_{2}}=\sum_{l=1}^{\infty} \beta^{l} A_{v_{1} v_{2}}^{l}</script></li>
<li>闭环形式：<script type="math/tex; mode=display">\boldsymbol{S}=\sum_{i=1}^{\infty} \beta^{i} \boldsymbol{A}^{i}=\underbrace{(\boldsymbol{I}-\beta \boldsymbol{A})^{-1}}_{=\sum_{i=0}^{\infty} \beta^{i} A^{i}}-\boldsymbol{I}</script></li>
</ul>
<h2 id="Graph-features"><a href="#Graph-features" class="headerlink" title="Graph features"></a>Graph features</h2><p><strong>focus on the Graph kernels</strong></p>
<ul>
<li>图G和G’，其kernel为K(G, G’)，且可以写成：<script type="math/tex">K\left(G, G^{\prime}\right)=\boldsymbol{f}_{G}{ }^{\mathrm{T}} \boldsymbol{f}_{G^{\prime}}</script> ，其中G的特征向量为$ \boldsymbol{f}_{G} $，称这种形式的特征表达为Kernel features</li>
<li>需要满足：<ul>
<li>kernel K(G, G’)能够表达G和G‘的相似性</li>
<li>kernel matrix <script type="math/tex">\boldsymbol{K}=\left(K\left(G, G^{\prime}\right)\right)_{G, G^{\prime}}</script></li>
<li>存在一种特征表达$\phi(\cdot)$满足$K\left(G, G^{\prime}\right)=\phi(\mathrm{G})^{\mathrm{T}} \phi\left(G^{\prime}\right)$</li>
</ul>
</li>
<li>最简单的，kernel可以是图不同degree的node个数组成的vector</li>
</ul>
<h3 id="Graphlet-kernel"><a href="#Graphlet-kernel" class="headerlink" title="Graphlet kernel"></a>Graphlet kernel</h3><ul>
<li>与node level的不同：<ul>
<li>不要求连通性，点个数在继续，不需要彼此能联通</li>
<li>不要求rooted，异构即可</li>
</ul>
</li>
<li>具体的：<ul>
<li>先确定graphlet的node个数k</li>
<li>求k个node的graphlets list： <script type="math/tex">\mathcal{G}_{k} = \left(g_{1}, g_{2}, \ldots, g_{n_{k}}\right)</script></li>
<li>求graphlets list中每个Graphlet在图中出现了几次，并构成vector： <script type="math/tex">\left(\boldsymbol{f}_{G}\right)_{i}=\#\left(g_{i} \subseteq G\right)</script> for <script type="math/tex">i=1,2, \ldots, n_{k}</script></li>
</ul>
</li>
</ul>
<img src="/2021/12/12/CS224WLec2/grahlet_vectors.png" class="" title="graphlet_vector">
<ul>
<li>problem1: graph不一样，kernel的值会skewd<ul>
<li>solution: normalize<script type="math/tex; mode=display">
\boldsymbol{h}_{G}=\frac{\boldsymbol{f}_{G}}{\operatorname{Sum}\left(\boldsymbol{f}_{G}\right)} \quad K\left(G, G^{\prime}\right)=\boldsymbol{h}_{G}{ }^{\mathrm{T}} \boldsymbol{h}_{G^{\prime}}</script></li>
</ul>
</li>
<li>problem2: 对size n的graph求size k的graphlet需要$n^{k}$</li>
<li>problem3: 图的degree上界是d，则需要$O\left(n d^{k-1}\right)$</li>
<li>problem4: 最糟糕的情况是判断一个图是不是另一个图的子图是<a href="https://chenzk1.github.io/2021/12/15/P&amp;NP&amp;NP-hard/">np-hard问题</a>（不确定性多项式hard问题，不能确定能不能在n的多项式内的复杂度完成）<h3 id="Weisfeiler-Lehman-Kernel"><a href="#Weisfeiler-Lehman-Kernel" class="headerlink" title="Weisfeiler-Lehman Kernel"></a>Weisfeiler-Lehman Kernel</h3></li>
<li>color refinement方法</li>
<li>也需要定义一个k，含义为step <script type="math/tex">c^{(k+1)}(v)=\operatorname{HASH}\left(\left\{c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right\}\right)</script></li>
<li>每一个step都用到了上一个step的值，而第一个step中，每个节点v的c(v)与其邻域有关，所以<strong> 第k个step能够得到v的k-hop邻域 </strong></li>
<li>example, k=1, G1和G2的vector都是[6，2，1，2，1]<img src="/2021/12/12/CS224WLec2/wl_kernel1.png" class="" title="wl_kernel1">
<img src="/2021/12/12/CS224WLec2/wl_kernel2.png" class="" title="wl_kernel2"></li>
<li>时间复杂度：#(edges)的线性</li>
<li>本质：bag of colors，colors代表了n-hop(n=1…k)的邻域结构</li>
<li>与Graph neural network很类似。？</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>CV-CNN</title>
    <url>/2021/12/10/CNN/</url>
    <content><![CDATA[<p>[TOC]</p>
<p><a href="https://zhuanlan.zhihu.com/p/27642620">原贴</a></p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><ul>
<li>卷积神经网络大致就是covolutional layer, pooling layer, ReLu layer, fully-connected layer的组合，例如下图所示的结构。<br><img src="https://pic4.zhimg.com/80/v2-cf87890eb8f2358f23a1ac78eb764257_hd.png" alt="ex-1"></li>
</ul>
<a id="more"></a>
<h3 id="图片的识别"><a href="#图片的识别" class="headerlink" title="图片的识别"></a>图片的识别</h3><ul>
<li>生物所看到的景象并非世界的原貌，而是长期进化出来的适合自己生存环境的一种感知方式</li>
<li>画面识别实际上是寻找/学习动物的视觉关联形式（即将能量与视觉关联在一起的方式）</li>
<li>画面的识别取决于：<ul>
<li>图片本身</li>
<li>被如何观察</li>
</ul>
</li>
<li>图像不变性：<ul>
<li>rotation</li>
<li>viewpoint</li>
<li>size</li>
<li>illumination</li>
<li>…<h3 id="前馈的不足"><a href="#前馈的不足" class="headerlink" title="前馈的不足"></a>前馈的不足</h3></li>
</ul>
</li>
<li>当出现上述variance时，前馈无法做到适应，即前馈只能对同样的内容进行识别，若出现其他情况时，只能增加样本重新训练</li>
<li>解决方法可以是让图片中不同的位置有相同的权重——<strong>共享权重</strong><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><h4 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h4></li>
<li><strong>空间共享</strong>（引入的先验知识）</li>
<li><strong>局部连接</strong>（得到的下一层节点与该层并非全连接）</li>
<li>depth上是<strong>全连接</strong>的<blockquote>
<p>每个filter会在width维, height维上，以局部连接和空间共享，并贯串整个depth维的方式得到一个Feature Map。</p>
</blockquote>
</li>
</ul>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p><img src="https://pic3.zhimg.com/80/v2-23db15ec3f783bbb5cf811711e46dbba_hd.png" alt="cnn_example"></p>
<ul>
<li>在输入depth为1时：被filter size为2x2所圈中的4个输入节点连接到1个输出节点上。</li>
<li>在输入depth为3时：被filter size为2x2，但是贯串3个channels后，所圈中的12个输入节点连接到1个输出节点上。</li>
<li>在输入depth为n时：2x2xn个输入节点连接到1个输出节点上。<blockquote>
<p>三个channels的权重并不共享。 即当深度变为3后，权重也跟着扩增到了三组。</p>
</blockquote>
</li>
</ul>
<h5 id="zero-padding"><a href="#zero-padding" class="headerlink" title="zero padding"></a>zero padding</h5><p>有时为了保证feature map与输入层保持同样大小，会添加zero padding，一般3*3的卷积核padding为1，5*5为2</p>
<p>Feature Map的尺寸等于(input_size + 2 *padding_size − filter_size)/stride+1</p>
<h4 id="形状、概念抓取"><a href="#形状、概念抓取" class="headerlink" title="形状、概念抓取"></a>形状、概念抓取</h4><ul>
<li>卷积层可以对基础形状（包括边缘、棱角、模糊等）、对比度、颜色等概念进行抓取</li>
<li>可以通过多层卷积实现对一个较大区域的抓取</li>
<li>抓取的特征取决于卷积核的权重，而此权重由网络根据数据学习得到，即CNN会自己学习以什么样的方式观察图片</li>
<li>可以有多个filter，从而可以学习到多种特征<ul>
<li>此时卷积层的输出depth也就不是1了</li>
<li>卷积层的输入输出均为长方体：其中depth与filters个数相同<br><img src="https://pic1.zhimg.com/80/v2-a9983c3cee935b68c73965bc1abe268c_hd.png" alt="ex4"><br><img src="https://pic1.zhimg.com/80/v2-d11e1d2f2c41b6df713573f8155bc324_hd.png" alt="ex2"><h4 id="非线性（以ReLu为例）"><a href="#非线性（以ReLu为例）" class="headerlink" title="非线性（以ReLu为例）"></a>非线性（以ReLu为例）</h4>增强模型的非线性拟合能力<br><img src="https://pic3.zhimg.com/80/v2-54a469b2873542e75abf2bc5d8fcaa1a_hd.png" alt="ex3"><h4 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h4><em>比如以步长为2，2x2的 filter pool</em><br><img src="https://pic4.zhimg.com/80/v2-cd717414dcf32dac4df73c00f1e7c6c3_hd.jpg" alt="ex5"></li>
</ul>
</li>
<li>pooling的主要功能是downsamping，有助减少conv过程中的冗余<h4 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h4></li>
<li>当抓取到足以用来识别图片的特征后，接下来的就是如何进行分类。 全连接层（也叫前馈层）就可以用来将最后的输出映射到线性可分的空间。 通常卷积网络的最后会将末端得到的长方体平摊(flatten)成一个长长的向量，并送入全连接层配合输出层进行分类。<h4 id="一些变体中用到的技巧"><a href="#一些变体中用到的技巧" class="headerlink" title="一些变体中用到的技巧"></a>一些变体中用到的技巧</h4></li>
<li>1x1卷积核：选择不同的个数，用来降维或升维</li>
<li>残差<blockquote>
<p>所有的这些技巧都是对各种不变性的满足</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>CV</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec1-Graph Intro &amp; Graph Representations</title>
    <url>/2021/12/12/CS224WLec1/</url>
    <content><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><ul>
<li>title: Machine Learning with Graphs</li>
<li>year: Fall 2021</li>
<li><a href="http://web.stanford.edu/class/cs224w/">CS224W page</a></li>
<li><a href="https://snap-stanford.github.io/cs224w-notes/">note</a>、<a href="https://yasoz.github.io/cs224w-zh/#/Introduction-and-Graph-Structure">note中文翻译</a><a id="more"></a></li>
<li>professor：<a href="https://profiles.stanford.edu/jure-leskovec">Jure Leskovec</a></li>
<li>videos: <a href="https://www.bilibili.com/video/BV1RZ4y1c7Co/?spm_id_from=333.788.recommend_more_video.0">bilibili</a></li>
<li>labs: <a href="https://docs.google.com/document/d/e/2PACX-1vRMprg-Uz9oEnjXOJpRPJ5oyEXRnJAz9qVeEB04sucx2o2RtQ-HRfom6IWS5ONhfoly0TOmJM7BxIzJ/pub">lab*5</a></li>
<li>schedule: <a href="http://web.stanford.edu/class/cs224w/index.html#schedule">schedule</a></li>
<li>goal: graph的表征学习和用于graph的机器学习算法</li>
<li>topics<ul>
<li>Traditional methods: Graphlets, Graph Kernels</li>
<li>Methods for node embeddings: DeepWalk, Node2Vec</li>
<li>Graph Neural Networks: GCN, GraphSAGE, GAT, Theory of GNNs</li>
<li>Knowledge graphs and reasoning: TransE, BetaE</li>
<li>Deep generative models for graphs: GraphRNN</li>
<li>Applications to Biomedicine, Science, Industry</li>
</ul>
</li>
</ul>
<h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><ul>
<li>graph level: graph classification,例如分子属性预测</li>
<li>node level: node classification, 比如用户/商品分类</li>
<li>edge level: link prediction，例如knowledge graph completioni、推荐系统、药物副作用预测</li>
<li>community(subgraph) level: clustering，比如social circle detections</li>
<li>others<ul>
<li>graph generation</li>
<li>graph evolution</li>
<li>…</li>
</ul>
</li>
</ul>
<h1 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h1><p>G=(V, E, R, T)</p>
<ul>
<li>node: V</li>
<li>edge: E</li>
<li>relation type: R</li>
<li>node type: T</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li>directed &amp; undirected</li>
<li>node degree<ul>
<li>avg degree: <script type="math/tex; mode=display">
\bar{k}=\langle k\rangle=\frac{1}{N} \sum_{i=1}^{N} k_{i}=\frac{2 E}{N}</script></li>
<li>in-degree &amp; out-degree</li>
</ul>
</li>
<li>bipartite graph二部图: 包含两种不同的node，node只和另外一部的node连接。可以通过投影的方式转化为folded/projected bipartite graphs</li>
</ul>
<h2 id="表示"><a href="#表示" class="headerlink" title="表示"></a>表示</h2><ul>
<li>Adjacency Matrix：无向图时为对称矩阵。<strong>graph大多数时为高度稀疏矩阵（degree远小于节点数），邻接矩阵会造成内存浪费</strong></li>
<li>Adjacency List: 对每一个节点存储其neighbors</li>
<li>图的附加属性：weight/ranking/type/sign/…</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec5-Label Propagation for Node Classification[LIP]</title>
    <url>/2021/12/19/CS224WLec5/</url>
    <content><![CDATA[<p><a href="http://web.stanford.edu/class/cs224w/slides/05-message.pdf">ppt</a></p>
<h1 id="Semi-supervised-binary-node-classification"><a href="#Semi-supervised-binary-node-classification" class="headerlink" title="Semi-supervised binary node classification"></a>Semi-supervised binary node classification</h1><ul>
<li>goal: 知道部分节点的label求其他节点的label</li>
<li>main assumption：网络中存在的同质性。同类的node倾向于相互连接，或者聚在一起。</li>
<li>framework<ul>
<li>初始化</li>
<li>迭代</li>
<li>收敛</li>
</ul>
</li>
<li>approaches<ul>
<li>relational classification</li>
<li>iterative classification</li>
<li>correct &amp; smooth</li>
</ul>
</li>
<li>以节点二分类为例</li>
</ul>
<a id="more"></a>
<h1 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h1><h2 id="Probabilistic-relational-classifier"><a href="#Probabilistic-relational-classifier" class="headerlink" title="Probabilistic relational classifier"></a>Probabilistic relational classifier</h2><ul>
<li><strong>只用到了邻居节点的label来做分类</strong></li>
<li>step1: 初始化所有unlabeled nodes为0.5</li>
<li>step2: update所有unlabeled nodes的预测值 <script type="math/tex">P\left(Y_{v}=c\right)=\frac{1}{\sum_{(v, u) \in E} A_{v, u}} \sum_{(v, u) \in E} A_{v, u} P\left(Y_{u}=c\right)</script><ul>
<li>这里A为邻接矩阵，若边有权重，则Au,v是带权邻接矩阵</li>
</ul>
</li>
<li>step3: 重复step2直到收敛：达到最大steps，或者所有节点label不再更新</li>
<li>challenges: <ul>
<li>不能保证收敛</li>
<li>没有用到节点特征信息</li>
</ul>
</li>
</ul>
<h2 id="Iterative-classification"><a href="#Iterative-classification" class="headerlink" title="Iterative classification"></a>Iterative classification</h2><ul>
<li>Classify node v based on its attributes <script type="math/tex">f_v</script> as well as labels <script type="math/tex">z_v</script> of neighbor set <script type="math/tex">N_v</script></li>
<li><script type="math/tex">z_v</script>: v的neibors的label特征，可以是<ul>
<li><script type="math/tex">N_v</script>中每种label的histogram</li>
<li><script type="math/tex">N_v</script>中数量最多的label</li>
<li>不同label的数量</li>
</ul>
</li>
<li>流程<img src="/2021/12/19/CS224WLec5/classifier1.png" class="" title="classifier1">
</li>
</ul>
<h2 id="Collective-classification-correct-amp-smooth"><a href="#Collective-classification-correct-amp-smooth" class="headerlink" title="Collective classification: correct &amp; smooth"></a>Collective classification: correct &amp; smooth</h2><ul>
<li>recent state-of-the-art collective classification method</li>
<li>一种后处理方法</li>
<li>基于的假设：相邻节点的误差应该接近</li>
<li>correct step<ul>
<li>初始error: labeled node: error = ground truth minus soft label; unlabeled node: error=0</li>
<li>利用扩散矩阵 <script type="math/tex">\widetilde{\boldsymbol{A}}</script>求取下一步的error<script type="math/tex; mode=display">\boldsymbol{E}^{(t+1)} \leftarrow(1-\alpha) \cdot \boldsymbol{E}^{(t)}+\alpha \cdot \widetilde{\boldsymbol{A}} \boldsymbol{E}^{(t)}</script></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec3-Node Embeddings</title>
    <url>/2021/12/15/CS224WLec3/</url>
    <content><![CDATA[<h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><ul>
<li>goal: 为node生成一个embedding</li>
<li>两个要素：<strong>encoder/相似度计量方法（encode前后都需要）</strong></li>
<li>framework: encoder 生成embedding —&gt; 相似度计量方法决定embedding学习的好坏</li>
<li><strong>unsupervised/self-supervised way</strong> based on random walks</li>
<li>task independent</li>
</ul>
<a id="more"></a>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><ul>
<li>goal: 原始graph中相似的node获得的embeddings也是相似的</li>
<li>类似于word2vec: 目标是求node u的embedding <script type="math/tex">\mathbf{z}_{u}</script>,而模型的预测目标是：<script type="math/tex">P\left(v \mid \mathbf{z}_{u}\right)</script>，即node v出现在以node u开始的walk上的概率。</li>
<li>如何获得“句子”：random walk</li>
<li>范式: <ul>
<li>encoder生成node embedding，本节的encoder为word2vec中的权重矩阵: <script type="math/tex">\operatorname{ENC}(v)=\mathbf{z}_{v}</script></li>
<li>decoder将node embedding映射回原空间，这里存在隐式的decoder，embedding空间两向量的点积可以表示原空间u,v的相似度: <script type="math/tex">\operatorname{similarity}(u, v) \approx \mathbf{z}_{v}^{\mathrm{T}} \mathbf{z}_{u}</script><ul>
<li>点击相似度：最小化两向量的模以及夹角余弦的乘积</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Deep-Walk"><a href="#Deep-Walk" class="headerlink" title="Deep Walk"></a>Deep Walk</h2><h3 id="Random-Walk"><a href="#Random-Walk" class="headerlink" title="Random Walk"></a>Random Walk</h3><ul>
<li>出发点：如果一个random walk中包括从u到v的路径，那u和v是相似的/有相似的高维的多跳信息</li>
<li>本质：DFS</li>
<li>$ N_{\mathrm{R}}(u) $为策略R下，从u出发的walk中，出现的所有nodes<script type="math/tex; mode=display">\max _{f} \sum_{u \in V} \log \mathrm{P}\left(N_{\mathrm{R}}(u) \mid \mathbf{z}_{u}\right)</script>—》<script type="math/tex; mode=display">\mathcal{L}=\sum_{u \in V} \sum_{v \in N_{R}(u)}-\log \left(P\left(v \mid \mathbf{z}_{u}\right)\right)</script></li>
<li>利用softmax求p<script type="math/tex; mode=display">P\left(v \mid \mathbf{z}_{u}\right)=\frac{\exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)}{\sum_{n \in V} \exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n}\right)}</script></li>
<li>problem: softmax分母 以及 最外层都需要|V|次遍历 —》<script type="math/tex">\mathrm{O}\left(|\mathrm{V}|^{2}\right)</script>的复杂度 —》<strong>优化</strong></li>
</ul>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><ul>
<li>使用所有样本做normalization —&gt; 只采样k个负样本做normalization<script type="math/tex; mode=display">P\left(v \mid \mathbf{z}_{u}\right)=\frac{\exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)}{\sum_{n \in V } \exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n}\right)} \approx \log \left(\sigma\left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)\right)-\sum_{i=1}^{k} \log \left(\sigma\left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n_{i}}\right)\right), n_{i} \sim P_{V}</script></li>
<li>k的选择：<ul>
<li>k越大，模型越鲁棒</li>
<li>k越大，对负样本考虑的越多</li>
<li>5~20间较常见</li>
</ul>
</li>
<li>负样本的选择：可以选择graph内任意样本，但更准确的方法是选择不在walk中的样本</li>
</ul>
<h2 id="Node2vec-better-random-walk-strategy"><a href="#Node2vec-better-random-walk-strategy" class="headerlink" title="Node2vec: better  random walk strategy"></a>Node2vec: better  random walk strategy</h2><ul>
<li>简单的random walk会限制walk中的node相似度与graph中node相似度的一致性</li>
</ul>
<h3 id="Biased-2nd-order-random-Walks"><a href="#Biased-2nd-order-random-Walks" class="headerlink" title="Biased 2nd-order random Walks"></a>Biased 2nd-order random Walks</h3><ul>
<li>trade off between local and global views of the network: BFS &amp; DFS</li>
<li>当前在w, 上一步在s的walk，有三种行走方向<ul>
<li>退后：回退到s</li>
<li>保持：走到和s距离一致的一个节点</li>
<li>前进：走到距离s更远的节点<img src="/2021/12/15/CS224WLec3/node2vec1.png" class="" title="node2vec1"></li>
</ul>
</li>
<li>实现：两个<strong>超参</strong>p/q，以及“1”来以非归一化的方法表示上述三种情况的概率<img src="/2021/12/15/CS224WLec3/node2vec2.png" class="" title="node2vec2"></li>
<li>流程<ul>
<li>Compute random walk probabilities</li>
<li>Simulate 𝑟 random walks of length 𝑙 starting from each node 𝑢</li>
<li>Optimize the node2vec objective using Stochastic Gradient Descent</li>
</ul>
</li>
<li>Linear-time complexity</li>
<li>All 3 steps are individually parallelizable</li>
</ul>
<h2 id="Embedding-entire-graphs"><a href="#Embedding-entire-graphs" class="headerlink" title="Embedding entire graphs"></a>Embedding entire graphs</h2><ul>
<li>approach1: add all node embeddings</li>
<li>approach2: introduce a “virtual node” or “super node” to represent the graph and learning embedding for this graph</li>
<li>approach3: anonymous walks embeddings</li>
</ul>
<h3 id="Anonymous-walk-embeddings"><a href="#Anonymous-walk-embeddings" class="headerlink" title="Anonymous walk embeddings"></a>Anonymous walk embeddings</h3><ul>
<li>Anonymous walk: random walk —&gt; 将node表示为距离start node的去重index。因此，确定了walk length的时候，就确定了anonymous walk中index的个数。</li>
<li>方法1：长度为l的annoy walk共有n种情况 —&gt; 做m次random walks —&gt; 统计每种情况的count，并形成一个vector</li>
<li>方法2：用Anonymous walks的概率分布，学习图的embedding<ul>
<li>Learn to predict walks that co-occur in 𝚫-size window (e.g., predict 𝑤3 given 𝑤1, 𝑤2 if Δ = 2)</li>
<li>objective:<script type="math/tex; mode=display">\max _{z_{G}} \sum_{t=\Delta+1}^{T} \log P\left(w_{t} \mid w_{t-\Delta}, \ldots, w_{t-1}, \mathbf{z}_{G}\right)</script></li>
</ul>
</li>
</ul>
<img src="/2021/12/15/CS224WLec3/annoywalks1.png" class="" title="annoywalks1">
<img src="/2021/12/15/CS224WLec3/annoywalks2.png" class="" title="annoywalks2">
<h2 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h2><ul>
<li>属于shallow encoding，有如下优缺点：<ul>
<li>需要O(|V|)的参数量，节点间的embedding不共享，每个node有独立的embedding</li>
<li>training时没有的node，不会有embedding</li>
<li>没有利用到节点的特征，只利用了graph structure</li>
</ul>
</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="http://web.stanford.edu/class/cs224w/slides/03-nodeemb.pdf">ppt</a></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
        <tag>DeepWalk</tag>
        <tag>Node2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec6~Lec9-GNN</title>
    <url>/2021/12/22/CS224WLec6/</url>
    <content><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><ul>
<li>DeepWalk/Node2vec属于shallow encoding，有如下优缺点：<ul>
<li>需要O(|V|)的参数量，节点间的embedding不共享，每个node有独立的embedding</li>
<li>推导式的（transductive）：training时没有的node，不会有embedding</li>
<li>没有利用到节点的特征，只利用了graph structure</li>
</ul>
</li>
<li>范式: <ul>
<li>encoder生成node embedding，DeepWalk&amp;Node2vec中为一个|V|*D的权重矩阵: <script type="math/tex">\operatorname{ENC}(v)=\mathbf{z}_{v}</script></li>
<li>decoder将node embedding映射回原空间，这里存在隐式的decoder，embedding空间两向量的点积可以表示原空间u,v的相似度: <script type="math/tex">\operatorname{similarity}(u, v) \approx \mathbf{z}_{v}^{\mathrm{T}} \mathbf{z}_{u}</script><ul>
<li>点积相似度：最小化两向量的模以及夹角余弦的乘积</li>
</ul>
</li>
</ul>
</li>
<li>GNN: deep encoding<ul>
<li>encoder为MLP</li>
<li>decoder为某种向量相似度</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h1 id="Basics-amp-Intros"><a href="#Basics-amp-Intros" class="headerlink" title="Basics &amp; Intros"></a>Basics &amp; Intros</h1><h2 id="Insights"><a href="#Insights" class="headerlink" title="Insights"></a>Insights</h2><ul>
<li>其他NN：<strong>依赖于IID(independent and identically distributed)，不同样本间是独立的</strong>，因此其无需满足排列不变性，而<strong>GNN节点间不独立</strong> —》 每个节点的特征依赖于其他节点 —》 节点顺序改变后，输入GNN的特征也会改变 —》 需要满足排列不变性，使得不同的节点排列，也能有同样的结果</li>
<li>原则<ul>
<li>排列不变性：调整输入节点的顺序，得到的同一个节点的表达应该一致。A为邻接矩阵，X为节点特征矩阵，两种不同的节点顺序下，得到的同一个节点的表达应该一致<script type="math/tex; mode=display">f\left(\boldsymbol{A}_{1}, \boldsymbol{X}_{1}\right)=f\left(\boldsymbol{A}_{2}, \boldsymbol{X}_{2}\right)</script></li>
</ul>
</li>
<li>MLP：不满足排列不变性<img src="/2021/12/22/CS224WLec6/GNN1.png" class="" title="GNN1"></li>
<li>利用MLP实现GNN不符合预期<img src="/2021/12/22/CS224WLec6/GNN2.png" class="" title="GNN2"></li>
<li>Insights: <strong>借鉴CNN，每次卷积操作只取某个点及其邻域点</strong><ul>
<li><strong>卷积：对邻域信息的提取以及归纳</strong></li>
</ul>
</li>
</ul>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><ul>
<li>利用每个节点的邻域节点为每个节点建立计算图。nn的层数k代表用了k hop的邻域。</li>
<li>每个节点都有不同的计算图<img src="/2021/12/22/CS224WLec6/GNN4.png" class="" title="GNN4"></li>
<li>layer0: node v的<strong>输入特征</strong></li>
<li>layerk: 经过了k跳后，node v的节点信息</li>
</ul>
<h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><ul>
<li>每一层包含两个阶段：信息aggregation &amp; passing<img src="/2021/12/22/CS224WLec6/GNN5.png" class="" title="GNN5"></li>
<li>每层参数共享</li>
<li><script type="math/tex">h_{v}^{k}</script>: the hidden representation of node v at layer k</li>
<li><script type="math/tex">W_{k}</script>: weight matrix for neighborhood aggregation</li>
<li><script type="math/tex">B_{k}</script>: weight matrix for transforming hidden vector of self</li>
<li>当aggregate为简单的平均时，可以转化为稀疏矩阵表达式 —》 稀疏矩阵便于优化<img src="/2021/12/22/CS224WLec6/GNN6.png" class="" title="GNN6">
</li>
</ul>
<h1 id="General-GNN-Framework"><a href="#General-GNN-Framework" class="headerlink" title="General GNN Framework"></a>General GNN Framework</h1><h2 id="A-Single-GNN-Layer"><a href="#A-Single-GNN-Layer" class="headerlink" title="A Single GNN Layer"></a>A Single GNN Layer</h2><ul>
<li>Goal: Compress a set of vectors into a single vector</li>
<li>Two steps:<ul>
<li>Message, v, 传送信息</li>
<li>Aggregate</li>
</ul>
</li>
<li>others:<ul>
<li>Nonlinearity(activation)：增加表达能力</li>
<li>residual/attention/dropout/BatchNorm/…</li>
</ul>
</li>
</ul>
<h3 id="Message"><a href="#Message" class="headerlink" title="Message"></a>Message</h3><ul>
<li>将l-1层的vectors过一个function，得到新的vectors，也称messages。对于每个node u：<script type="math/tex; mode=display">\mathbf{m}_{u}^{(l)}=\mathrm{MSG}^{(l)}\left(\mathbf{h}_{u}^{(l-1)}\right)</script></li>
<li>比如使用线性层作为该函数：<script type="math/tex; mode=display">\mathbf{m}_{u}^{(l)}=\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}</script></li>
</ul>
<h3 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h3><ul>
<li>对于节点v，将其领域内的所有messages聚合在一起<script type="math/tex; mode=display">\mathbf{h}_{v}^{(l)}=\mathrm{AGG}^{(l)}\left(\left\{\mathbf{m}_{u}^{(l)}, u \in N(v)\right\}\right)</script></li>
<li>principle: Aggregation需要满足排列不变性</li>
</ul>
<h3 id="Issues-amp-Solutions"><a href="#Issues-amp-Solutions" class="headerlink" title="Issues &amp; Solutions"></a>Issues &amp; Solutions</h3><ul>
<li>Issue1: 只考虑了领域，节点本身的信息被丢弃了</li>
<li>Solution1: 计算<script type="math/tex">{h}_{v}^{(l)}</script>的时候，考虑 <script type="math/tex">{h}_{v}^{(l-1)}</script>，可以认为是node v有一个self-edge<ul>
<li>Message<script type="math/tex; mode=display">\mathbf{m}_{u}^{(l)}=\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}, \mathbf{m}_{v}^{(l)}=\mathbf{B}^{(l)} \mathbf{h}_{v}^{(l-1)}</script></li>
<li>Aggregation<script type="math/tex; mode=display">\mathbf{m}_{u}^{(l)}=\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)} \mathbf{m}_{v}^{(l)}=\mathbf{B}^{(l)} \mathbf{h}_{v}^{(l-1)}</script></li>
</ul>
</li>
<li>Issue2: </li>
<li>Solution2: Residual</li>
</ul>
<h2 id="Classical-GNN-Layers"><a href="#Classical-GNN-Layers" class="headerlink" title="Classical GNN Layers"></a>Classical GNN Layers</h2><h3 id="GCN-Graph-Convolutional-Networks"><a href="#GCN-Graph-Convolutional-Networks" class="headerlink" title="GCN(Graph Convolutional Networks)"></a>GCN(Graph Convolutional Networks)</h3><ul>
<li>Message：利用出度作归一化。包含了self-edge。</li>
<li>Aggregation: sum<script type="math/tex; mode=display">
\mathbf{h}_{v}^{(l)}=\sigma\left(\sum_{u \in N(v)} \mathbf{W}^{(l)} \frac{\mathbf{h}_{u}^{(l-1)}}{|N(v)|}\right)</script></li>
</ul>
<h3 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h3><ul>
<li>Message和Aggregation结合在一起，且做了多次Aggregation</li>
<li>先对neighbors做aggregate，再和node本身aggregate，之后再计算一次Message<ul>
<li>邻域的Aggregation可以是Mean, pool, LSTM等<script type="math/tex; mode=display">
\mathbf{h}_{v}^{(l)}=\sigma\left(\mathbf{w}^{(l)} \cdot \operatorname{CONCAT}\left(\mathbf{h}_{v}^{(l-1)}, \mathrm{AGG}\left(\left\{\mathbf{h}_{u}^{(l-1)}, \forall u \in N(v)\right\}\right)\right)\right)</script></li>
</ul>
</li>
<li>l2 normalization: Apply l2 normalization to <script type="math/tex">{h}_{v}^{(l)}</script> at every layer。标准化后，每个vector的l2 norm都为1<script type="math/tex; mode=display">
\mathbf{h}_{v}^{(l)} \leftarrow \frac{\mathbf{h}_{v}^{(l)}}{\left\|\mathbf{h}_{v}^{(l)}\right\|_{2}} \forall v \in V</script></li>
</ul>
<h3 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h3><ul>
<li><strong>显式地获得不同邻域节点对目标节点的重要性</strong></li>
<li><script type="math/tex; mode=display">\alpha_{v u} $$ 为u(key)对v(value)的attention. 
- 例如GCN中 $$ \alpha_{v u}=\frac{1}{|N(v)|} $$ ，每个节点u对v的attention是一样的
$$ \mathbf{h}_{v}^{(l)}=\sigma\left(\sum_{u \in N(v)}{\alpha_{v u}} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}\right)</script></li>
<li>attention计算<ul>
<li>attention系数计算：相似度/MLP<script type="math/tex; mode=display">e_{v u}=a\left(\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}, \mathbf{W}^{(l)} \boldsymbol{h}_{v}^{(l-1)}\right)</script></li>
<li>归一化: softmax<script type="math/tex; mode=display">\alpha_{v u}=\frac{\exp \left(e_{v u}\right)}{\sum_{k \in N(v)} \exp \left(e_{v k}\right)}</script></li>
</ul>
</li>
<li>多头attention<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{h}_{v}^{(l)}[1]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^{1} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}\right) \\
\mathbf{h}_{v}^{(l)}[2]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^{2} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}\right) \\
\mathbf{h}_{v}^{(l)}[3]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^{3} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}\right)
\end{array}</script></li>
</ul>
<script type="math/tex; mode=display">
\mathbf{h}_{v}^{(l)}=\mathrm{AGG}\left(\mathbf{h}_{v}^{(l)}[1], \mathbf{h}_{v}^{(l)}[2], \mathbf{h}_{v}^{(l)}[3]\right)</script><ul>
<li>Pros<ul>
<li>Computationally efficien: attention系数可以并行计算所有的edges；aggregation可以并行计算所有nodes</li>
<li>Storage efficient: 不超过O(V+E)</li>
</ul>
</li>
</ul>
<h2 id="Stacking-GNN-layers"><a href="#Stacking-GNN-layers" class="headerlink" title="Stacking GNN layers"></a>Stacking GNN layers</h2><h3 id="The-over-smoothing-problem"><a href="#The-over-smoothing-problem" class="headerlink" title="The over-smoothing problem"></a>The over-smoothing problem</h3><ul>
<li>all the node embeddings converge to the same value</li>
<li><strong>每个node的embedding由其感受野决定，如果两个node的感受野重合的很多，其embedding越相近。而感受野由GNN的层数决定，层数越多，over-smoothing越严重</strong></li>
<li>Solution1: GNN层数的控制。先分析感受野大小，再<strong>根据所需感受野大小决定layers number</strong></li>
<li>Solution2: skip connections/residual。<ul>
<li>相当于mixture of models. N次skip, <script type="math/tex">2^{N}</script>个可能的路径/模型</li>
<li>Option1: 连到非线性前<script type="math/tex; mode=display">\mathbf{h}_{v}^{(l)}=\sigma\left(\sum_{u \in N(v)} \mathbf{W}^{(l)} \frac{\mathbf{h}_{u}^{(l-1)}}{|N(v)|}+\mathbf{h}_{v}^{(l-1)}\right)</script></li>
<li>Option2: 连到下一层</li>
</ul>
</li>
</ul>
<h3 id="提高GNN表达能力"><a href="#提高GNN表达能力" class="headerlink" title="提高GNN表达能力"></a>提高GNN表达能力</h3><ul>
<li>Solution1: make aggregation / transformation become a deep neural network!</li>
<li>Solution2: 增加其他层（MLP/CNN等）<img src="/2021/12/22/CS224WLec6/GNN7.png" class="" title="GNN7">
</li>
</ul>
<h2 id="Graph-Augmentation"><a href="#Graph-Augmentation" class="headerlink" title="Graph Augmentation"></a>Graph Augmentation</h2><ul>
<li>idea: raw input graph ≠ computation graph: <strong>计算图不必和实际的图结构保持一致</strong></li>
</ul>
<h3 id="Reason-for-augmentation"><a href="#Reason-for-augmentation" class="headerlink" title="Reason for augmentation"></a>Reason for augmentation</h3><ul>
<li>特征：原始图的特征可能比较少 —》 feature augmentation</li>
<li>图结构<ul>
<li>图太稀疏：inefficient message passing —》 Add virtual nodes / edges</li>
<li>图太稠密：costly —》 Sample neighbors when doing message passing</li>
<li>图太大：GPU放不下 —》 Sample subgraphs to compute embeddings </li>
</ul>
</li>
</ul>
<h3 id="Feature-augementation"><a href="#Feature-augementation" class="headerlink" title="Feature augementation"></a>Feature augementation</h3><ul>
<li>one-hot/constant encoding</li>
<li>others: 一般会使用：<ul>
<li>node degree</li>
<li>clustering coefficient</li>
<li>pagerank</li>
<li>centrality</li>
</ul>
</li>
</ul>
<h3 id="Sparse-graphs-augmentation"><a href="#Sparse-graphs-augmentation" class="headerlink" title="Sparse graphs augmentation"></a>Sparse graphs augmentation</h3><ul>
<li>add virtual nodes/edges</li>
<li>Add virtual edges<ul>
<li>例如在使用邻接矩阵A的时候，改为使用 <script type="math/tex">A+A^{2}</script>，A2相当于添加了virtual edges</li>
</ul>
</li>
<li>Add vitual nodes<ul>
<li>提高稀疏图中的message passing</li>
</ul>
</li>
</ul>
<h3 id="Node-neighborhood-sampling"><a href="#Node-neighborhood-sampling" class="headerlink" title="Node neighborhood sampling"></a>Node neighborhood sampling</h3><ul>
<li>针对很稠密的图引起的costly问题</li>
<li>方案：sampling<ul>
<li>对某个target节点，sample其neighbors</li>
<li>sample target节点</li>
</ul>
</li>
</ul>
<h2 id="GNN-training-pipeline"><a href="#GNN-training-pipeline" class="headerlink" title="GNN training pipeline"></a>GNN training pipeline</h2><img src="/2021/12/22/CS224WLec6/GNN8.png" class="" title="GNN8">
<ul>
<li>以上工作将计算得到set of d-dim node embs</li>
<li>还存在两个问题：<ul>
<li>这些emb需要被应用在具体的任务中（前向过程需要完善）</li>
<li>emb需要更新（反向传播需要定义）</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
\left\{\mathbf{h}_{v}^{(L)} \in \mathbb{R}^{d}, \forall v \in G\right\}</script><h3 id="Prediction-heads"><a href="#Prediction-heads" class="headerlink" title="Prediction heads"></a>Prediction heads</h3><h4 id="node-level"><a href="#node-level" class="headerlink" title="node-level"></a>node-level</h4><ul>
<li>直接利用node embs，其中<script type="math/tex">\mathbf{W}^{(H)} \in \mathbb{R}^{k * d}</script>: map node embeddings from <script type="math/tex">\mathbf{h}_{v}^{(L)} \in \mathbb{R}^{d}</script> to <script type="math/tex">\widehat{\boldsymbol{y}}_{v} \in \mathbb{R}^{k}</script>  so that we can compute the loss</li>
</ul>
<script type="math/tex; mode=display">\widehat{\boldsymbol{y}}_{v}=\operatorname{Head}_{\text {node }}\left(\mathbf{h}_{v}^{(L)}\right)=\mathbf{W}^{(H)} \mathbf{h}_{v}^{(L)}</script><h4 id="edge-level"><a href="#edge-level" class="headerlink" title="edge-level"></a>edge-level</h4><ul>
<li>使用pair of node embs</li>
<li>option1: concat+linear(map 2d-dim to k-dim)<script type="math/tex; mode=display">
\left.\widehat{\boldsymbol{y}}_{u v}=\text { Linear(Concat }\left(\mathbf{h}_{u}^{(L)}, \mathbf{h}_{v}^{(L)}\right)\right)</script></li>
<li>option2: dot product. 得到一个连续值，只能应用于二分类预测/一维回归</li>
<li>option3: 多头的dot product，多个加权的option2<script type="math/tex; mode=display">
\begin{array}{c}
\widehat{\boldsymbol{y}}_{u v}^{(1)}=\left(\mathbf{h}_{u}^{(L)}\right)^{T} \mathbf{W}^{(1)} \mathbf{h}_{v}^{(L)} \\
\widehat{\boldsymbol{y}}_{u v}^{(k)}=\left(\mathbf{h}_{u}^{(L)}\right)^{T} \mathbf{W}^{(k)} \mathbf{h}_{v}^{(L)} \\
\widehat{\boldsymbol{y}}_{u v}=\operatorname{Concat}\left(\widehat{y}_{u v}^{(1)}, \ldots, \widehat{\boldsymbol{y}}_{u v}^{(k)}\right) \in \mathbb{R}^{k}
\end{array}</script></li>
</ul>
<h4 id="graph-level"><a href="#graph-level" class="headerlink" title="graph-level"></a>graph-level</h4><ul>
<li>option1: global pooling<ul>
<li>问题：无法区分不同scale的graph<script type="math/tex; mode=display">
\begin{array}{l}
\text { Prediction for } G_{1}: \hat{y}_{G}=\operatorname{Sum}(\{-1,-2,0,1,2\})=0 \\
\text { Prediction for } G_{2}: \hat{y}_{G}=\operatorname{Sum}(\{-10,-20,0,10,20\})=0
\end{array}</script></li>
</ul>
</li>
<li>option2: hierarchically global pooling<ul>
<li>需要同时实现两个GNN任务：GNN A：计算node embeddings; GNN B：聚类</li>
<li>两个任务可以并行训练</li>
<li>为每一个cluster创建一个新的node，为相连的nodes创建edge，并生成一个新的pooled network<img src="/2021/12/22/CS224WLec6/GNN9.png" class="" title="GNN9">
</li>
</ul>
</li>
</ul>
<h3 id="Loss-defines"><a href="#Loss-defines" class="headerlink" title="Loss defines"></a>Loss defines</h3><ul>
<li>和其他nn无区别</li>
</ul>
<h3 id="Dataset-split"><a href="#Dataset-split" class="headerlink" title="Dataset split"></a>Dataset split</h3><ul>
<li>难点：graph的各个node/edge之间不满足iid假设。random split会带来information leakage。</li>
<li>solution1(Transductive setting): 会有部分leakage。仅仅split node labels.<ul>
<li>At training time, we compute embeddings using <strong>the entire graph</strong>, and train <strong>using node 1&amp;2’s labels</strong></li>
<li>At validation time, we compute embeddings using <strong>the entire graph</strong>, and evaluate on <strong>node 3&amp;4’s labels</strong></li>
<li>training / validation / test sets都在同一个graph上。三个dataset组成一个graph。</li>
<li>可以应用在node/edge tasks。因为graph task需要在unseen graphs上做测试，而transductive方法无法满足。</li>
</ul>
</li>
<li>solution2(Inductive setting): We break the edges between splits to get multiple graphs。没有信息泄漏，图被分成了三个子图。<ul>
<li>At training time, we compute embeddings <strong>using the graph over node 1&amp;2</strong>, and train using node 1&amp;2’s labels</li>
<li>At validation time, we compute embeddings <strong>using the graph over node 3&amp;4</strong>, and evaluate on node 3&amp;4’s labels</li>
<li>training / validation / test sets不在同一个graph上。三个dataset组成是三个graph。</li>
<li>可以应用在node/edge/graph tasks</li>
</ul>
</li>
</ul>
<h4 id="link-prediction"><a href="#link-prediction" class="headerlink" title="link prediction"></a>link prediction</h4><ul>
<li>link prediction是无监督任务，需要定义label &amp; split</li>
<li>step1: 先划分message edges和supervision edges。其中supervision edges不入图，只作为label。</li>
<li>step2: split<ul>
<li>Transductive method: 有四种edge: training message edges &amp; training supervision edges &amp; validation edges &amp; predict test edges.<img src="/2021/12/22/CS224WLec6/GNN10.png" class="" title="GNN10"></li>
<li>Inductive method: In train or val or test set, each graph will have 2 types of edges。<img src="/2021/12/22/CS224WLec6/GNN11.png" class="" title="GNN11">
</li>
</ul>
</li>
</ul>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><ul>
<li>data processing: use normalization</li>
<li>optimizer: adam is relatively robust to learning rate</li>
<li>activation: relu</li>
<li>bias term in every layer</li>
<li>debug:<ul>
<li>小数据集上，loss应该很小。如果underfit, something is wrong</li>
<li>loss</li>
<li>visualizations</li>
<li>initialization</li>
<li>adjust hyperparameters such as learning rate</li>
</ul>
</li>
</ul>
<h1 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h1><ul>
<li>本质：<strong>假设将节点作为特征向量作为输入feed into MLP, 特征向量会包含邻域节点的信息，因此不能满足排列不变性，而GNN则是把节点的领域信息存在了NN结构里，因此每个节点都拥有自己的计算图/神经网络结构</strong></li>
<li>流程：1）定义neighborhood aggregation function; 2) 定义loss function; 3) train; 4) generate node embeddings</li>
<li>权重矩阵共享：参数量为|V|的次线性</li>
<li>Inductive(归纳式的)：<strong>可以为没出现过的node生成embedding</strong>。由于权重矩阵W/B的共享，即使没有出现过的node，也可以为其生成计算图，进而生成embedding</li>
<li><strong>可以为新的graph生成embedding</strong>，前提是new graph中的节点都出现在了old graph中</li>
</ul>
<h1 id="与MLP-CNN-Transformer的异同"><a href="#与MLP-CNN-Transformer的异同" class="headerlink" title="与MLP/CNN/Transformer的异同"></a>与MLP/CNN/Transformer的异同</h1><h2 id="GNN-VS-MLP"><a href="#GNN-VS-MLP" class="headerlink" title="GNN VS MLP"></a>GNN VS MLP</h2><ul>
<li>MLP假设IID，不同样本间独立，因此无需满足排列不变性</li>
<li>Graph数据node间不独立，因此需要满足排列不变性</li>
<li>将node的特征向量作为输入，使用MLP —》 将graph的结构作为MLP的输入 —》 一般一个数据集就一种nn结构</li>
<li>GNN —》 将graph的结构作为nn的结构 —》 一个graph有多种nn结构</li>
</ul>
<h2 id="GNN-VS-CNN"><a href="#GNN-VS-CNN" class="headerlink" title="GNN VS CNN"></a>GNN VS CNN</h2><ul>
<li>GNN: <script type="math/tex">\mathrm{h}_{v}^{(l+1)}=\sigma\left(\mathrm{W}_{l} \sum_{u \in \mathrm{N}(v)} \frac{\mathrm{h}_{u}^{(l)}}{|\mathrm{N}(v)|}+\mathrm{B}_{l} \mathrm{~h}_{v}^{(l)}\right), \forall l \in\{0, \ldots, L-1\}</script></li>
<li>CNN: <script type="math/tex">\mathrm{h}_{v}^{(l+1)}=\sigma\left(\sum_{u \in \mathrm{N}(v) \cup\{v\}} \mathrm{W}_{l}^{u} \mathrm{~h}_{u}^{(l)}\right), \forall l \in\{0, \ldots, L-1\}</script></li>
<li>重写CNN：<script type="math/tex">\mathrm{h}_{v}^{(l+1)}=\sigma\left(\sum_{u \in \mathrm{N}(v)} \mathrm{W}_{l}^{u} \mathrm{~h}_{u}^{(l)}+\mathrm{B}_{l} \mathrm{~h}_{v}^{(l)}\right), \forall l \in\{0, \ldots, L-1\}</script></li>
<li>从邻域选择讲，CNN是邻域确定的特殊GNN</li>
<li>CNN不满足排列不变性，改变像素排列，输出也不同</li>
</ul>
<h2 id="GNN-VS-Transformer"><a href="#GNN-VS-Transformer" class="headerlink" title="GNN VS Transformer"></a>GNN VS Transformer</h2><ul>
<li>Transformer是针对序列数据的，一个序列中两个node互为上下文，因此可以看做是所有n ode都互相连接的graph</li>
</ul>
<h1 id="GNN-Theory"><a href="#GNN-Theory" class="headerlink" title="GNN Theory"></a>GNN Theory</h1><ul>
<li>GNN表达能力的衡量，如何设计表达能力强的GNN</li>
<li>表达能力强的GNN能够为不同的节点生成不同的embedding
</li>
</ul>
<h2 id="GNN的表达能力"><a href="#GNN的表达能力" class="headerlink" title="GNN的表达能力"></a>GNN的表达能力</h2><ul>
<li>GNN的计算图：每个节点的有根子树<img src="/2021/12/22/CS224WLec6/GNN13.png" class="" title="GNN13"></li>
<li>Injective function: 内射函数。f(x)=Y，不同的X能映射为不同的Y，称之为内射函数</li>
<li>如果每一步的neighbor aggregation是内射函数，则GNN可以分辨不同的有根子树。</li>
<li>GCN(mean-pool, ex. MeanPool([1,0],[0,1])与MeanPool([1,0],[0,1],[1,0],[0,1])一样)/GraphSAGE(max-pool)都不是内射函数</li>
<li>Any injective multi-set function can be expressed as: <script type="math/tex">\Phi\left(\sum_{x \in S} f(x)\right)</script>, f和外层φ为非线性函数，<strong>中间是对multi-set做sum</strong>。而非线性函数可以用MLP来建模</li>
</ul>
<h2 id="GIN-Graph-Isomorphism-Network"><a href="#GIN-Graph-Isomorphism-Network" class="headerlink" title="GIN(Graph Isomorphism Network)"></a>GIN(Graph Isomorphism Network)</h2><ul>
<li>THE most expressive GNN in the class of message-passing GNNs</li>
<li>Apply an MLP, element-wise sum, followed by another MLP.<script type="math/tex; mode=display">\mathrm{MLP}_{\Phi}\left(\sum_{x \in S} \operatorname{MLP}_{f}(x)\right)</script></li>
</ul>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="http://web.stanford.edu/class/cs224w/slides/06-GNN1.pdf">ppt Lec6: Graph Neural Networks 1: GNN Model</a></li>
<li><a href="http://web.stanford.edu/class/cs224w/slides/07-GNN2.pdf">ppt Lec7: Graph Neural Networks 2: Design Space</a></li>
<li><a href="http://web.stanford.edu/class/cs224w/slides/08-GNN-application.pdf">ppt Lec8: Applications of Graph Neural Networks</a></li>
<li><a href="http://web.stanford.edu/class/cs224w/slides/09-theory.pdf">ppt Lec9: Theory of Graph Neural Networks</a></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
        <tag>GCN</tag>
        <tag>GNN</tag>
        <tag>GraphSAGE</tag>
        <tag>GAT</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec4-PageRank</title>
    <url>/2021/12/18/CS224WLec4/</url>
    <content><![CDATA[<h1 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h1><ul>
<li>一个node的重要性由指向它的节点的重要性决定 —&gt; <strong>指向该node的节点数越多，且这些节点重要性越高，则该节点越重要</strong>。node j的重要性（传递函数）为：<script type="math/tex; mode=display">r_{j}=\sum_{i \rightarrow j} \frac{r_{i}}{d_{i}}</script><script type="math/tex; mode=display">d_{i} \ldots out-degree of node i</script></li>
<li><p>矩阵形式：</p>
<ul>
<li>随机邻接矩阵M。j有<script type="math/tex">d_{j}</script>个出链，如果j -&gt; i，则<script type="math/tex">M_{ij}=1/d_{j}</script>，因此每一列上的值要么为0，要么为<script type="math/tex">1/d_{j}</script>，且加和为1，称为列随机矩阵</li>
<li>rank vector r: <script type="math/tex">r_{j}</script>为node j的重要性score，且<script type="math/tex">\sum_{i} r_{i}=1</script></li>
<li>n*1 = n*n * n*1:<br><script type="math/tex">\boldsymbol{r}=\boldsymbol{M} \cdot \boldsymbol{r}</script>,</li>
</ul>
<script type="math/tex; mode=display">\quad r_{j}=\sum_{i \rightarrow j} \frac{r_{i}}{d_{i}}</script></li>
<li>PageRank VS RandomWalk: 当random walk到达静态分布状态时满足<script type="math/tex">\boldsymbol{r}=\boldsymbol{M} \cdot \boldsymbol{r}</script>，即<strong>PageRank得到的重要性向量v是random walk的静态分布</strong></li>
<li>PangRank VS Eigenvector: <strong>PageRank得到的重要性向量v是当特征值为1时得到的主特征向量</strong><a id="more"></a>
</li>
</ul>
<h2 id="Solve-the-equation"><a href="#Solve-the-equation" class="headerlink" title="Solve the equation"></a>Solve the equation</h2><ul>
<li>method: power iteration<ul>
<li>initialize: 初始化，给每个node一个page rank值。e.g. <script type="math/tex; mode=display">
\boldsymbol{r}^{0}=[1 / N, \ldots, 1 / N]^{T}</script></li>
<li>iterate: 迭代<script type="math/tex; mode=display">
r_{j}^{(t+1)}=\sum_{i \rightarrow j} \frac{r_{i}^{(t)}}{d_{i}}</script>即：<script type="math/tex; mode=display">
\boldsymbol{r}^{(t+1)}=\boldsymbol{M} \cdot \boldsymbol{r}^{t}</script></li>
<li>stop: 也可以选择其他度量方式<script type="math/tex; mode=display">
\left|\boldsymbol{r}^{(t+1)}-\boldsymbol{r}^{t}\right|_{1}<\varepsilon</script></li>
</ul>
</li>
</ul>
<h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h3><ul>
<li>dead ends: 遇到死胡同 —》 某个节点所在列的M为全0 —》随机矩阵不再随机，秩 &lt; n —》pagerank会收敛为全0<ul>
<li>solution: 填充M中的该列，例如填充为1/n</li>
</ul>
</li>
<li>spider trap: 在某个节点处死循环<ul>
<li>solution: 允许random surfer跳到一个随机page。加一个超参β，表示是否沿着这条link走的概率，1-β表示随机jump的概率 —》相当于改了M</li>
</ul>
</li>
<li>Google solution: teleport rankpage。β表示page j由其他page的出链决定，1-β表示由其他随机page决定<ul>
<li>equation<script type="math/tex; mode=display">
r_{j}=\sum_{i \rightarrow j} \beta \frac{r_{i}}{d_{i}}+(1-\beta) \frac{1}{N}</script></li>
<li>matrix G<script type="math/tex; mode=display">\boldsymbol{G}=\beta {\boldsymbol{M}}+(1-\beta)[1/N]_{N*N}</script></li>
</ul>
</li>
</ul>
<img src="/2021/12/18/CS224WLec4/pagerank1.png" class="" title="pagerank1">
<h1 id="PageRank扩展"><a href="#PageRank扩展" class="headerlink" title="PageRank扩展"></a>PageRank扩展</h1><ul>
<li>Personalized PageRank: teleport到在指定的子图S</li>
<li>Random walks with restarts: teleport到start page<img src="/2021/12/18/CS224WLec4/pagerank2.png" class="" title="pagerank2">
</li>
</ul>
<h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><ul>
<li>无法获得训练集里没有的node的embedding，必须重新训练</li>
<li>无法捕获结构相似性：例如同一张图中的局部结构相似性</li>
<li>无法利用node/edge以及图的特征：例如无法利用节点的其他属性</li>
<li>解决方法：deep learning, GNN</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
        <tag>ML</tag>
        <tag>PageRank</tag>
      </tags>
  </entry>
  <entry>
    <title>CS学习路线、资源推荐</title>
    <url>/2021/12/10/CS%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</url>
    <content><![CDATA[<ul>
<li><a href="https://conanhujinming.github.io/post/how_to_learn_cs/">转载自胡神的“自学CS路线、资源推荐”</a></li>
<li><a href="https://conanhujinming.github.io/comments-for-awesome-courses/">名校公开课程评价网
</a></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>Resource</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-DBSCAN以及sklearn实现DBSCAN</title>
    <url>/2021/12/10/DBSCAN/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]<br><a href="https://www.cnblogs.com/pinard/p/6208966.html">原文1</a><br><a href="https://www.cnblogs.com/pinard/p/6217852.html">原文2</a><br>DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的<strong>密度聚类算法</strong>，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN<strong>既可以适用于凸样本集，也可以适用于非凸样本集</strong>。</p>
<a id="more"></a>
<h1 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h1><p>其原理为：同一类别的样本，其样本分布一定是紧密的；可以将各组紧密相连的样本划分为不同的类别来得到聚类类别结果。</p>
<h1 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h1><h2 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h2><p>参数(ϵ, MinPts)描述领域的样本分布紧密程度，其中ϵ描述了某一样本的领域<strong>距离阈值</strong>，MinPts描述某一样本的距离为ϵ的领域中样本<strong>个数的阈值</strong>。</p>
<p>假设样本集是D=(x1,x2,…,xm),则DBSCAN具体的密度描述定义如下：</p>
<ol>
<li>ϵ-邻域：对于xj∈D，其ϵ-邻域包含样本集D中与xj的距离不大于ϵ的子样本集，即Nϵ(xj)={xi∈D|distance(xi,xj)≤ϵ}, 这个子样本集的个数记为|Nϵ(xj)|</li>
<li>核心对象：对于任一样本xj∈D，如果其ϵ-邻域对应的Nϵ(xj)至少包含MinPts个样本，即如果|Nϵ(xj)|≥MinPts，则xj是核心对象。</li>
<li>密度直达：如果xi位于xj的ϵ-邻域中，且xj是核心对象，则称xi由xj密度直达。</li>
<li>密度可达：对于xi和xj,如果存在样本样本序列p1,p2,…,pT,满足p1=xi,pT=xj, 且pt+1由pt密度直达，则称xj由xi密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本p1,p2,…,pT−1均为核心对象，因为只有核心对象才能使其他样本密度直达。</li>
<li>密度相连：对于xi和xj,如果存在核心对象样本xk，使xi和xj均由xk密度可达，则称xi和xj密度相连。注意密度相连关系是满足对称性的。</li>
</ol>
<p>图中MinPts = 5。红点为核心对象。<br><img src="https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161222112847323-1346197243.png" alt="示意图"></p>
<h2 id="聚类思想"><a href="#聚类思想" class="headerlink" title="聚类思想"></a>聚类思想</h2><p>由密度可达关系导出的最大密度相连的样本集合，即为最终聚类的一个类别。</p>
<p>方法：任意选择一个没有类别的核心对象作为种子，然后找到该核心对象密度可达的样本集合，为一个聚类。接着选择另一个没有类比的核心对象…直到所有核心对象都有类别。</p>
<p>问题：</p>
<ol>
<li>outlier.不在任何一个核心对象周围的点定义为异常样本点或噪声点，不考虑。</li>
<li>距离。少量样本而言，搜索周围样本一般用最近邻的方法；大量样本，可以用KD树，球树等搜索最近邻。</li>
<li>若某样本到两个核心对象的距离都小于ϵ，但这两个核心对象不可达，此时采取先来后到原则，标记其为先聚类的cluster类别。</li>
</ol>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>输入：样本集D=(x1,x2,…,xm)，邻域参数(ϵ,MinPts), 样本距离度量方式</p>
<p>输出： 簇划分C.　</p>
<ol>
<li>初始化核心对象集合Ω=∅, 初始化聚类簇数k=0，初始化未访问样本集合Γ = D,  簇划分C = ∅</li>
<li>对于j=1,2,…m, 按下面的步骤找出所有的核心对象：<ul>
<li>通过距离度量方式，找到样本xj的ϵ-邻域子样本集Nϵ(xj)</li>
<li>如果子样本集样本个数满足|Nϵ(xj)|≥MinPts， 将样本xj加入核心对象样本集合：Ω=Ω∪{xj}</li>
</ul>
</li>
<li>如果核心对象集合Ω=∅，则算法结束，否则转入步骤4.</li>
<li>在核心对象集合Ω中，随机选择一个核心对象o，初始化当前簇核心对象队列Ωcur={o}, 初始化类别序号k=k+1，初始化当前簇样本集合Ck={o}, 更新未访问样本集合Γ=Γ−{o}</li>
<li>如果当前簇核心对象队列Ωcur=∅，则当前聚类簇Ck生成完毕, 更新簇划分C={C1,C2,…,Ck}, 更新核心对象集合Ω=Ω−Ck， 转入步骤3。</li>
<li>在当前簇核心对象队列Ωcur中取出一个核心对象o′,通过邻域距离阈值ϵ找出所有的ϵ-邻域子样本集Nϵ(o′)，令Δ=Nϵ(o′)∩Γ, 更新当前簇样本集合Ck=Ck∪Δ, 更新未访问样本集合Γ=Γ−Δ,  更新Ωcur=Ωcur∪(Δ∩Ω)−o′，转入步骤5.</li>
</ol>
<p>输出结果为： 簇划分C={C1,C2,…,Ck}</p>
<h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><p>对比图：<img src="https://img-blog.csdn.net/20170419143546349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="与其他算法的对比图"><br>一般用于数据集稠密时的情况，或数据集是非凸的。</p>
<p>DBSCAN的主要优点有：</p>
<ol>
<li>可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li>可以在聚类的同时发现异常点，对数据集中的异常点不敏感。</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>
<p>DBSCAN的主要缺点有：</p>
<ol>
<li>如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
<li>如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。</li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。</li>
</ol>
<h1 id="sklearn-cluster-DBSCAN"><a href="#sklearn-cluster-DBSCAN" class="headerlink" title="sklearn.cluster.DBSCAN"></a>sklearn.cluster.DBSCAN</h1><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>按其算法，包括DBSCAN本身的参数，以及求取最近邻时的参数。</p>
<ol>
<li>eps：DBSCAN算法参数，ϵ-邻域的距离阈值。默认值是0.5.eps过大，则更多的点会落在核心对象的ϵ-邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。</li>
<li>min_samples：DBSCAN算法参数，上文的MinPts。默认值是5.通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。</li>
<li>metric：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。可以使用的距离度量参数有：<ul>
<li>欧式距离 “euclidean”</li>
<li>曼哈顿距离 “manhattan”</li>
<li>切比雪夫距离“chebyshev”</li>
<li>闵可夫斯基距离 “minkowski”</li>
<li>带权重闵可夫斯基距离 “wminkowski”</li>
<li>标准化欧式距离 “seuclidean”</li>
<li>马氏距离“mahalanobis”</li>
</ul>
</li>
<li>algorithm：最近邻搜索算法参数，算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现，‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。个人的经验，一般情况使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用”auto”建树时间可能会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。</li>
<li>leaf_size：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 因为这个值一般只影响算法的运行速度和使用内存大小，因此一般情况下可以不管它。</li>
<li>p: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。</li>
<li>n_jobs：使用CPU格式，-1代表全开。</li>
</ol>
<p>输出：</p>
<ul>
<li>core<em>sample_indices</em>:核心样本指数。（此参数在代码中有详细的解释）</li>
<li>labels_:数据集中每个点的集合标签给,噪声点标签为-1。</li>
<li>components_ ：核心样本的副本</li>
</ul>
<p>主要是<strong>eps和min_samples</strong>的调参。</p>
<h2 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h2><p>原文2中有。<br><a href="https://www.cnblogs.com/pinard/p/6217852.html">原文2</a></p>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Clustering</tag>
        <tag>DBSCAN</tag>
      </tags>
  </entry>
  <entry>
    <title>CV-目标检测综述</title>
    <url>/2021/12/10/CV-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul>
<li>图像识别+定位</li>
<li>识别：分类问题，准确率</li>
<li>定位：分类/回归问题，找到一个框/4个坐标，IOU</li>
</ul>
<h2 id="传统目标检测"><a href="#传统目标检测" class="headerlink" title="传统目标检测"></a>传统目标检测</h2><h3 id="用回归做定位问题"><a href="#用回归做定位问题" class="headerlink" title="用回归做定位问题"></a>用回归做定位问题</h3><ul>
<li>训练一个cnn网络，在最后一个卷积层后分两个head，一个head做分类，另一个回归</li>
<li>先fine tuning分类任务，再fine tuning回归</li>
<li>缺点：回归问题，很难做</li>
</ul>
<h3 id="用分类做定位问题"><a href="#用分类做定位问题" class="headerlink" title="用分类做定位问题"></a>用分类做定位问题</h3><ul>
<li>滑动窗口（选择不同位置不同大小的区域），对其定位，对每个框内的图像做分类</li>
<li>缺点：窗口冗余、复杂度高、多物体多分类时复杂度更高</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DL</tag>
        <tag>CV</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-GBDT &amp; XGBoost</title>
    <url>/2021/12/10/GBDT%20&amp;%20XGBoost/</url>
    <content><![CDATA[<p>[TOC]</p>
<p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Doc</a><br><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">Slides</a></p>
<a id="more"></a>
<h1 id="为何要推导出目标函数而不是直接增加树"><a href="#为何要推导出目标函数而不是直接增加树" class="headerlink" title="为何要推导出目标函数而不是直接增加树"></a>为何要推导出目标函数而不是直接增加树</h1><p><img src="http://i.imgur.com/quPhp1K.png" alt="Objective function"></p>
<ul>
<li>理论上：搞清楚learning的目的，以及其收敛性。</li>
<li>工程上：<ul>
<li>gi和hi是对loss function的一次、二次导</li>
<li>目标函数以及整个学习过程只依赖于gi和hi</li>
<li>可以根据实际问题，自定义loss function</li>
</ul>
</li>
</ul>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><img src="http://i.imgur.com/L7PhJwO.png" alt="Summary"></p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><script type="math/tex; mode=display">\text{obj} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i)</script><p>l为loss，\ \Omega \ 为正则项</p>
<ul>
<li>loss：采用加法策略，第t颗树时：<script type="math/tex; mode=display">\hat{y}_i^{(0)} = 0</script><script type="math/tex; mode=display">\hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)</script><script type="math/tex; mode=display">\hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)</script><script type="math/tex; mode=display">\dots</script><script type="math/tex; mode=display">\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)</script>在添加第t颗树时，需要优化的目标函数为：<script type="math/tex; mode=display">\sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)</script>其中h和f：<script type="math/tex; mode=display">g_i = \partial_{\hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)})</script><script type="math/tex; mode=display">h_i = \partial_{\hat{y}_i^{(t-1)}}^2 l(y_i, \hat{y}_i^{(t-1)})</script>note: 是对谁的导</li>
<li>正则项：复杂度：<script type="math/tex; mode=display">\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2</script>其中w是叶子上的score vector，T是叶子数量</li>
</ul>
<h2 id="DART-Booster"><a href="#DART-Booster" class="headerlink" title="DART Booster"></a>DART Booster</h2><p>为了解决过拟合，会随机drop trees:</p>
<ul>
<li>训练速度可能慢于gbtree</li>
<li>由于随机性，早停可能不稳定</li>
</ul>
<h1 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h1><h2 id="Monotonic-Constraints单调性限制"><a href="#Monotonic-Constraints单调性限制" class="headerlink" title="Monotonic Constraints单调性限制"></a>Monotonic Constraints单调性限制</h2><ul>
<li><p>一个可选特性:<br>会限制模型的结果按照某个特征 单调的进行增减</p>
<p>也就是说可以降低模型对数据的敏感度，如果明确已知某个特征与预测结果呈单调关系时，那在生成模型的时候就会跟特征数据的单调性有关。</p>
</li>
</ul>
<h2 id="Feature-Interaction-Constraints单调性限制"><a href="#Feature-Interaction-Constraints单调性限制" class="headerlink" title="Feature Interaction Constraints单调性限制"></a>Feature Interaction Constraints单调性限制</h2><ul>
<li><p>一个可选特性：<br>不用时，在tree生成的时候，一棵树上的节点会无限制地选用多个特征</p>
<p>设置此特性时，可以规定，哪些特征可以有interaction（一般独立变量之间可以interaction，非独立变量的话可能会引入噪声）</p>
</li>
<li>好处：<ul>
<li>预测时更小的噪声</li>
<li>对模型更好地控制</li>
</ul>
</li>
</ul>
<h2 id="Instance-Weight-File"><a href="#Instance-Weight-File" class="headerlink" title="Instance Weight File"></a>Instance Weight File</h2><ul>
<li>规定了模型训练时data中每一条instance的权重</li>
<li>有些instance质量较差，或与前一示例相比变化不大，所以可以调节其所占权重</li>
</ul>
<h1 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h1><h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><p>与overfitting有关的参数：</p>
<ul>
<li>直接控制模型复杂度：max_depth, min_child_weight and gamma.</li>
<li>增加模型随机性以使得模型对噪声有更强的鲁棒性：<ul>
<li>subsample and colsample_bytree. </li>
<li>Reduce stepsize eta. Remember to increase num_round when you do so.</li>
</ul>
</li>
</ul>
<h2 id="Imbalanced-Dataset"><a href="#Imbalanced-Dataset" class="headerlink" title="Imbalanced Dataset"></a>Imbalanced Dataset</h2><ul>
<li>只关注测量指标的大小<ul>
<li>平衡数据集 via scale_pos_weight</li>
<li>使用AUC作为metric</li>
</ul>
</li>
<li>关注预测正确的概率<ul>
<li>此时不能re-balance数据集</li>
<li>Set parameter max_delta_step to a finite number (say 1) to help convergence</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-GBDT</title>
    <url>/2021/12/10/GBDT/</url>
    <content><![CDATA[<p>[TOC]</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul>
<li>GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型,所以说，在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。</li>
<li>在GradientBoosting算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。</li>
<li>GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此<strong>GBDT的树都是CART回归树</strong>，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。</li>
</ul>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ul>
<li>GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显，1、它能灵活的处理各种类型的数据；2、在相对较少的调参时间下，预测的准确度较高。</li>
<li>当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph-与图有关的资源</title>
    <url>/2021/12/22/Graph/</url>
    <content><![CDATA[<h2 id="OpenResource"><a href="#OpenResource" class="headerlink" title="OpenResource"></a>OpenResource</h2><ul>
<li><a href="https://ogb.stanford.edu/">OGB</a>: The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs.</li>
<li><a href="https://github.com/snap-stanford/GraphGym">GraphGym</a>: GraphGym is a platform for designing and evaluating Graph Neural Networks. —&gt; 和pytorch配合</li>
<li><a href="https://github.com/deepmind/graph_nets">GraphNets</a>: 配合tensorflow</li>
</ul>
<h2 id="Class"><a href="#Class" class="headerlink" title="Class"></a>Class</h2><ul>
<li><a href="http://web.stanford.edu/class/cs224w/">CS224W: Machine Learning with Graphs</a></li>
</ul>
<h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h2><ul>
<li>Tutorials and overviews:<ul>
<li>Relational inductive biases and graph networks (Battaglia et al., 2018)</li>
<li>Representation learning on graphs: Methods and applications (Hamilton et al., 2017)</li>
</ul>
</li>
<li>Attention-based neighborhood aggregation:<ul>
<li>Graph attention networks (Hoshen, 2017; Velickovic et al., 2018; Liu et al., 2018)</li>
</ul>
</li>
<li>Embedding entire graphs:<ul>
<li>Graph neural nets with edge embeddings (Battaglia et al., 2016; Gilmer et. al., 2017)</li>
<li>Embedding entire graphs (Duvenaud et al., 2015; Dai et al., 2016; Li et al., 2018) and graph pooling (Ying et al., 2018, Zhang et al., 2018)</li>
<li>Graph generation and relational inference (You et al., 2018; Kipf et al., 2018)</li>
<li>How powerful are graph neural networks(Xu et al., 2017)</li>
</ul>
</li>
<li>Embedding nodes:<ul>
<li>Varying neighborhood: Jumping knowledge networks (Xu et al., 2018), GeniePath (Liu et al., 2018)</li>
<li>Position-aware GNN (You et al. 2019)</li>
</ul>
</li>
<li>Spectral approaches to graph neural networks:<ul>
<li>Spectral graph CNN &amp; ChebNet (Bruna et al., 2015; Defferrard et al., 2016)</li>
<li>Geometric deep learning (Bronstein et al., 2017; Monti et al., 2017)</li>
</ul>
</li>
<li>Other GNN techniques:<ul>
<li>Pre-training Graph Neural Networks (Hu et al., 2019)</li>
<li>GNNExplainer: Generating Explanations for Graph Neural Networks (Ying et al., 2019)</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>ML</tag>
        <tag>Resource</tag>
      </tags>
  </entry>
  <entry>
    <title>GraphEmbedding-LINE</title>
    <url>/2021/12/20/GraphEmbedding-LINE/</url>
    <content><![CDATA[<p>paper: <a href="https://arxiv.org/pdf/1503.03578.pdf">LINE: Large-scale Information Network Embedding</a></p>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><ul>
<li>包含一阶信息（直接相连的两节点的邻近度）和二阶信息（一对顶点的邻近度用临近节点的相似性衡量）的GE学习模型：<strong>只用到了一阶邻点</strong></li>
<li>DeepWalk/node2vec: 基于高阶相似度的GE学习模型</li>
</ul>
<a id="more"></a>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><ul>
<li>边权重矩阵W：损失函数计算时用</li>
<li>节点权重矩阵：节点采样时用</li>
</ul>
<h2 id="一阶相似度"><a href="#一阶相似度" class="headerlink" title="一阶相似度"></a>一阶相似度</h2><ul>
<li><strong>只针对无向图，不支持有向图</strong></li>
<li>边(u,v)的权重Wuv表示u和v之间的一阶相似性，如果在u和v之间无边，它们的一阶相似性为0。</li>
<li>经验分布<script type="math/tex">\hat{p}_{1}(i, j)=\frac{w_{i j}}{W}</script>，这里W是W矩阵的模</li>
<li>预测分布 <script type="math/tex">p_{1}\left(v_{i}, v_{j}\right)=\frac{1}{1+\exp \left(-\vec{u}_{i}^{T} \cdot \vec{u}_{j}\right)}</script></li>
<li>目标函数 <script type="math/tex">O_{1}=d\left(\hat{p}_{1}(\cdot, \cdot), p_{1}(\cdot, \cdot)\right)</script>, d为距离函数，例如当d为KL散度且忽略常数项时（同交叉熵）：<script type="math/tex; mode=display">
O_{1}=-\sum_{(i, j) \in E} w_{i j} \log p_{1}\left(v_{i}, v_{j}\right)</script></li>
</ul>
<h2 id="二阶相似度"><a href="#二阶相似度" class="headerlink" title="二阶相似度"></a>二阶相似度</h2><ul>
<li>如果没有同样的相邻节点，二阶相似性为0</li>
<li>经验分布 <script type="math/tex">\hat{p}_{2}\left(v_{j} \mid v_{i}\right)=\frac{w_{i j}}{d_{i}}</script>，di是节点i的带权出度，<script type="math/tex">w_{i j}</script>是边(i,j)的权重，<script type="math/tex">d_{i}=\sum_{k \in N(i)} w_{i k}</script></li>
<li>预测分布 <script type="math/tex">p_{2}\left(v_{j} \mid v_{i}\right)=\frac{\exp \left(\vec{u}_{j}^{\prime T} \cdot \vec{u}_{i}\right)}{\sum_{k=1}^{|V|} \exp \left(\vec{u}_{k}^{\prime T} \cdot \vec{u}_{i}\right)}</script></li>
<li>目标函数 <script type="math/tex">O_{2}=\sum_{i \in V} \lambda_{i} d\left(\hat{p}_{2}\left(\cdot \mid v_{i}\right), p_{2}\left(\cdot \mid v_{i}\right)\right)</script>，这里<script type="math/tex">\lambda_{i}</script>为对节点i的加权，一般取 <script type="math/tex">\lambda_{i}</script> 为 <script type="math/tex">d_{i}</script>，并取KL散度为距离函数时：<script type="math/tex">O_{2}=-\sum_{(i, j) \in E} w_{i j} \log p_{2}\left(v_{j} \mid v_{i}\right)</script></li>
</ul>
<h2 id="结合一阶和二阶"><a href="#结合一阶和二阶" class="headerlink" title="结合一阶和二阶"></a>结合一阶和二阶</h2><ul>
<li>method1: 训完一阶训二阶，并将两者embedding concat</li>
<li>method2: 一起训</li>
</ul>
<h2 id="其他损失函数"><a href="#其他损失函数" class="headerlink" title="其他损失函数"></a>其他损失函数</h2><ul>
<li>pair loss: 用learn2rank的loss</li>
</ul>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="二阶相似度计算的负采样"><a href="#二阶相似度计算的负采样" class="headerlink" title="二阶相似度计算的负采样"></a>二阶相似度计算的负采样</h3><ul>
<li>对softmax的优化：负采样，<script type="math/tex">\sigma</script>为sigmoid函数，<script type="math/tex">P_{n}(v) \propto d_{v}^{3 / 4}</script>利用出度构成的节点权重做采样，与w2v分布一致<script type="math/tex; mode=display">
\log \sigma\left(\vec{u}_{j}^{\prime T} \cdot \vec{u}_{i}\right)+\sum_{i=1}^{K} E_{v_{n} \sim P_{n}(v)}\left[\log \sigma\left(-\vec{u}_{n}^{\prime T} \cdot \vec{u}_{i}\right)\right]</script></li>
</ul>
<h3 id="边采样"><a href="#边采样" class="headerlink" title="边采样"></a>边采样</h3><ul>
<li>O2的梯度与边权重wij有关，若学习率的设定由小权重决定，则遇到大权重时会梯度爆炸；反之则会梯度消失<script type="math/tex; mode=display">
\frac{\partial O_{2}}{\partial \vec{u}_{i}}=w_{i j} \cdot \frac{\partial \log p_{2}\left(v_{j} \mid v_{i}\right)}{\partial \vec{u}_{i}}</script></li>
<li>简单的优化方式：将权重为w的edge变为w条二进制edge —&gt; oom —&gt; edge sampling</li>
<li>采样算法：alias算法，可以达到O(1)复杂度</li>
</ul>
<h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><ul>
<li>新节点的GE（新节点）：<ul>
<li>新节点和已有节点相连：优化如下目标函数 <script type="math/tex">-\sum_{j \in N(i)} w_{j i} \log p_{1}\left(v_{j}, v_{i}\right), \text { or }-\sum_{j \in N(i)} w_{j i} \log p_{2}\left(v_{j} \mid v_{i}\right)</script></li>
<li>新节点不和已有节点相连：文中没给 —》利用side info</li>
</ul>
</li>
<li>低度数顶点（孤岛节点）：对于一些顶点由于其邻接点非常少会导致embedding向量的学习不充分，论文提到可以利用邻居的邻居构造样本进行学习，这里也暴露出LINE方法仅考虑一阶和二阶相似性，对高阶信息的利用不足。</li>
</ul>
<h1 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h1><ul>
<li>边权重矩阵和节点权重矩阵（pagerank/出度/centrality/clustering coefficient/…）的设计</li>
<li>不同的loss: KL/cross-entropy/rankloss/…</li>
<li>采样算法</li>
</ul>
<h1 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h1><ul>
<li>效率高</li>
<li>只用了一跳信息</li>
<li>对孤岛节点和新节点没有好的处理</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>LINE</tag>
      </tags>
  </entry>
  <entry>
    <title>Java学习资源</title>
    <url>/2021/12/10/Java%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90/</url>
    <content><![CDATA[<h2 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h2><ul>
<li><a href="https://github.com/apachecn/thinking-in-java-zh">Java编程思想</a></li>
<li><a href="https://github.com/Chenzk1/PrivateNotes/blob/master/books/Head%20First%20Java%28%E7%AC%AC2%E7%89%88%29.pdf">Head First Java</a></li>
<li><a href="https://github.com/Chenzk1/PrivateNotes/blob/master/books/Java%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%20%E5%8D%B71%20%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%20%E5%8E%9F%E4%B9%A6%E7%AC%AC9%E7%89%88%20%E5%AE%8C%E6%95%B4%E4%B8%AD%E6%96%87%E7%89%88%20.pdf">Java核心技术卷1</a></li>
</ul>
<h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><ul>
<li><a href="https://www.liaoxuefeng.com/wiki/1252599548343744">廖雪峰的Java教程</a></li>
</ul>
<h2 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h2><ul>
<li><a href="https://segmentfault.com/a/1190000014933213">Java学习的四个阶段</a></li>
<li><a href="https://juejin.cn/post/6844903985170612238">可能是最适合你的 Java 学习路线和方法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/115890802">2020年最新Java学习路线图</a></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>Resource</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Decision Tree</title>
    <url>/2021/12/10/ML-Decision%20Tree/</url>
    <content><![CDATA[<p>[TOC]</p>
<ol>
<li>决策树</li>
</ol>
<p><strong>问题：如何挑选用于分裂节点的特征—&gt;ID3 C4.5 …(一个标准：使分裂出来的节点尽可能纯，即一个分支尽可能属于同类)</strong></p>
<a id="more"></a>
<ol>
<li><p>ID3<br><strong>信息增益</strong></p>
<p> 信息增益 = 信息熵 - 条件熵</p>
<ul>
<li>信息增益：针对每个 <strong>属性</strong></li>
<li>信息熵：整个样本空间的不确定度。其中Pk一定是label取值的概率。</li>
<li><p>条件熵：给定某个属性，求其信息熵</p>
<p>—&gt; 问题：某属性所包括的类别越多，信息增益越大。极限：每个类别仅有1个实例（label数量为1），log p = log1 = 0， 所以最终条件熵=0。或：属性类别越多，条件熵越小，其纯度越高。</p>
<p>—&gt; 信息增益准则其实是对可取值数目较多的属性有所偏好！</p>
<p>—&gt; 泛化能力不强</p>
</li>
</ul>
</li>
<li><p>C4.5 <strong>信息增益率+信息增益</strong></p>
<p> 属性a的信息增益率 = 属性a的信息增益 / a的某个固有统计量IV(a)</p>
<p> <img src="https://pic4.zhimg.com/80/v2-812104c0291d20935e910919a9fa5c27_hd.png" alt="IV(a)公式"></p>
<p> V为a的取值数目。<br> （实际上是属性a的信息熵）</p>
<ul>
<li>直接使用信息增益率：偏好取值数目小的属性。</li>
<li>先选择高于平均水平信息增益的属性，再选择最高信息增益率的属性。</li>
</ul>
</li>
<li><p>CART <strong>基尼系数+MAE/MSE</strong></p>
<p>与ID3、C4.5的不同：形成二叉树，因此 —&gt; 既要确定要分割的属性，也要确定要分割的值</p>
<ul>
<li><p>回归树：MAE/MSE</p>
<ul>
<li>example(MSE)：<blockquote>
<ol>
<li>考虑数据集 D 上的所有特征 j，遍历每一个特征下所有可能的取值或者切分点 s，将数据集 D 划分成两部分 D1 和 D2</li>
<li>分别计算上述两个子集的平方误差和，选择最小的平方误差对应的特征与分割点，生成两个子节点。</li>
<li>对上述两个子节点递归调用步骤1 2,直到满足停止条件。</li>
</ol>
</blockquote>
</li>
</ul>
</li>
<li><p>分类树：(Gini不纯度)</p>
<p><img src="https://img-blog.csdn.net/20150109184544578?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQW5kcm9pZGx1c2hhbmdkZXJlbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="某属性A的基尼不纯度"><br>基尼不纯度越小，纯度越高</p>
</li>
</ul>
<blockquote>
<ol>
<li>对每个特征 A，对它的所有可能取值 a，将数据集分为 A＝a，和 A!＝a 两个子集，计算集合 D 的基尼指数：<br>Gini(A) = D1/D <em> Gini(D1) + D2/D </em> Gini(D2)</li>
<li>遍历所有的特征 A，计算其所有可能取值 a 的基尼指数，选择 D 的基尼指数最小值对应的特征及切分点作为最优的划分，将数据分为两个子集。</li>
<li>对上述两个子节点递归调用步骤1 2, 直到满足停止条件。</li>
<li>生成 CART 决策树。</li>
</ol>
</blockquote>
<pre><code>停止条件有：
1. 节点中的样本个数小于预定阈值;
2. 样本集的Gini系数小于预定阈值（此时样本基本属于同一类）;
3. 没有更多特征。
</code></pre><ul>
<li>剪枝</li>
<li>例子：<a href="https://www.jianshu.com/p/b90a9ce05b28">example</a></li>
</ul>
</li>
<li><p>控制决策树过拟合的方法</p>
<ul>
<li>剪枝</li>
<li>控制终止条件，避免树形结构过细</li>
<li>构建随机森林</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DecisionTree</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-LightGBM</title>
    <url>/2021/12/10/LightGBM/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>解决GBDT遇到海量数据时的问题：GBDT 在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。</p>
<a id="more"></a>
<h3 id="对xgboost的优化"><a href="#对xgboost的优化" class="headerlink" title="对xgboost的优化"></a>对xgboost的优化</h3><ul>
<li>基于 Histogram 的决策树算法</li>
<li>带深度限制的 Leaf-wise 的叶子生长策略</li>
<li>直方图做差加速</li>
<li>直接支持类别特征(Categorical Feature)</li>
<li>Cache 命中率优化</li>
<li>基于直方图的稀疏特征优化</li>
<li>多线程优化</li>
</ul>
<h4 id="基于-Histogram-的决策树算法"><a href="#基于-Histogram-的决策树算法" class="headerlink" title="基于 Histogram 的决策树算法"></a>基于 Histogram 的决策树算法</h4><ul>
<li>做法：把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</li>
<li>优点<ul>
<li>内存消耗小：不需要保存既保存数据的特征值，又保存预排序结果，只保存k个key-values即可</li>
<li>时间消耗小：只需要计算k次增益</li>
<li>泛化性能增强：决策树是弱模型，分割点精确与否不是很重要，更粗的分割点还可以引入噪音，增强泛化性能，防止过拟合</li>
</ul>
</li>
</ul>
<h4 id="带深度限制的-Leaf-wise-的叶子生长策略"><a href="#带深度限制的-Leaf-wise-的叶子生长策略" class="headerlink" title="带深度限制的 Leaf-wise 的叶子生长策略"></a>带深度限制的 Leaf-wise 的叶子生长策略</h4><ul>
<li>Level-wise 过一次数据可以<strong>同时分裂同一层的叶子</strong>，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</li>
<li>Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到<strong>分裂增益最大的一个叶子</strong>，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</li>
</ul>
<h4 id="直方图加速"><a href="#直方图加速" class="headerlink" title="直方图加速"></a>直方图加速</h4><ul>
<li>一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图。加速一倍。</li>
</ul>
<h4 id="接受categorical-features"><a href="#接受categorical-features" class="headerlink" title="接受categorical features"></a>接受categorical features</h4><p>LightGBM 可以直接使用 categorical features（分类特征）作为 input（输入）. 它不需要被转换成 one-hot coding（独热编码）, 并且它比 one-hot coding（独热编码）更快（约快上 8 倍）。</p>
<h4 id="并行学习"><a href="#并行学习" class="headerlink" title="并行学习"></a>并行学习</h4><p>目前支持特征并行和数据并行的两种。</p>
<ul>
<li>特征并行：主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。</li>
<li>数据并行：让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。</li>
</ul>
<h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><pre><code>params = &#123;  
    &#39;boosting_type&#39;: &#39;gbdt&#39;,  
    &#39;objective&#39;: &#39;binary&#39;,  
    &#39;metric&#39;: &#123;&#39;binary_logloss&#39;, &#39;auc&#39;&#125;,  #二进制对数损失
    &#39;num_leaves&#39;: 5,  
    &#39;max_depth&#39;: 6,  
    &#39;min_data_in_leaf&#39;: 450,  
    &#39;learning_rate&#39;: 0.1,  
    &#39;feature_fraction&#39;: 0.9,  
    &#39;bagging_fraction&#39;: 0.95,  
    &#39;bagging_freq&#39;: 5,  
    &#39;lambda_l1&#39;: 1,    
    &#39;lambda_l2&#39;: 0.001,  # 越小l2正则程度越高  
    &#39;min_gain_to_split&#39;: 0.2,  
    &#39;verbose&#39;: 5,  
    &#39;is_unbalance&#39;: True  
&#125;  
</code></pre>]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>LightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Metrics</title>
    <url>/2021/12/10/ML-metrics/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]</p>
<h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><h2 id="ACC-Accuracy"><a href="#ACC-Accuracy" class="headerlink" title="ACC(Accuracy)"></a>ACC(Accuracy)</h2><ul>
<li>Accuracy = 预测正确的样本数量 / 总样本数量 = (TP+TN) / (TP+FP+TN+FN)</li>
</ul>
<a id="more"></a>
<h2 id="Precision查准"><a href="#Precision查准" class="headerlink" title="Precision查准"></a>Precision查准</h2><ul>
<li>Precision = TP / (TP+FP)</li>
</ul>
<h2 id="Recall-TPR查全"><a href="#Recall-TPR查全" class="headerlink" title="Recall/TPR查全"></a>Recall/TPR查全</h2><ul>
<li>Recall = TPR = TP / (TP+FN)</li>
</ul>
<h2 id="F1-amp-Fn"><a href="#F1-amp-Fn" class="headerlink" title="F1&amp;Fn"></a>F1&amp;Fn</h2><h3 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h3><ul>
<li>precision和recall的调和平均</li>
</ul>
<h3 id="Fn"><a href="#Fn" class="headerlink" title="Fn"></a>Fn</h3><ul>
<li>可用来解决分类不均衡问题/对precision和recall进行强调<ul>
<li>如F1认为precision和recall同等重要，F2认为recall的重要程度是precision的2倍<script type="math/tex; mode=display">
F_{\beta}=\left(1+\beta^{2}\right) \cdot \frac{\text { precision } \cdot \text { recall }}{\left(\beta^{2} \cdot \text { precision }\right)+\text { recall }}</script></li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
F_{\beta}=\frac{\left(1+\beta^{2}\right) \cdot \text { true positive }}{\left(1+\beta^{2}\right) \cdot \text { true positive }+\beta^{2} \cdot \text { false negative }+\text { false positive }}</script><h2 id="P-R曲线-Precision-Recall"><a href="#P-R曲线-Precision-Recall" class="headerlink" title="P-R曲线(Precision-Recall)"></a>P-R曲线(Precision-Recall)</h2><ul>
<li>P为纵轴，R为横轴</li>
<li>主要关心正例</li>
<li>公式：<script type="math/tex; mode=display">
\sum_{n}\left(R_{n}-R_{n-1}\right) P_{n}</script></li>
</ul>
<h2 id="FPR"><a href="#FPR" class="headerlink" title="FPR"></a>FPR</h2><ul>
<li>FPR = FP/(FP+TN) 真实label为0的样本中，被预测为1的样本占的比例</li>
</ul>
<h2 id="ROC曲线-amp-AUC"><a href="#ROC曲线-amp-AUC" class="headerlink" title="ROC曲线&amp;AUC"></a>ROC曲线&amp;AUC</h2><ul>
<li>衡量分类准确性，同时考虑了模型对正样本(TPR)和负样本(FPR)的分类能力，因此在样本非均衡的情况下也能做出合理的评价。侧重于<strong>排序</strong>。</li>
</ul>
<h3 id="ROC-Receiver-operating-characteristic-curve"><a href="#ROC-Receiver-operating-characteristic-curve" class="headerlink" title="ROC(Receiver operating characteristic curve)"></a>ROC(Receiver operating characteristic curve)</h3><ul>
<li>横轴FPR，纵轴TPR</li>
<li>ROC曲线在绘制时，需要先对所有样本的预测概率做排序，并不断取不同的阈值计算TPR和FPR，因此AUC在计算时会侧重于排序。</li>
</ul>
<h3 id="AUC-Area-under-Curve"><a href="#AUC-Area-under-Curve" class="headerlink" title="AUC(Area under Curve)"></a>AUC(Area under Curve)</h3><ul>
<li>ROC曲线与x轴的面积</li>
<li>一般认为：AUC最小值=0.5（其实存在AUC小于0.5的情况，例如每次都预测与真实值相反的情况，但是这种情况，只要把预测值取反就可以得到大于0.5的值，因此还是认为AUC最小值=0.5）</li>
<li>AUC的物理意义为<strong>任取一对例和负例，正例得分大于负例得分的概率</strong>，AUC越大，表明方法效果越好。—&gt; 排序<script type="math/tex; mode=display">A U C=\frac{\sum p r e d_{p o s}>p r e d_{n e g}}{p o s i t i v e N u m * n e g a ti v e N u m}</script>分母是正负样本总的组合数，分子是正样本大于负样本的组合数<script type="math/tex; mode=display">
A U C=\frac{\sum_{\text {ins}_{i} \in \text {positiveclass}} \operatorname{rank}_{\text {ins}_{i}}-\frac{M \times(M+1)}{2}}{M \times N}</script></li>
</ul>
<h1 id="区分度"><a href="#区分度" class="headerlink" title="区分度"></a>区分度</h1><h2 id="KS-Kolmogorov-Smirnov"><a href="#KS-Kolmogorov-Smirnov" class="headerlink" title="KS(Kolmogorov-Smirnov)"></a>KS(Kolmogorov-Smirnov)</h2><ul>
<li>KS用于模型风险区分能力进行评估，指标衡量的是好坏样本累计分布之间的差值。好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。</li>
<li>KS的计算步骤如下： <ul>
<li>1)计算每个评分区间的好坏账户数。 </li>
<li>2)计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 </li>
<li>3)计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。</li>
</ul>
</li>
<li>低分段累计坏百分比应高于累计好百分比，之后会经历两者差距先扩大再缩小的变化<br><img src="https://img-blog.csdnimg.cn/2019013111150139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NzY2NfbGVhcm5pbmc=,size_16,color_FFFFFF,t_70" alt="KS-曲线"></li>
</ul>
<h2 id="gini"><a href="#gini" class="headerlink" title="gini"></a>gini</h2><ul>
<li>计算每个评分区间的好坏账户数。 </li>
<li>计算每个评分区间的累计好账户数占总好账户数比率（累计good%）和累计坏账户数占总坏账户数比率(累计bad%)。 </li>
<li>按照累计好账户占比和累计坏账户占比得出下图所示曲线ADC。 </li>
<li>计算出图中阴影部分面积，阴影面积占直角三角形ABC面积的百分比，即为GINI系数。</li>
</ul>
<h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><ul>
<li><a href="http://note.youdao.com/s/D64Y5VaG">http://note.youdao.com/s/D64Y5VaG</a></li>
<li>map和ndcg都是<strong>属于per item的评估</strong>，即逐条对搜索结果进行分等级的打分，并计算其指标。</li>
<li><strong>基于precision的指标，例如MAP，每个条目的值是的评价用0或1表示；而DCG可以使用多值指标来评价</strong>。</li>
<li><strong>基于precision的指标，天然考虑了排序信息</strong>。</li>
</ul>
<h2 id="map-k"><a href="#map-k" class="headerlink" title="map@k"></a>map@k</h2><h3 id="prec-k"><a href="#prec-k" class="headerlink" title="prec@k"></a>prec@k</h3><ul>
<li>k指到第k个正确的召回结果，precision的值<script type="math/tex; mode=display">P @ k(\pi, l)=\frac{\left.\sum_{t \leq k} I_{\left\{l_{\pi}-1_{(t)}\right.}=1\right\}}{k}</script><ul>
<li>这里$\pi$代表documents list，即推送结果列。$I$是指示函数，$\pi^{(-1)}(t)$代表排在位置$t$处的document的标签（相关为1，否则为0）。这一项可以理解为前k个documents中，标签为1的documents个数与k的比值。</li>
</ul>
</li>
<li>只能表示单点的策略效果</li>
</ul>
<h3 id="ap-k"><a href="#ap-k" class="headerlink" title="ap@k"></a>ap@k</h3><script type="math/tex; mode=display">
\mathrm{AP}(\pi, l)=\frac{\left.\sum_{k=1}^{m} P @ k \cdot I_{\left\{l_{\pi}-1(k)\right.}=1\right\}}{m_{1}}</script><ul>
<li>其中$m_1$代表与该query相关的document的数量（即真实标签为1），$m$则代表模型找出的前$m$个documents，本例中 [公式] ，并假设 [公式] ，即真正和query相关的document有6个。（但是被模型找出来的7个doc中仅仅有3个标签为1，说明这个模型召回并不怎么样）</li>
<li>@1到@k的precision的平均</li>
</ul>
<h3 id="map-k-1"><a href="#map-k-1" class="headerlink" title="map@k"></a>map@k</h3><ul>
<li>多个query的ap@k平均</li>
</ul>
<h2 id="ndcg"><a href="#ndcg" class="headerlink" title="ndcg"></a>ndcg</h2><ul>
<li>基于CG的评价指标允许我们使用多值评价一个item：例如对一个结果可评价为Good（好）、Fair（一般）、Bad（差），然后可以赋予为3、2、1.</li>
</ul>
<h3 id="cg-Cumulative-Gain"><a href="#cg-Cumulative-Gain" class="headerlink" title="cg(Cumulative Gain)"></a>cg(Cumulative Gain)</h3><script type="math/tex; mode=display">
\mathrm{CG}_{\mathrm{p}}=\sum_{i=1}^{p} r e l_{i}</script><ul>
<li>CG是在这个查询输出结果里面所有的结果的等级对应的得分的总和。如一个输出结果页面有P个结果.</li>
</ul>
<h3 id="dcg-Discounted-Cumulative-Gain"><a href="#dcg-Discounted-Cumulative-Gain" class="headerlink" title="dcg(Discounted Cumulative Gain)"></a>dcg(Discounted Cumulative Gain)</h3><script type="math/tex; mode=display">
\mathrm{DCG}_{\mathrm{p}}=\sum_{i=1}^{p} \frac{r e l_{i}}{\log _{2}(i+1)}</script><ul>
<li>取对数是因为：根据大量的用户点击与其所点内容的位置信息，模拟出一条衰减的曲线。</li>
</ul>
<h3 id="ndcg-normalize-DCG"><a href="#ndcg-normalize-DCG" class="headerlink" title="ndcg(normalize DCG)"></a>ndcg(normalize DCG)</h3><script type="math/tex; mode=display">nDCG = \frac{DCG}{IDCG}</script><ul>
<li>IDCG（Ideal DCG）就是理想的DCG。</li>
<li>IDCG如何计算？首先要拿到搜索的结果，然后对这些结果进行排序，排到最好的状态后，算出这个排列下的DCG，就是iDCG。因此nDCG是一个0-1的值，nDCG越靠近1，说明策略效果越好，或者说只要nDCG&lt;1，策略就存在优化调整空间。因为nDCG是一个相对比值，那么不同的搜索结果之间就可以通过比较nDCG来决定哪个排序比较好。</li>
</ul>
<h2 id="MRR-Mean-Reciprocal-Rank"><a href="#MRR-Mean-Reciprocal-Rank" class="headerlink" title="MRR(Mean Reciprocal Rank)"></a>MRR(Mean Reciprocal Rank)</h2><script type="math/tex; mode=display">
\operatorname{MRR}=\frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\operatorname{rank}_{i}}</script><ul>
<li>其中|Q|是查询个数，$rank_i$是第i个查询，第一个相关的结果所在的排列位置</li>
</ul>
<h1 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h1><ul>
<li>评价一个检测算法时，主要看两个指标<ul>
<li>是否正确的预测了框内物体的类别</li>
<li>预测的框和人工标注框的重合程度</li>
</ul>
</li>
</ul>
<h2 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h2><ul>
<li>求取每个<strong>类别</strong>的ap的平均值<ul>
<li>即，对每个类别进行预测，得到其预测值的排序，求ap</li>
<li>求map</li>
</ul>
</li>
</ul>
<h2 id="IOU-Interest-Over-Union"><a href="#IOU-Interest-Over-Union" class="headerlink" title="IOU(Interest Over Union)"></a>IOU(Interest Over Union)</h2><h3 id="检测里的IOU"><a href="#检测里的IOU" class="headerlink" title="检测里的IOU"></a>检测里的IOU</h3><ul>
<li>框的IOU<script type="math/tex; mode=display">IOU = \frac{ {DetectionResult}\cap{GroundTruth}} { {DetectionResult}\cup{GroundTruth} }</script><img src="https://pic3.zhimg.com/80/v2-99faeb1f9876f11a32f90263ff1cafba_1440w.jpg" alt="示意"></li>
</ul>
<h3 id="语义分割里的IOU"><a href="#语义分割里的IOU" class="headerlink" title="语义分割里的IOU"></a>语义分割里的IOU</h3><ul>
<li>像素集合的IOU<script type="math/tex; mode=display">
\begin{array}{c}
I O U=\frac{p_{i i}}{\sum_{j=0}^{k} p_{i j}+\sum_{j=0}^{k} p_{j i}-p_{i i}} \\
I o U=\frac{T P}{F N+F P+T P}
\end{array}</script></li>
<li>$p<em>{ij}$表示真实值为i，被预测为j的数量， K+1是类别个数（包含空类）。$p</em>{ii}$是真正的数量。$p<em>{ij}$、$p</em>{ji}$则分别表示假正和假负。</li>
</ul>
<h2 id="mIOU-mean-IOU"><a href="#mIOU-mean-IOU" class="headerlink" title="mIOU(mean IOU)"></a>mIOU(mean IOU)</h2><ul>
<li><a href="https://blog.csdn.net/baidu_27643275/article/details/90445422">blog</a></li>
<li>用于语义分割<script type="math/tex; mode=display">
\begin{array}{c}
M I O U=\frac{1}{k+1} \sum_{i=0}^{k} \frac{p_{i i}}{\sum_{j=0}^{k} p_{i j}+\sum_{j=0}^{k} p_{j i}-p_{i i}} \\
M I o U=\frac{1}{k+1} \sum_{i=0}^{k} \frac{T P}{F N+F P+T P}
\end{array}</script></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>CV</tag>
        <tag>Metrics</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-信息熵&amp;交叉熵&amp;条件熵&amp;相对熵(KL散度)&amp;互信息</title>
    <url>/2021/12/10/ML-%E4%BF%A1%E6%81%AF%E7%86%B5&amp;%E4%BA%A4%E5%8F%89%E7%86%B5&amp;%E6%9D%A1%E4%BB%B6%E7%86%B5&amp;%E7%9B%B8%E5%AF%B9%E7%86%B5(KL%E6%95%A3%E5%BA%A6)&amp;%E4%BA%92%E4%BF%A1%E6%81%AF/</url>
    <content><![CDATA[<ul>
<li>设随机变量X，有n个事件$x_i$ –&gt; $x_n$，概率分布为$p(x)$</li>
</ul>
<a id="more"></a>
<h2 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h2><ul>
<li>某随机变量X取值为xi的信息为 $I(X=xi)=log_2( 1 / p(x_i) ) = - log_2p(x_i)$</li>
<li>某事件xi的信息代表这个事件能提供的信息，一个发生概率越小的事件能够提供的信息量越大。</li>
</ul>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><ul>
<li>信息代表一个事件的不确定性，信息熵是整个随机变量X不确定性的度量：信息的期望。<script type="math/tex; mode=display">H(X) = \sum_0^n p(x_i)*I(x_i) = - \sum_0^n p(x_i)\log_2(p(x_i))</script></li>
<li>信息熵只与变量X的分布有关，与其取值无关。例如二分类中，两取值的概率均为0.5时，其熵最大，也最难预测某时刻哪一类别会发生。</li>
<li><a href="https://www.zhihu.com/question/41252833/answer/195901726">如何通俗的解释交叉熵与相对熵?</a></li>
<li>对于一个系统而言，若获知其真实分布，则我们能够找到一个最优策略，以最小的代价来消除系统的不确定性，而这个最小的代价（猜题次数、编码长度等）就是信息熵。</li>
</ul>
<h2 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h2><ul>
<li>给定条件X下，Y的分布（Y|X）的熵对X的数学期望：$H(Y|X)=\sum_x p(x)H(Y|X=x)$, 即给定条件分布下的信息熵对某个条件的数学期望</li>
<li>在ML中，即选定某个特征X(X有n类)后，label(Y)的条件概率熵求期望：给定X特征的条件下Y的信息熵。</li>
<li>条件熵越小，代表在这个特征下，label的信息熵越小，也就是说要解决问题的代价越小。</li>
</ul>
<h2 id="信息增益-ID3"><a href="#信息增益-ID3" class="headerlink" title="信息增益 - ID3"></a>信息增益 - ID3</h2><ul>
<li>$IG(Y|X)=H(Y)-H(Y|X)$ 信息熵-条件熵</li>
<li>在决策树中作为选择特征的指标，IG越大，这个特征的选择性越好，也可以理解为：待分类的集合的熵和选定某个特征的条件熵之差越大，这个特征对整个集合的影响越大。</li>
<li>对于条件熵来说，条件熵越小，分类后的纯度越高，但是问题是：X的取值越多，每个取值下Y的纯度越高，H(Y|X)越小，但此时并不有利于Y的区分。信息增益也是如此。–&gt; 信息增益率。</li>
</ul>
<h2 id="信息增益率-信息增益比-—-C4-5"><a href="#信息增益率-信息增益比-—-C4-5" class="headerlink" title="信息增益率/信息增益比 — C4.5"></a>信息增益率/信息增益比 — C4.5</h2><ul>
<li>信息增益 / 条件的信息熵</li>
<li>偏好取值少的特征。C4.5：先选择高于平均水平信息增益的特征，再在其中选择最高信息增益率的特征。见Decision Tree</li>
</ul>
<h2 id="基尼不纯度-—-CART"><a href="#基尼不纯度-—-CART" class="headerlink" title="基尼不纯度 — CART"></a>基尼不纯度 — CART</h2><ul>
<li>GINI：先对特征分区，对每个区中的label求gini系数，pk(1-pk)；然后对每个区的基尼系数求期望</li>
<li>表示数据的不纯度。既有分类也有回归，既要确定特征，也要确定特征的分叉值。</li>
<li>见<a href="https://chenzk1.github.io/2020/10/25/ML-Decision%20Tree/">Decision Tree</a></li>
</ul>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><ul>
<li>前面提到：信息熵是最优策略下，消除系统不确定性的最小代价。这里的前提是：我们得到了系统的真实分布。</li>
<li>实际中，一般难以获知系统真实分布，所以要以假设分布去近似。交叉熵：用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。$CEH(p,q)=\sum_{k=1}^np_k\log_2\frac{1}{q_k}$，注意这里log中是q，是基于非真实分布q的信息量对真实分布的期望。<br>当假设分布$q_k$与真实分布$p_k$相同时，交叉熵最低，等于信息熵，所以得到的策略为最优策略。</li>
<li>在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。<ul>
<li>例如：在逻辑斯蒂回归或者神经网络中都有用到交叉熵作为评价指标，其中p即为真实分布的概率，而q为预测的分布，以此衡量两不同分布的相似性。</li>
</ul>
</li>
<li>如何衡量不同策略的差异：相对熵</li>
</ul>
<h2 id="相对熵-K-L散度"><a href="#相对熵-K-L散度" class="headerlink" title="相对熵/K-L散度"></a>相对熵/K-L散度</h2><ul>
<li>用来衡量两个概率分布之间的差异。两者相同相对熵为0</li>
<li>使用非真实分布q的交叉熵，与使用真实分布p的的信息熵的差值：相对熵，又称K-L散度。</li>
<li>KL(p, q) = 交叉熵(p, q) - 信息熵(p)<br>$KL(p,q)=CEH(p,q)-H(p)=\sum_{i=1}^np(x_i)\log\frac{p(x_i)}{q(x_i)}$</li>
</ul>
<h2 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h2><ul>
<li>H(X,Y) 随机变量X,Y联合表示的信息熵</li>
</ul>
<h2 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h2><ul>
<li>H（X；Y）俩变量交集，也记作I(X;Y)</li>
<li>H（X；Y) = H(X,Y)-H(Y|X)-H(X|Y)</li>
<li>I(X;Y)=KL(P(X,Y), P(X)P(Y))</li>
<li>互信息越小，两变量独立性越强，P(X,Y)与P(X)P(Y)差异越小，P(X,Y)与P(X)P(Y)的相对熵越小</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>信息论</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Naive Bayes及其sklearn实现</title>
    <url>/2021/12/10/Naive%20Bayes/</url>
    <content><![CDATA[<p>[TOC]</p>
<p>P(B|A) = P(A|B)*P(B)/P(A)</p>
<p>朴素：特征之间相互独立</p>
<h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><ol>
<li>x = {a1, a2, …, am}为待分类项，a是特征。</li>
<li>类别集合C = {y1, …, yn}.</li>
<li>计算P(y1|x), P(y2|x) …</li>
<li>P(yk|x) = max{P(yi|x)}，则x属于yk类</li>
</ol>
<a id="more"></a>
<p><strong>总结：</strong>某类在待分类项出现的条件下的概率是所有类中最大的，这个分类项就属于这一类。</p>
<p>e.g.判断一个黑人来自哪个洲，求取每个洲黑人的比率，非洲最高，选非洲。</p>
<p>其中x = {a1, a2, …, am}，即P(C|a1,a2…) = P(C)*P(a1,a2,…|C)/P(a1,a2…)。posterior = prior * likelihood / evidence, 这里evidence是常数，不影响。</p>
<p>——-&gt;求解P(C) * P(a1,a2,a3…|C)</p>
<p>——-&gt;链式法则：P(C) * P(a2,a3…|C, a1) * P(a1|C)</p>
<p>—-&gt; …</p>
<p>—-&gt; P(C) * P(a1|C) * P(a2|C, a1) * P(a3|C, a1, a2)…<br>由于特征之间的相互独立性，a2发生于a1无关，转化为</p>
<p>—-&gt; P(C) * P(a1|C) * P(a2|C) …  * P(am|C)</p>
<p>——-&gt;问题转化为求取条件概率：</p>
<ol>
<li>找到一个已知分类的待分类项集合，这个集合叫做训练样本集。</li>
<li>统计得到在各类别下各个特征属性的条件概率估计。</li>
</ol>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>NaiveBayes</tag>
      </tags>
  </entry>
  <entry>
    <title>P、NP、NP-hard问题</title>
    <url>/2021/12/15/P&amp;NP&amp;NP-hard/</url>
    <content><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><ul>
<li>P Problem: <strong>任意性</strong>，对于<strong>任意的</strong>输入规模n，问题都可以在n的多项式时间内得到解决；</li>
<li>NP(Non-deterministic Polynomial) Problem: <strong>存在性</strong>，<strong>可以</strong>在多项式的时间里验证一个解的问题；</li>
<li>NPC(Non-deterministic Polynomial Complete) Problem: 满足两个条件 (1)是一个NP问题 (2)所有的NP问题都可以约化到它。可以理解为<strong>NP的泛化问题</strong>。</li>
<li>NP-Hard Problem: 满足NPC问题的第二条，但不一定要满足第一条 —&gt; <strong>不一定可以在多项式时间内解决的问题</strong></li>
</ul>
<h1 id="搞笑版P-NP证明"><a href="#搞笑版P-NP证明" class="headerlink" title="搞笑版P=NP证明"></a>搞笑版P=NP证明</h1><blockquote>
<p>反证法。设P = NP。令y为一个P = NP的证明。证明y可以用一个合格的计算机科学家在多项式时间内验证，我们认定这样的科学家的存在性为真。但是，因为P = NP，该证明y可以在多项式时间内由这样的科学家发现。但是这样的发现还没有发生（虽然这样的科学家试图发现这样的一个证明），我们得到了矛盾。</p>
</blockquote>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Python-面向对象</title>
    <url>/2021/12/10/Python-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
    <content><![CDATA[<h3 id="属性命名"><a href="#属性命名" class="headerlink" title="属性命名"></a>属性命名</h3><ul>
<li>属性以双下划线开头，类内变量，实例无法访问。但可以通过某些方式访问，例如Student例中定义了__name变量，可以用_Student_name来实现访问，但不建议，因为不同的解释器的转化方式不一样。</li>
<li>单下划线可以打开，但需要注意不能随意更改。</li>
<li>双下划线结尾与开头，特殊变量，类内可以访问，实例不知。</li>
</ul>
<a id="more"></a>
<h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3><p>开闭原则：定义一个类Animal及其多个之类Dog/Cat/…，当定义一个函数或操作时：</p>
<ul>
<li>对扩展开放：允许新增Animal的子类；</li>
<li>对修改封闭：不需要修改依赖Animal类型的run_twice()等函数，仍然可以传入Dog/Cat等类。<br>事实上，不需要继承也可以实现多态————鸭子类型。</li>
</ul>
<h3 id="若干方法"><a href="#若干方法" class="headerlink" title="若干方法"></a>若干方法</h3><ul>
<li>isinstance(object,class) 判断是否属于某个类</li>
<li>dir() 列举出一个对象的属性和方法</li>
<li>getattr()、setattr()、hasattr()可以获得、添加、查询是否需要某个属性<ul>
<li>__slots__ 限制可以添加的属性，__slots__ = (‘name’, ‘age’) # 用tuple定义允许绑定的属性名称</li>
</ul>
</li>
<li>装饰器</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python-sort&amp;sorted</title>
    <url>/2021/12/10/Python-sort&amp;sorted%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="不同"><a href="#不同" class="headerlink" title="不同"></a>不同</h2><ul>
<li>sorted<ul>
<li>返回已排序<strong>列表</strong></li>
<li>built-in函数，接受任何可迭代对象</li>
</ul>
</li>
<li>sort<ul>
<li>原位排序，返回None</li>
<li>是list的成员函数，因此只接受list</li>
</ul>
</li>
</ul>
<h2 id="相同"><a href="#相同" class="headerlink" title="相同"></a>相同</h2><ul>
<li>稳定排序</li>
</ul>
<a id="more"></a>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li>key：接受一个参数，返回用于排序的<strong>键</strong><ul>
<li>operator模块函数：一些常用key函数的封装<ul>
<li>itemgetter()：适用tuple/list的list</li>
<li>attrgetter()：适用dict/object的list</li>
<li>methodcaller()</li>
</ul>
</li>
</ul>
</li>
<li>reverse</li>
</ul>
<h3 id="自定义排序规则"><a href="#自定义排序规则" class="headerlink" title="自定义排序规则"></a>自定义排序规则</h3><h4 id="Python2-cmp函数"><a href="#Python2-cmp函数" class="headerlink" title="Python2: cmp函数"></a>Python2: cmp函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cmp</span>(<span class="params">a, b</span>):</span></span><br><span class="line">  <span class="comment"># 如果逻辑上认为 a &lt; b ，返回 -1</span></span><br><span class="line">  <span class="comment"># 如果逻辑上认为 a &gt; b , 返回 1</span></span><br><span class="line">  <span class="comment"># 如果逻辑上认为 a == b, 返回 0 </span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">a = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">a = <span class="built_in">sorted</span>(a, cmp=cmp)</span><br></pre></td></tr></table></figure>
<h4 id="python3：无cmp，只存key。两种自定义排序规则的做法。"><a href="#python3：无cmp，只存key。两种自定义排序规则的做法。" class="headerlink" title="python3：无cmp，只存key。两种自定义排序规则的做法。"></a>python3：无cmp，只存key。两种自定义排序规则的做法。</h4><ul>
<li>做法1：利用functools module里封装的cmp_to_key</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cmp_to_key</span>(<span class="params">mycmp</span>):</span></span><br><span class="line">    <span class="string">&#x27;Convert a cmp= function into a key= function&#x27;</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">K</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, obj, *args</span>):</span></span><br><span class="line">            self.obj = obj</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) &lt; <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__gt__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) &gt; <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) == <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__le__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) &lt;= <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__ge__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) &gt;= <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__ne__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) != <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> K</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cmp</span>(<span class="params">a, b</span>):</span></span><br><span class="line">    <span class="keyword">if</span> b &lt; a:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">if</span> a &lt; b:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br><span class="line">print(<span class="built_in">sorted</span>(a, key=functools.cmp_to_key(cmp)))</span><br></pre></td></tr></table></figure>
<ul>
<li>做法2：自己实现cmp类</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LargerNumKey</span>(<span class="params"><span class="built_in">str</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span>(<span class="params">x, y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x+y &gt; y+x</span><br><span class="line"></span><br><span class="line">largest_num = <span class="string">&#x27;&#x27;</span>.join(<span class="built_in">sorted</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, nums), key=LargerNumKey))</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-XGBoost</title>
    <url>/2021/12/10/XGBoost/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[toc]<br><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Doc</a><br><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">Slides</a></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p><img src="http://i.imgur.com/L7PhJwO.png" alt="Summary"><br><img src="https://img-blog.csdnimg.cn/2019031119461897.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI0ODUyNDM5,size_16,color_FFFFFF,t_70" alt="Summary"></p>
<a id="more"></a>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul>
<li>每轮生成新的树，该树的生成标准为：损失函数最小化。损失函数代表的意义：保证模型复杂度越低的同时，使预测误差尽可能小。其中模型复杂度包括树的个数以及叶子数值尽可能不极端。<em>（这个怎么看，如果某个样本label数值为4，那么第一个回归树预测3，第二个预测为1；另外一组回归树，一个预测2，一个预测2，那么倾向后一种，为什么呢？前一种情况，第一棵树学的太多，太接近4，也就意味着有较大的过拟合的风险）</em></li>
<li>每一轮产生新的树的时候，其loss是本轮加之前模型与y的差，然后通过泰勒展开做成<strong>t-1轮损失函数对于t-1轮模型即上一轮残差</strong>的一阶导和二阶导与当前树及其平方的乘积</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><script type="math/tex; mode=display">\text{obj} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i)</script><p>其中l为loss，l为二次则利用二次优化，不是二次则用泰勒展开；Omega为正则项。</p>
<ul>
<li><p>loss：采用加法策略，第t颗树时：</p>
<script type="math/tex; mode=display">\hat{y}_i^{(0)} = 0</script><script type="math/tex; mode=display">\hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)</script><script type="math/tex; mode=display">\dots</script><script type="math/tex; mode=display">\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)</script><p>所以在添加第t颗树时，需要优化的目标函数为：</p>
<script type="math/tex; mode=display">l(y_i, \hat{y}_i^{(t)}) = l(y_i, \hat{y}_i^{(t-1)} + f_{t-1}(x_i)) = \sum_{i=1}^n [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)</script><p>其中g和h分别为t-1轮损失函数对于t-1轮模型的一阶导和二阶导：</p>
<script type="math/tex; mode=display">g_i = \partial_{\hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)})</script><script type="math/tex; mode=display">h_i = \partial_{\hat{y}_i^{(t-1)}}^2 l(y_i, \hat{y}_i^{(t-1)})</script><p>note: 是对谁的导</p>
<p>此处为了简化目标函数，用到了泰勒二阶展开：<br><img src="https://img-blog.csdnimg.cn/20190311165107976.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI0ODUyNDM5,size_16,color_FFFFFF,t_70" alt="Summary"><br>loss第一项是真实值与t-1轮预测结果的loss，对t轮不明显，因此删去。所以对于一个模型来说，在每一次优化时，先定义好loss函数，然后计算每一轮loss的一阶导和二阶导即可写出loss，优化即可（每个特征求最佳分裂点，使用最小的特征）。</p>
</li>
<li>正则项：复杂度：<script type="math/tex; mode=display">\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2</script>其中w是叶子上的score vector，T是叶子数量</li>
</ul>
<h3 id="损失函数求解"><a href="#损失函数求解" class="headerlink" title="损失函数求解"></a>损失函数求解</h3><ul>
<li>最佳树结构（特征和分裂值）以及叶子节点的预测分数都是通过优化目标函数来得出的，其中叶子节点的预测分数是正则项的一部分；但其实分裂时是通过增益最大化来寻找最佳分裂特征和分裂值的，只有w是通过计算目标函数得出的<br><img src="https://upload-images.jianshu.io/upload_images/1371984-364c3b6e258cc671.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/782/format/webp" alt="Summary"><br>求出各个叶子节点的最佳值以及此时目标函数的值:<br><img src="https://upload-images.jianshu.io/upload_images/1371984-3b24159e2a85b4a3.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/365/format/webp" alt="Summary"></li>
</ul>
<h3 id="分裂"><a href="#分裂" class="headerlink" title="分裂"></a>分裂</h3><ul>
<li>分裂方式<ul>
<li>枚举所有树结构的贪心法（先特征，再分裂点）：<ul>
<li>首先，对所有特征都按照特征的数值进行预排序。</li>
<li>其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。</li>
<li>最后，找到一个特征的分割点后，将数据分裂成左右子节点。</li>
</ul>
</li>
<li>优缺点：<ul>
<li>优点：精确找到分裂点</li>
<li>缺点：<strong>空间消耗大</strong>（保存数据的特征值，还保存了特征排序的结果，即2<em>数据大小）、<strong>时间开销大</strong>（遍历每一个分割点的时候，都需要进行分裂增益的计算）、<em>*cache优化不友好</em></em>（预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对 cache 进行优化）</li>
</ul>
</li>
</ul>
</li>
<li>分裂的标准：<strong>增益最大化</strong>，每次节点分裂，loss function被影响的只有这个节点的样本，因而每次分裂，计算分裂的<strong>增益</strong>（loss function的降低量）只需要关注打算分裂的那个节点的样本<br><img src="https://upload-images.jianshu.io/upload_images/1371984-d0a9c89dbbc34f7c.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/544/format/webp" alt="Summary"></li>
<li>终止条件<ul>
<li>树的最大数量</li>
<li>max_depth</li>
<li>最小增益：当引入的分裂带来的增益小于一个阀值的时候，我们可以剪掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思</li>
<li>当样本权重和小于设定阈值时则停止建树，这个解释一下，涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样，大意就是一个叶子节点样本太少了终止</li>
</ul>
</li>
</ul>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ul>
<li>每个树的每个节点上的预测值相加<br><img src="https://upload-images.jianshu.io/upload_images/1371984-bbe17b3b253a6d1a.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/901/format/webp" alt="Summary"></li>
</ul>
<h2 id="DART-Booster"><a href="#DART-Booster" class="headerlink" title="DART Booster"></a>DART Booster</h2><p>为了解决过拟合，会随机drop trees:</p>
<ul>
<li>训练速度可能慢于gbtree</li>
<li>由于随机性，早停可能不稳定</li>
</ul>
<h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul>
<li>算法：二阶导、正则化、自定义loss、缺失数据处理、支持类别特征、早停</li>
<li>工程：行采样、列采样、</li>
</ul>
<h3 id="正则化-amp-行采样-amp-列采样-amp-早停"><a href="#正则化-amp-行采样-amp-列采样-amp-早停" class="headerlink" title="正则化&amp;行采样&amp;列采样&amp;早停"></a>正则化&amp;行采样&amp;列采样&amp;早停</h3><ul>
<li>通过最优化求出w，而不是平均值或者多数表决</li>
<li>防止过拟合</li>
</ul>
<h3 id="支持自定义loss"><a href="#支持自定义loss" class="headerlink" title="支持自定义loss"></a>支持自定义loss</h3><h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><h3 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h3><ul>
<li><strong>特征间并行</strong>：由于将数据按列存储，可以同时访问所有列，那么可以对所有属性同时执行split finding算法，从而并行化split finding（切分点寻找）</li>
<li><strong>特征内并行</strong>：可以用多个block(Multiple blocks)分别存储不同的样本集，多个block可以并行计算-特征内并行</li>
</ul>
<h3 id="Monotonic-Constraints单调性限制"><a href="#Monotonic-Constraints单调性限制" class="headerlink" title="Monotonic Constraints单调性限制"></a>Monotonic Constraints单调性限制</h3><ul>
<li><p>一个可选特性:<br>会限制模型的结果按照某个特征 单调的进行增减</p>
<p>也就是说可以降低模型对数据的敏感度，如果明确已知某个特征与预测结果呈单调关系时，那在生成模型的时候就会跟特征数据的单调性有关。</p>
</li>
</ul>
<h3 id="Feature-Interaction-Constraints单调性限制"><a href="#Feature-Interaction-Constraints单调性限制" class="headerlink" title="Feature Interaction Constraints单调性限制"></a>Feature Interaction Constraints单调性限制</h3><ul>
<li><p>一个可选特性：<br>不用时，在tree生成的时候，一棵树上的节点会无限制地选用多个特征</p>
<p>设置此特性时，可以规定，哪些特征可以有interaction（一般独立变量之间可以interaction，非独立变量的话可能会引入噪声）</p>
</li>
<li>好处：<ul>
<li>预测时更小的噪声</li>
<li>对模型更好地控制</li>
</ul>
</li>
</ul>
<h3 id="Instance-Weight-File"><a href="#Instance-Weight-File" class="headerlink" title="Instance Weight File"></a>Instance Weight File</h3><ul>
<li>规定了模型训练时data中每一条instance的权重</li>
<li>有些instance质量较差，或与前一示例相比变化不大，所以可以调节其所占权重</li>
</ul>
<h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><ul>
<li>通用参数：宏观函数控制。booster/slient/nthread</li>
<li>Booster参数：控制每一步的booster(tree/regression)。</li>
<li>学习目标参数：控制训练目标的表现。</li>
</ul>
<h3 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h3><p>与overfitting有关的参数：</p>
<ul>
<li>学习率</li>
<li>直接控制模型复杂度：max_depth, min_child_weight, gamma, lambda, alpha, max_leaf_nodes</li>
<li>增加模型随机性以使得模型对噪声有更强的鲁棒性：<ul>
<li>subsample and colsample_bytree. </li>
</ul>
</li>
</ul>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ul>
<li>选择较高的学习速率(learning rate)。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。</li>
<li>对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数。</li>
<li>xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。</li>
<li>降低学习速率，确定理想参数。</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>健身Basics</title>
    <url>/2022/01/16/bodyfitting/</url>
    <content><![CDATA[<h1 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h1><ul>
<li>水平对照：<a href="https://strengthlevel.com/strength-standards">https://strengthlevel.com/strength-standards</a></li>
</ul>
<h1 id="Guides"><a href="#Guides" class="headerlink" title="Guides"></a>Guides</h1><ul>
<li><a href="https://stronglifts.com/#guides">https://stronglifts.com/#guides</a></li>
</ul>
<a id="more"></a>
]]></content>
      <categories>
        <category>BodyFitness</category>
      </categories>
      <tags>
        <tag>Resource</tag>
        <tag>健身</tag>
      </tags>
  </entry>
  <entry>
    <title>CUDA介绍以及耗时分析</title>
    <url>/2021/12/28/cuda-intros/</url>
    <content><![CDATA[<ul>
<li><a href="https://www.cnblogs.com/1024incn/p/4537177.html">https://www.cnblogs.com/1024incn/p/4537177.html</a>)</li>
<li><a href="https://developer.download.nvidia.cn/CUDA/training/StreamsAndConcurrencyWebinar.pdf">https://developer.download.nvidia.cn/CUDA/training/StreamsAndConcurrencyWebinar.pdf</a><a id="more"></a>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>git操作整理</title>
    <url>/2021/12/10/git-git%E6%93%8D%E4%BD%9C%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<ul>
<li>git log: 记录版本历史</li>
<li>git reflog：记录操作历史</li>
</ul>
<h2 id="远程库"><a href="#远程库" class="headerlink" title="远程库"></a>远程库</h2><ul>
<li>git remote add <remote name> xxx 其中<remote name>是git对远程库的命名，origin是默认叫法。这个命名是本地对远程库的一个命名。   </li>
<li>git remote -v 查看远程库</li>
<li>git remote rm <remote name></li>
</ul>
<h2 id="工作区、缓存区"><a href="#工作区、缓存区" class="headerlink" title="工作区、缓存区"></a>工作区、缓存区</h2><ul>
<li>stage：缓存，add后缓存区和工作区文件一致，commit后缓存区清空，进入下一个版本</li>
</ul>
<h3 id="diff"><a href="#diff" class="headerlink" title="diff"></a>diff</h3><ul>
<li>工作区 ↔ 缓存区：git diff，当前修改对比上次add都有啥，git add后当前工作区的文件就不能通过git diff查到了</li>
<li>缓存区 ↔ 版本库(HEAD)：git diff —cached 当前add的文件和上个版本的diff</li>
<li>工作区 ↔ 库(HEAD)：git diff HEAD — filename</li>
<li>库 ↔ 库：git diff 243550a 24bc01b filename     #较旧的id 较新的id</li>
</ul>
<h2 id="文件修改撤销"><a href="#文件修改撤销" class="headerlink" title="文件修改撤销"></a>文件修改撤销</h2><h3 id="文件修改撤销（老版本git）"><a href="#文件修改撤销（老版本git）" class="headerlink" title="文件修改撤销（老版本git）"></a>文件修改撤销（老版本git）</h3><h4 id="checkout-—（撤销工作区修改）"><a href="#checkout-—（撤销工作区修改）" class="headerlink" title="checkout —（撤销工作区修改）"></a>checkout —（撤销工作区修改）</h4><ul>
<li>不加—可能变成切换分支</li>
<li>文件回到最近一次git commit或git add时的状态。<ul>
<li>修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态（git commit）；</li>
<li>已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态（git add）。</li>
</ul>
</li>
</ul>
<h4 id="reset-HEAD-（撤销暂存区修改）"><a href="#reset-HEAD-（撤销暂存区修改）" class="headerlink" title="reset HEAD （撤销暂存区修改）"></a>reset HEAD <fileName>（撤销暂存区修改）</h4><ul>
<li><strong>把暂存区的文件回退到工作区</strong></li>
<li>再利用git checkout —就可以删除修改了</li>
<li>或者直接用git checkout HEAD <fileName> 直接从HEAD恢复文件</li>
</ul>
<h3 id="文件修改撤销（新版本git）"><a href="#文件修改撤销（新版本git）" class="headerlink" title="文件修改撤销（新版本git）"></a>文件修改撤销（新版本git）</h3><ul>
<li>撤销工作区的修改：git restore <fileName></li>
<li>撤销工作区和暂存区的修改，恢复到HEAD：git resotre —worktree <fileName></li>
<li>撤销暂存区的修改，前提是上次add后该文件未做其他修改：git restore —staged <fileName></li>
<li>从master同时恢复工作区和暂存区：git restore —source=HEAD —staged —worktree <fileName></li>
</ul>
<h2 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h2><h3 id="回退"><a href="#回退" class="headerlink" title="回退"></a>回退</h3><ul>
<li>HEAD始终指向当前分支的最新commit，HEAD^ 上一个commit，HEAD~100 上一百个commit</li>
<li>若想reset更新的commit，需要知道版本号 git reset [版本号]</li>
</ul>
<h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><ul>
<li>分支的切换：HEAD一直指向当前分支最新commit，切换分支时，HEAD指向另一个分支</li>
<li>git branch: list all branchs</li>
<li>git branch &lt;&gt;: create branch</li>
<li>git switch &lt;&gt; / git checkout &lt;&gt;：切换</li>
<li>git switch -c &lt;&gt; / git checkout -b &lt;&gt;: 创建并切换</li>
<li>git branch -d &lt;&gt;: 删除</li>
<li>git merge <new branch>: 合并new branch到当前分支<ul>
<li>无冲突：fast forward</li>
<li>有冲突：解决冲突，commit。git log —graph可以看分支合并图</li>
</ul>
</li>
<li>git rebase:把本地未push的分叉提交历史整理成直线<br><a href="https://blog.csdn.net/liuxiaoheng1992/article/details/79108233">rebase 和 merge</a></li>
</ul>
<h3 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h3><ul>
<li>fast forward：不会留下commit信息</li>
<li>—no-ff：会留下commit信息，用法：git merge —no-ff -m “merge with no-ff” dev</li>
</ul>
<h3 id="工作现场保护-amp-Bug分支"><a href="#工作现场保护-amp-Bug分支" class="headerlink" title="工作现场保护&amp;Bug分支"></a>工作现场保护&amp;Bug分支</h3><p><a href="https://www.liaoxuefeng.com/wiki/896043488029600/900388704535136">https://www.liaoxuefeng.com/wiki/896043488029600/900388704535136</a></p>
<ul>
<li>修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除；</li>
<li>当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场；</li>
<li>在master分支上修复的bug，想要合并到当前dev分支，可以用git cherry-pick <commit>命令，把bug提交的修改“复制”到当前分支，避免重复劳动。</li>
</ul>
<h3 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h3><ul>
<li>试图用git push origin <your-branch>推送自己的修改；</li>
<li>如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并；<ul>
<li>如果合并有冲突，则解决冲突，并在本地提交；</li>
<li>如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch —set-upstream-to=origin/<origin-branch> <your-branch></li>
</ul>
</li>
<li>git push origin <your-branch></li>
</ul>
<p>或者：</p>
<ul>
<li>git push -u <remote> <branch>，其中u为upstream。git push -u origin branch：把本地branch分支push到origin/branch</li>
<li>git push —set-upstream <remote> <branch></li>
</ul>
<h2 id="ignore"><a href="#ignore" class="headerlink" title="ignore"></a>ignore</h2><ul>
<li><a href="https://github.com/github/gitignore">部分规则</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Kaggle-Kaggle相关</title>
    <url>/2021/12/10/kaggle%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="如何在-Kaggle-首战中进入前-10"><a href="#如何在-Kaggle-首战中进入前-10" class="headerlink" title="如何在 Kaggle 首战中进入前 10%"></a>如何在 Kaggle 首战中进入前 10%</h1><p><a href="https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/">原文</a></p>
<a id="more"></a>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><h3 id="Exploration-Data-Analysis-EDA"><a href="#Exploration-Data-Analysis-EDA" class="headerlink" title="Exploration Data Analysis(EDA)"></a>Exploration Data Analysis(EDA)</h3><h4 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h4><p>matplotlib + seaborn</p>
<ul>
<li>查看目标变量的分布。当分布不平衡时，根据评分标准和具体模型的使用不同，可能会严重影响性能。</li>
<li>对 Numerical Variable，可以用 Box Plot 来直观地查看它的分布。</li>
<li>对于坐标类数据，可以用 Scatter Plot 来查看它们的分布趋势和是否有离群点的存在。</li>
<li>对于分类问题，将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助。</li>
<li>绘制变量之间两两的分布和相关度图表。</li>
</ul>
<p><a href="https://www.kaggle.com/benhamner/python-data-visualizations">example_visualization</a></p>
<h4 id="Statistical-Tests"><a href="#Statistical-Tests" class="headerlink" title="Statistical Tests"></a>Statistical Tests</h4><p>可视化为定性，这里专注于定量，例如对于新创造的特征，可以将其加入原模型当中，看结果的变化。</p>
<p>在某些比赛中，由于数据分布比较奇葩或是噪声过强，Public LB(Leader board)的分数可能会跟 Local CV(Cross Validation)的结果相去甚远。可以根据一些统计测试的结果来粗略地建立一个阈值，用来衡量一次分数的提高究竟是实质的提高还是由于数据的随机性导致的。</p>
<h3 id="Data-Preprossing"><a href="#Data-Preprossing" class="headerlink" title="Data Preprossing"></a>Data Preprossing</h3><p>处理策略主要依赖于EDA中得到的结论。</p>
<ul>
<li>有时数据会分散在几个不同的文件中，需要 Join 起来。</li>
<li>处理 Missing Data。</li>
<li>处理 Outlier。</li>
<li>必要时转换某些 Categorical Variable 的表示方式。例如应用one-hot encoding(pd.get_dummies)将categorical variable转化为数字变量。</li>
<li>有些 Float 变量可能是从未知的 Int 变量转换得到的，这个过程中发生精度损失会在数据中产生不必要的 Noise，即两个数值原本是相同的却在小数点后某一位开始有不同。这对 Model 可能会产生很负面的影响，需要设法去除或者减弱 Noise。</li>
</ul>
<h3 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h3><h4 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h4><p>总的来说，应该<strong>生成尽量多的 Feature，相信 Model 能够挑出最有用的 Feature</strong>。但有时先做一遍 Feature Selection 也能带来一些好处：</p>
<ul>
<li>Feature 越少，训练越快。</li>
<li>有些 Feature 之间可能存在线性关系，影响 Model 的性能。</li>
<li>通过挑选出最重要的 Feature，可以将它们之间进行各种运算和操作的结果作为新的 Feature，可能带来意外的提高。</li>
<li>Feature Selection 最实用的方法也就是看 Random Forest 训练完以后得到的 Feature Importance 了。其他有一些更复杂的算法在理论上更加 Robust，但是缺乏实用高效的实现。从原理上来讲，增加 Random Forest 中树的数量可以在一定程度上加强其对于 Noisy Data 的 Robustness。</li>
</ul>
<p>看 Feature Importance 对于某些数据经过脱敏处理的比赛尤其重要。这可以免得你浪费大把时间在琢磨一个不重要的变量的意义上。(脱敏：数据脱敏(Data Masking),又称数据漂白、数据去隐私化或数据变形。百度百科对数据脱敏的定义为：指对某些敏感信息通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。在涉及客户安全数据或者一些商业性敏感数据的情况下，在不违反系统规则条件下，对真实数据进行改造并提供测试使用，如身份证号、手机号、卡号、客户号等个人信息都需要进行数据脱敏。)</p>
<h4 id="Feature-Encoding"><a href="#Feature-Encoding" class="headerlink" title="Feature Encoding"></a>Feature Encoding</h4><p>假设有一个 Categorical Variable 一共有几万个取值可能，那么创建 Dummy Variables 的方法就不可行了。这时一个比较好的方法是根据 Feature Importance 或是这些取值本身在数据中的出现频率，为最重要（比如说前 95% 的 Importance）那些取值（有很大可能只有几个或是十几个）创建 Dummy Variables，而所有其他取值都归到一个“其他”类里面。</p>
<h3 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h3><p>Base Model:</p>
<ul>
<li>SVM</li>
<li>Linear Regression</li>
<li>Logistic Regression</li>
<li>Neural Networks</li>
</ul>
<p>Most Used Models:</p>
<ul>
<li>Gradient Boosting</li>
<li>Random Forest</li>
<li><p>Extra Randomized Trees</p>
<p><strong>XGBoost</strong></p>
</li>
</ul>
<h4 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h4><p>通过Grid Search来确定模型的最佳参数。<br>e.g.</p>
<ul>
<li>sklearn 的 RandomForestClassifier 来说，比较重要的就是随机森林中树的数量 n_estimators 以及在训练每棵树时最多选择的特征数量 max_features。</li>
<li><p>Xgboost 的调参。通常认为对它性能影响较大的参数有：</p>
<ul>
<li>eta：每次迭代完成后更新权重时的步长。越小训练越慢。</li>
<li>num_round：总共迭代的次数。</li>
<li>subsample：训练每棵树时用来训练的数据占全部的比例。用于防止 Overfitting。</li>
<li>colsample_bytree：训练每棵树时用来训练的特征的比例，类似 RandomForestClassifier 的 max_features。</li>
<li>max_depth：每棵树的最大深度限制。与 Random Forest 不同，Gradient Boosting 如果不对深度加以限制，最终是会 Overfit 的。</li>
<li>early_stopping_rounds：用于控制在 Out Of Sample 的验证集上连续多少个迭代的分数都没有提高后就提前终止训练。用于防止 Overfitting。</li>
</ul>
<p>一般的调参步骤是：</p>
<ol>
<li>将训练数据的一部分划出来作为验证集。</li>
<li>先将 eta 设得比较高（比如 0.1），num_round 设为 300 ~ 500。</li>
<li>用 Grid Search 对其他参数进行搜索。</li>
<li>逐步将 eta 降低，找到最佳值。</li>
<li>以验证集为 watchlist，用找到的最佳参数组合重新在训练集上训练。注意观察算法的输出，看每次迭代后在验证集上分数的变化情况，从而得到最佳的 early_stopping_rounds。</li>
</ol>
<p><em>所有具有随机性的 Model 一般都会有一个 seed 或是 random_state 参数用于控制随机种子。得到一个好的 Model 后，在记录参数时务必也记录下这个值，从而能够在之后重现 Model。</em></p>
</li>
</ul>
<h4 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h4><p>一般5-fold。</p>
<p>fold越多训练越慢。</p>
<h4 id="Ensemble-Generation"><a href="#Ensemble-Generation" class="headerlink" title="Ensemble Generation"></a>Ensemble Generation</h4><p>常见的 Ensemble 方法有这么几种：</p>
<ul>
<li>Bagging：使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。</li>
<li>Boosting：迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting 的原理。比 Bagging 效果好，但更容易 Overfit。</li>
<li>Blending：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。</li>
<li>Stacking：接下来会详细介绍。</li>
</ul>
<p>从理论上讲，Ensemble 要成功，有两个要素：</p>
<ul>
<li>Base Model 之间的相关性要尽可能的小。这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。</li>
<li>Base Model 之间的性能表现不能差距太大。这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。</li>
</ul>
<h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>workflow比较复杂，因此一个高自动化的pipeline比较重要。</p>
<p>这里是以一个例子：<a href="https://github.com/ChenglongChen/Kaggle_CrowdFlower">example</a></p>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>MTL多任务学习</title>
    <url>/2022/01/18/mtl/</url>
    <content><![CDATA[<blockquote>
<p>MTL更多的是一种思想</p>
</blockquote>
<h1 id="两种模式"><a href="#两种模式" class="headerlink" title="两种模式"></a>两种模式</h1><ul>
<li>hard share: 不同任务直接共享部分模型参数</li>
<li>soft share: 不共享参数，添加正则来保证参数的相似</li>
</ul>
<a id="more"></a>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><ul>
<li>隐式数据增加（implicit data augmentation）：为了训练任务A，通过其他任务的数据来扩充任务A训练过程的数据量。这些数据可以看做是引入额外的噪声，在理想情况下起到提高模型泛化效果的作用</li>
<li>注意力机制（Attention focusing）：当训练数据量有限且高维时，模型很难区分出相关的特征和不相关的特征。多任务学习可以使模型更关注于有用的特征</li>
<li>监听机制（Eavesdropping）：从任务B中有可能容易学习到特征G，但是从任务A中很难学习得到。通过多任务学习，可以通过任务B学习到特征G，再利用特征G预测任务A<br>特征偏置（Representation bias）：多任务学习使得模型更容易去选择其他任务容易选择到的特征，这样有助于模型在假设空间下获得更好的模型泛化能力。</li>
</ul>
<h1 id="常见模型结构"><a href="#常见模型结构" class="headerlink" title="常见模型结构"></a>常见模型结构</h1><ul>
<li>常见模型：share bottom/MMoE/SNR/ESMM；损失函数按照指定权重融合，或者使用GradNorm做动态融合</li>
</ul>
<h2 id="Share-bottom"><a href="#Share-bottom" class="headerlink" title="Share bottom"></a>Share bottom</h2><ul>
<li>最常见的mtl结构，不同任务共享底层</li>
</ul>
<h2 id="MMoE-Multi-gate-Mixture-of-Experts"><a href="#MMoE-Multi-gate-Mixture-of-Experts" class="headerlink" title="MMoE(Multi-gate Mixture-of-Experts)"></a>MMoE(Multi-gate Mixture-of-Experts)</h2><ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007">paper</a></li>
<li>是一种soft share</li>
<li>添加了expert机制获取不同信息</li>
<li>添加gate机制对expert进行加权<img src="/2022/01/18/mtl/mmoe.png" class="" title="mmoe">
</li>
</ul>
<h2 id="CGC-Customized-Gate-Control-PLE-Progressive-Layered-Extraction"><a href="#CGC-Customized-Gate-Control-PLE-Progressive-Layered-Extraction" class="headerlink" title="CGC(Customized Gate Control)/PLE(Progressive Layered Extraction)"></a>CGC(Customized Gate Control)/PLE(Progressive Layered Extraction)</h2><ul>
<li>MMoE的基础上，把experts分为share experts和domain experts</li>
<li>单层多任务网络结构(CGC)，多层多任务网络结构（PLE）<img src="/2022/01/18/mtl/cgc.png" class="" title="cgc">
<img src="/2022/01/18/mtl/ple.png" class="" title="ple">
</li>
</ul>
<h2 id="SNR-Sub-Network-Routing"><a href="#SNR-Sub-Network-Routing" class="headerlink" title="SNR(Sub-Network Routing)"></a>SNR(Sub-Network Routing)</h2><ul>
<li><a href="https://ojs.aaai.org//index.php/AAAI/article/view/3788">paper</a><img src="/2022/01/18/mtl/snr.png" class="" title="snr"></li>
<li>SNR-Trans: 下层输出通过权重矩阵变换，再做加权后输出至下一层。会引入更多参数。其中z为二进制。<script type="math/tex; mode=display">
\left[\begin{array}{l}
\boldsymbol{v}_{1} \\
\boldsymbol{v}_{2}
\end{array}\right]=\left[\begin{array}{lll}
z_{11} W_{11} & z_{12} W_{12} & z_{13} W_{13} \\
z_{21} W_{21} & z_{22} W_{22} & z_{23} W_{23}
\end{array}\right]\left[\begin{array}{l}
u_{1} \\
u_{2} \\
u_{3}
\end{array}\right]</script></li>
<li>SNR-Avg: 下层输出加权后输出至下一层<script type="math/tex; mode=display">
\left[\begin{array}{l}
\boldsymbol{v}_{1} \\
\boldsymbol{v}_{2}
\end{array}\right]=\left[\begin{array}{lll}
z_{11} \boldsymbol{I}_{11} & z_{12} \boldsymbol{I}_{12} & z_{13} \boldsymbol{I}_{13} \\
z_{21} \boldsymbol{I}_{21} & z_{22} \boldsymbol{I}_{22} & z_{23} \boldsymbol{I}_{23}
\end{array}\right]\left[\begin{array}{l}
\boldsymbol{u}_{1} \\
\boldsymbol{u}_{2} \\
\boldsymbol{u}_{3}
\end{array}\right]</script></li>
<li>损失函数：<script type="math/tex; mode=display">
\min _{\boldsymbol{W}, \boldsymbol{\pi}} \boldsymbol{E}_{\boldsymbol{z} \sim p(\boldsymbol{z} ; \boldsymbol{\pi})} \frac{1}{N} \sum_{i=1}^{N} L\left(f\left(\boldsymbol{x}_{i} ; \boldsymbol{W}, \boldsymbol{z}\right), \boldsymbol{y}_{i}\right)</script></li>
<li>z为二进制，无法优化，需要把z松弛为平滑变量，将z变为hardSigmoid，其中s是一个服从q分布的连续的随机变量<script type="math/tex; mode=display">
z=g(s)=\min (1, \max (0, s))</script></li>
<li>可以继续转换，其中epsilon是一个噪声变量，r(epsilon)是一个无参数的噪声分布，h是一个确定且可微的分布<script type="math/tex; mode=display">
\min _{\boldsymbol{W}, \boldsymbol{\pi}} \boldsymbol{E}_{\boldsymbol{\epsilon} \sim r(\boldsymbol{\epsilon})} \frac{1}{N} \sum_{i=1}^{N} L\left(f\left(\boldsymbol{x}_{i} ; \boldsymbol{W}, g(h(\boldsymbol{\phi}, \boldsymbol{\epsilon}))\right), \boldsymbol{y}_{i}\right)</script></li>
<li>在实际应用中，结合重采样技术和hard concrete distribution，可以继续做转换，其中u为均匀分布，log(α)为需要学习的参数，其他为超参<script type="math/tex; mode=display">
\begin{aligned}
u \sim U(0,1), s &=\operatorname{sigmoid}((\log (u)-\log (1-u)+\log (\alpha) / \beta)\\
\bar{s} &=s(\zeta-\gamma)+\gamma, z=\min (1, \max (\bar{s}, 0))
\end{aligned}</script></li>
<li>training的时候，加入z的L0正则，则最终loss为：<script type="math/tex; mode=display">
\begin{array}{r}
\boldsymbol{E}_{\boldsymbol{\epsilon} \sim r(\boldsymbol{\epsilon})} \frac{1}{N} \sum_{i=1}^{N} L\left(f\left(\boldsymbol{x}_{i} ; \boldsymbol{W}, g(h(\boldsymbol{\phi}, \boldsymbol{\epsilon}))\right), \boldsymbol{y}_{i}\right) \\
+\lambda \sum_{j=1}^{|\boldsymbol{z}|} 1-Q\left(s_{j}<0 ; \phi_{j}\right)
\end{array}</script></li>
<li>training: 总结来说，模型需要学习的参数是W和隐变量分布变量log(alpha)。训练流程如下所示：<ul>
<li>首先，采样一组均匀分布的随机变量u；</li>
<li>其次，计算z来获得网络结构；</li>
<li>最后，将训练数据喂给模型来计算损失函数。W和log(alpha)的梯度通过反馈计算得到。</li>
</ul>
</li>
<li>serving: 使用如下的estimator来得到z的值<script type="math/tex; mode=display">
\hat{\boldsymbol{z}}=\min (1, \max (0, \operatorname{sigmoid}(\log (\boldsymbol{\alpha}))(\zeta-\gamma)+\gamma)) \text {. }</script></li>
</ul>
<p>When sigmoid <script type="math/tex">\left(\log \left(\alpha_{i j}\right)\right)(\zeta-\gamma)+\gamma<0</script>, we will have <script type="math/tex">\hat{z}_{i j}=0</script> and the resulted model will be sparsely connected.</p>
<h2 id="Star-One-Model-to-Serve-All-Star-Topology-Adaptive-Recommender-for-Multi-Domain-CTR-Prediction"><a href="#Star-One-Model-to-Serve-All-Star-Topology-Adaptive-Recommender-for-Multi-Domain-CTR-Prediction" class="headerlink" title="Star(One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction)"></a>Star(One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction)</h2><ul>
<li><a href="https://arxiv.org/abs/2101.11427">paper</a><img src="/2022/01/18/mtl/star.png" class="" title="star"></li>
<li>center tower和domain tower的结合，论文中是两塔权重element-wise相乘，也可以改为两塔logits相加</li>
<li>PN: partitioned normalization, 在BN的基础上，对每个domain引入domain相关的两个scale参数<script type="math/tex; mode=display">
z^{\prime}=\left(\gamma * \gamma_{p}\right) \frac{z-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\left(\beta+\beta_{p}\right)</script></li>
</ul>
<h2 id="十字绣-Cross-Stitch-Network"><a href="#十字绣-Cross-Stitch-Network" class="headerlink" title="十字绣(Cross-Stitch Network)"></a>十字绣(Cross-Stitch Network)</h2><ul>
<li><a href="https://arxiv.org/pdf/1604.03539.pdf">paper</a></li>
</ul>
<h2 id="ESMM-Entire-Space-Multi-Task-Model"><a href="#ESMM-Entire-Space-Multi-Task-Model" class="headerlink" title="ESMM(Entire Space Multi-Task Model)"></a>ESMM(Entire Space Multi-Task Model)</h2><ul>
<li>混合ctr、cvr数据流</li>
<li>传统CVR预估模型的本质，不是预测“item被点击，然后被转化”的概率（CTCVR），而是“假设item被点击，那么它被转化”的概率（CVR）。即CVR模型的样本空间，是click空间。</li>
<li>ESMM<ul>
<li>使用了全样本空间，通过预测CTR和CTCVR间接求CVR</li>
<li>解决training样本有偏的问题: training的时候在点击空间，serving的时候在展示空间。</li>
<li>直接在show空间求ctcvr，label太稀疏，建模ctcvr=cvr*ctr，解决稀疏问题</li>
</ul>
</li>
<li>利用全概率公式，<strong>隐式学习pCVR</strong></li>
<li><a href="https://arxiv.org/pdf/1804.07931.pdf">paper</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/57481330">blog</a><script type="math/tex; mode=display">
\underbrace{p(y=1, z=1 \mid x)}_{p C T C V R}=\underbrace{p(y=1 \mid x)}_{p C T R} \times \underbrace{p(z=1 \mid y=1, x)}_{p C V R}</script></li>
<li>pCVR只是一个variable，无显式监督信号<img src="/2022/01/18/mtl/esmm.png" class="" title="esmm">
</li>
</ul>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="GradNorm"><a href="#GradNorm" class="headerlink" title="GradNorm"></a>GradNorm</h2><ul>
<li><a href="https://openreview.net/pdf?id=H1bM1fZCW">paper</a></li>
<li>动态调整不同任务损失函数的权重：不同目标重要性不同/收敛的程度/loss的大小 diff较大，可以考虑使用</li>
<li><a href="https://blog.csdn.net/Leon_winter/article/details/105014677">https://blog.csdn.net/Leon_winter/article/details/105014677</a></li>
</ul>
<h2 id="share-vs-not-share"><a href="#share-vs-not-share" class="headerlink" title="share vs not share"></a>share vs not share</h2><ul>
<li>not share bias emb: tend to useful</li>
<li>share sparse: tend to useful</li>
<li>share bottom use small learning rate, tower use adam or bigger learning rate</li>
</ul>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="https://blog.nowcoder.net/n/8a9d69d063c546b291a3c9a5091cfbbe">一篇综述</a></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>MTL</tag>
      </tags>
  </entry>
  <entry>
    <title>tools-vim&amp;tmux</title>
    <url>/2021/12/10/tools/</url>
    <content><![CDATA[<h1 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h1><p>Some useful tools.</p>
<h2 id="Vim"><a href="#Vim" class="headerlink" title="Vim"></a>Vim</h2><p><a href="https://github.com/Chenzk1/vimrc">https://github.com/Chenzk1/vimrc</a></p>
<h2 id="Tmux"><a href="#Tmux" class="headerlink" title="Tmux"></a>Tmux</h2><p><a href="https://github.com/gpakosz/.tmux">https://github.com/gpakosz/.tmux</a></p>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-Apriori</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94Apriori/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]</p>
<h2 id="协同过滤推荐有哪些类型"><a href="#协同过滤推荐有哪些类型" class="headerlink" title="协同过滤推荐有哪些类型"></a>协同过滤推荐有哪些类型</h2><ul>
<li><p>基于用户(user-based)的协同过滤</p>
<p>基于用户(user-based)的协同过滤主要考虑的是用户和用户之间的相似度，只要找出相似用户喜欢的物品，并预测目标用户对对应物品的评分，就可以找到评分最高的若干个物品推荐给用户。 </p>
</li>
<li><p>基于项目(item-based)的协同过滤</p>
<p>基于项目(item-based)的协同过滤和基于用户的协同过滤类似，只不过这时我们转向找到物品和物品之间的相似度，只有找到了目标用户对某些物品的评分，那么我们就可以对相似度高的类似物品进行预测，将评分最高的若干个相似物品推荐给用户 </p>
</li>
<li><p>基于模型(model based)的协同过滤 </p>
<p>用机器学习的思想来建模解决，主流的方法可以分为：用关联算法，聚类算法，分类算法，回归算法，矩阵分解，神经网络,图模型以及隐语义模型来解决。</p>
</li>
</ul>
<a id="more"></a>
<h2 id="基于模型的协同过滤"><a href="#基于模型的协同过滤" class="headerlink" title="基于模型的协同过滤"></a>基于模型的协同过滤</h2><ul>
<li><p>用关联算法做协同过滤</p>
<p>做频繁集挖掘，找到满足支持度阈值的关联物品的频繁N项集或者序列。将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括<strong>支持度</strong>，<strong>置信度</strong>和<strong>提升度</strong>等。 常用的关联推荐算法有<strong>Apriori</strong>，<strong>FP Tree</strong>和<strong>PrefixSpan</strong> </p>
</li>
<li><p>用聚类算法做协同过滤</p>
<ul>
<li><p>基于用户聚类，则可以将用户按照一定距离度量方式分成不同的目标人群，将<strong>同样目标人群评分高的物品推荐给目标用户</strong>。</p>
</li>
<li><p>基于物品聚类，则是<strong>将用户评分高物品的相似同类物品推荐给用户</strong>。常用的聚类推荐算法有<strong>K-Means</strong>, <strong>BIRCH</strong>, <strong>DBSCAN</strong>和<strong>谱聚类</strong></p>
</li>
</ul>
</li>
<li><p>用分类算法做协同过滤</p>
<p>设置一份评分阈值，评分高于阈值的就是推荐，评分低于阈值就是不推荐，我们<strong>将问题变成了一个二分类问题</strong>。虽然分类问题的算法多如牛毛，但是目前使用最广泛的是逻辑回归。因为<strong>逻辑回归的解释性比较强</strong>，每个物品是否推荐我们都有一个明确的概率放在这，同时可以对数据的特征做工程化，得到调优的目的。常见的分类推荐算法有逻辑回归和朴素贝叶斯，两者的特点是解释性很强。</p>
</li>
<li><p>用回归算法做协同过滤</p>
<p>评分可以是一个连续的值而不是离散的值，<strong>通过回归模型</strong>我们可以得到目标用户对某商品的<strong>预测打分</strong>。常用的回归推荐算法有Ridge回归，回归树和支持向量回归。</p>
</li>
<li><p>用矩阵分解做协同过滤</p>
<p>用矩阵分解做协同过滤是目前使用也很广泛的一种方法。由于传统的奇异值分解SVD要求矩阵不能有缺失数据，必须是稠密的，而我们的用户物品评分矩阵是一个很典型的稀疏矩阵，直接使用传统的SVD到协同过滤是比较复杂的。 </p>
</li>
<li><p>用神经网络做协同过滤</p>
<p>用神经网络乃至深度学习做协同过滤应该是以后的一个趋势。目前比较主流的用两层神经网络来做推荐算法的是限制玻尔兹曼机(RBM) </p>
</li>
<li><p>用隐语义模型做协同过滤</p>
<p>隐语义模型主要是基于NLP的，涉及到<strong>对用户行为的语义分析来做评分推荐</strong>，主要方法有隐性语义分析LSA和隐含狄利克雷分布LDA，</p>
</li>
<li><p>用图模型做协同过滤</p>
<p>用图模型做协同过滤，则将用户之间的相似度放到了一个图模型里面去考虑，常用的算法是SimRank系列算法和马尔科夫模型算法。</p>
<h2 id="频繁项集的评估标准"><a href="#频繁项集的评估标准" class="headerlink" title="频繁项集的评估标准"></a>频繁项集的评估标准</h2></li>
<li><p>支持度: </p>
<ul>
<li>支持度就是几个关联的数据在数据集中出现的次数占总数据集的比重。或者说几个数据关联出现的概率。 <script type="math/tex; mode=display">
\text {Support} (X, Y)=P(X Y)=\frac{\text { number }(X Y)}{\text { num (AllSamples) }}</script></li>
</ul>
</li>
</ul>
<ul>
<li><p>置信度:</p>
<ul>
<li>一个数据出现后，另一个数据出现的概率，或者说数据的条件概率。 <script type="math/tex; mode=display">
\text {Confidence }(X \Leftarrow Y)=P(X | Y)=\frac{P(X Y)}{ P(Y)}</script></li>
</ul>
</li>
<li><p>提升度 ：</p>
<ul>
<li>提升度表示含有Y的条件下，同时含有X的概率，与X总体发生的概率之比 <script type="math/tex; mode=display">
\text {Lift }(X \Leftarrow Y)=\frac{P(X | Y)}{ P(X)} = \frac{\text { Confidence }(X \Leftarrow Y) }{ P(X)}</script></li>
</ul>
</li>
<li><p>注意：</p>
<ul>
<li>支持度高的数据不一定构成频繁项集，但是支持度太低的数据肯定不构成频繁项集。 </li>
<li>提升度体先了$X$和$Y$之间的关联关系, 提升度大于1则$X\Leftarrow Y$是有效的强关联规则， 提升度小于等于1则$X\Leftarrow Y$是无效的强关联规则 。一个特殊的情况，如果$X$和$Y$独立,则$\operatorname{Lift}(X \Leftarrow Y)=1$，因此$P(X | Y)=P(X)$</li>
</ul>
</li>
</ul>
<h2 id="使用Aprior算法找出频繁k项集"><a href="#使用Aprior算法找出频繁k项集" class="headerlink" title="使用Aprior算法找出频繁k项集"></a>使用Aprior算法找出频繁k项集</h2><p>输入：数据集合$D$，支持度阈值$\alpha$</p>
<p>输出：最大的频繁$k$项集</p>
<ul>
<li><p>扫描整个数据集，得到所有出现过的数据，作为候选频繁1项集。$k=1$，频繁0项集为空集。</p>
</li>
<li><p>挖掘频繁$k$项集</p>
<ul>
<li>扫描数据计算候选频繁$k$项集的支持度</li>
<li>去除候选频繁$k$项集中支持度低于阈值的数据集,得到频繁$k$项集。如果得到的频繁$k$项集为空，则直接返回频繁$k-1$项集的集合作为算法结果，算法结束。如果得到的频繁$k$项集只有一项，则直接返回频繁$k$项集的集合作为算法结果，算法结束。</li>
<li>基于频繁$k$项集，连接生成候选频繁$k+1$项集。</li>
</ul>
</li>
<li><p>令$k=k+1$，转入步骤挖掘频繁$k$项集。</p>
</li>
</ul>
<p>从算法的步骤可以看出，Aprior算法每轮迭代都要扫描数据集，因此在数据集很大，数据种类很多的时候，算法效率很低。</p>
<p>具体实现:</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170117161036255-1753157633.png" alt=""></p>
<h2 id="使用Aprior算法找出强关联规则"><a href="#使用Aprior算法找出强关联规则" class="headerlink" title="使用Aprior算法找出强关联规则"></a>使用Aprior算法找出强关联规则</h2><ul>
<li><p>强关联规则:</p>
<ul>
<li>如果规则$R$:$\Rightarrow $满足 :</li>
</ul>
<script type="math/tex; mode=display">
\tag{1} { support }(X \Rightarrow Y) \geq \min {sup}</script><script type="math/tex; mode=display">
\tag{2} confidence (X \Rightarrow Y) \geq \min conf</script><p>称关联规则$X\Rightarrow Y$为强关联规则,否则称关联规则$X\Rightarrow Y$为弱关联规则。在挖掘关联规则时,产生的关联规则要经过$\min sup$和$\min conf$的衡量筛选出来的强关联规则才能用商家的决策 </p>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Apriori</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-RNN&amp;LSTM</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94RNN+LSTM/</url>
    <content><![CDATA[<p><meta name="referrer" content="no-referrer"/><br>[TOC]</p>
<h2 id="LSTM产生的原因"><a href="#LSTM产生的原因" class="headerlink" title="LSTM产生的原因"></a>LSTM产生的原因</h2><ul>
<li><strong>RNN在处理长期依赖</strong>（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成<strong>梯度消失或者梯度膨胀</strong>的现象。RNN结构之所以出现梯度爆炸或者梯度消失，最本质的原因是因为梯度在传递过程中存在极大数量的连乘 。</li>
<li>相对于RNN，LSTM的神经元加入了<strong>输入门i、遗忘门f、输出门o 、内部记忆单元c</strong> </li>
</ul>
<a id="more"></a>
<h2 id="分别介绍一下输入门i、遗忘门f、输出门o-、内部记忆单元c"><a href="#分别介绍一下输入门i、遗忘门f、输出门o-、内部记忆单元c" class="headerlink" title="分别介绍一下输入门i、遗忘门f、输出门o 、内部记忆单元c"></a>分别介绍一下输入门i、遗忘门f、输出门o 、内部记忆单元c</h2><ul>
<li><p>内部记忆单元$c$</p>
<ul>
<li>类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。  </li>
<li>操作步骤：</li>
<li>示意图<br><img src="https://img-blog.csdn.net/20170919124608594?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbHJlYWRlcmw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></li>
</ul>
</li>
</ul>
<ul>
<li><p>遗忘门$f$</p>
<ul>
<li><p>将<strong>内部记忆单元</strong>中的信息选择性的遗忘</p>
</li>
<li><p>操作步骤：</p>
<ul>
<li>读取：$h_{t-1}$、$x_t$，</li>
<li>输出：$f<em>{t}=\sigma\left(W</em>{f} \cdot\left[h<em>{t-1}, x</em>{t}\right]+b_{f}\right)$</li>
<li>$\sigma$表示一个在 0 到 1 之间的数值。1 表示“完全保留”，0 表示“完全舍弃”</li>
</ul>
</li>
<li><p>示意图<br><img src="https://pic4.zhimg.com/80/v2-11ca9e4a19504874202ac9880da9840f_1440w.jpg" alt="遗忘门"></p>
</li>
</ul>
<script type="math/tex; mode=display">
f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)</script></li>
<li><p>输入门$i$</p>
<ul>
<li><p>将新的信息记录到<strong>内部记忆单元</strong>中</p>
</li>
<li><p>操作步骤：</p>
<ul>
<li><p>步骤一：$sigmoid$ 层称 <strong>输入门层</strong>决定什么值我们将要更新。</p>
</li>
<li><p>步骤二：$tanh$ 层创建一个新的候选值向量$\tilde{C}_t$加入到状态中。其示意图如下：</p>
<p><img src="https://img-blog.csdn.net/20170301115512234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="1578624753318"></p>
<script type="math/tex; mode=display">
i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right)</script><script type="math/tex; mode=display">
\tilde{C}_{t}=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)</script></li>
<li><p>步骤三：将$C<em>{t-1}$更新为$C</em>{t}$。将旧状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息。接着加上$i_t * \tilde{C}_t$得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：</p>
<p><img src="https://img-blog.csdn.net/20170301120227745?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="1578624724604"></p>
<script type="math/tex; mode=display">
C_{t}=f_{t} * C_{t-1}+i_{t} * \tilde{C}_{t}</script></li>
</ul>
</li>
</ul>
</li>
<li><p>输出门$o$</p>
<ul>
<li><p>确定隐层$h_t$输出什么值</p>
</li>
<li><p>操作步骤：</p>
<ul>
<li><p>步骤一：通过sigmoid 层来确定细胞状态的哪个部分将输出。</p>
</li>
<li><p>步骤二：把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p><img src="https://pic4.zhimg.com/80/v2-f928df2c02e17fb5da95bf8354880613_1440w.jpg" alt="输出门"></p>
<script type="math/tex; mode=display">
\begin{array}{l}
{o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right)} \\
{h_{t}=o_{t} * \tanh \left(C_{t}\right)}
\end{array}</script></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-梯度下降</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    <content><![CDATA[<h2 id="1-机器学习中为什么需要梯度下降"><a href="#1-机器学习中为什么需要梯度下降" class="headerlink" title="1. 机器学习中为什么需要梯度下降"></a>1. 机器学习中为什么需要梯度下降</h2><ul>
<li>梯度下降的作用：<ul>
<li>梯度下降是迭代法的一种，可以用于<strong>求解最小二乘问题</strong>。</li>
<li>在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，<strong>得到最小化的损失函数和模型参数值。</strong></li>
<li>如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。<strong>梯度下降法和梯度上升法可相互转换</strong>。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="2-梯度下降法缺点"><a href="#2-梯度下降法缺点" class="headerlink" title="2. 梯度下降法缺点"></a>2. 梯度下降法缺点</h2><p><strong>缺点</strong>：</p>
<ul>
<li>靠近极小值时收敛速度减慢。</li>
<li>直线搜索时可能会产生一些问题。</li>
<li>可能会“之字形”地下降。</li>
</ul>
<p><strong>注意</strong>：</p>
<ul>
<li>梯度是一个向量，即<strong>有方向有大小</strong>。 </li>
<li>梯度的方向是<strong>最大方向导数的方向</strong>。 </li>
<li>梯度的值是<strong>最大方向导数的值</strong>。</li>
</ul>
<h2 id="3-梯度下降法直观理解"><a href="#3-梯度下降法直观理解" class="headerlink" title="3. 梯度下降法直观理解"></a>3. 梯度下降法直观理解</h2><p>​    形象化举例，由上图所示，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。<br>​    由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<h2 id="4-梯度下降核心思想归纳"><a href="#4-梯度下降核心思想归纳" class="headerlink" title="4. 梯度下降核心思想归纳"></a>4. 梯度下降核心思想归纳</h2><ul>
<li>确定优化模型的假设函数及损失函数。</li>
<li>初始化参数，随机选取取值范围内的任意数；</li>
</ul>
<ul>
<li>迭代操作：<ul>
<li>计算当前梯度</li>
<li>修改新的变量</li>
<li>计算朝最陡的下坡方向走一步</li>
<li>判断是否需要终止，如否，<strong>梯度更新</strong></li>
</ul>
</li>
<li>得到全局最优解或者接近全局最优解。</li>
</ul>
<h2 id="5-如何对梯度下降法进行调优"><a href="#5-如何对梯度下降法进行调优" class="headerlink" title="5. 如何对梯度下降法进行调优"></a>5. 如何对梯度下降法进行调优</h2><p>实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：</p>
<p>(1)<strong>算法迭代步长$\alpha$选择。</strong><br>    在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。</p>
<p>(2)<strong>参数的初始值选择。</strong><br>    初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
<p>(3)<strong>标准化处理。</strong><br>    由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。</p>
<h2 id="6-随机梯度和批量梯度区别"><a href="#6-随机梯度和批量梯度区别" class="headerlink" title="6. 随机梯度和批量梯度区别"></a>6. 随机梯度和批量梯度区别</h2><p>​    随机梯度下降(SDG)和批量梯度下降(BDG)是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。<br>下面通过介绍两种梯度下降法的求解思路，对其进行比较。<br>假设函数为：</p>
<script type="math/tex; mode=display">
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n</script><p>损失函数为：</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1, ... , \theta_n) = 
            \frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)^2</script><p>其中，$m$为样本个数，$j$为参数个数。</p>
<p>1、 <strong>批量梯度下降的求解思路如下：</strong><br>a) 得到每个$\theta$对应的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i</script><p>b) 由于是求最小化风险函数，所以按每个参数 $\theta$ 的梯度负方向更新 $ \theta_i $ ：</p>
<script type="math/tex; mode=display">
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i</script><p>c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，如果样本数据很大，这种方法迭代速度就很慢。<br>相比而言，随机梯度下降可避免这种问题。</p>
<p>2、<strong>随机梯度下降的求解思路如下：</strong><br>a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。<br>损失函数可以写成如下这种形式，</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1, ... , \theta_n) = 
            \frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
            ,x^{j}_1,...,x^{j}_n))^2 = 
            \frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))</script><p>b)对每个参数 $ \theta$ 按梯度方向更新 $ \theta$：</p>
<script type="math/tex; mode=display">
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))</script><p>c) 随机梯度下降是通过每个样本来迭代更新一次。<br>随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。</p>
<p><strong>小结：</strong><br>随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">批量梯度下降</td>
<td style="text-align:left">a)采用所有数据来梯度下降。<br/>b)批量梯度下降法在样本量很大的时候，训练速度慢。</td>
</tr>
<tr>
<td style="text-align:center">随机梯度下降</td>
<td style="text-align:left">a)随机梯度下降用一个样本来梯度下降。<br/>b)训练速度很快。<br />c)随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br />d)收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</td>
</tr>
</tbody>
</table>
</div>
<p>下面介绍能结合两种方法优点的小批量梯度下降法。</p>
<p>3、 <strong>小批量(Mini-Batch)梯度下降的求解思路如下</strong><br>对于总数为$m$个样本的数据，根据样本的数据，选取其中的$n(1&lt; n&lt; m)$个子样本来迭代。其参数$\theta$按梯度方向更新$\theta_i$公式如下：</p>
<script type="math/tex; mode=display">
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
        ( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}</script><h2 id="7-各种梯度下降法性能比较"><a href="#7-各种梯度下降法性能比较" class="headerlink" title="7.  各种梯度下降法性能比较"></a>7.  各种梯度下降法性能比较</h2><p>​    下表简单对比随机梯度下降(SGD)、批量梯度下降(BGD)、小批量梯度下降(Mini-batch GD)、和Online GD的区别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">BGD</th>
<th style="text-align:center">SGD</th>
<th style="text-align:center">Mini-batch GD</th>
<th style="text-align:center">Online GD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">训练集</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">实时更新</td>
</tr>
<tr>
<td style="text-align:center">单次迭代样本数</td>
<td style="text-align:center">整个训练集</td>
<td style="text-align:center">单个样本</td>
<td style="text-align:center">训练集的子集</td>
<td style="text-align:center">根据具体算法定</td>
</tr>
<tr>
<td style="text-align:center">算法复杂度</td>
<td style="text-align:center">高</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">低</td>
</tr>
<tr>
<td style="text-align:center">时效性</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">高</td>
</tr>
<tr>
<td style="text-align:center">收敛性</td>
<td style="text-align:center">稳定</td>
<td style="text-align:center">不稳定</td>
<td style="text-align:center">较稳定</td>
<td style="text-align:center">不稳定</td>
</tr>
</tbody>
</table>
</div>
<p>​    Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p>
<p>​    Online GD在互联网领域用的较多，比如搜索广告的点击率(CTR)预估模型，网民的点击行为会随着时间改变。用普通的BGD算法(每天更新一次)一方面耗时较长(需要对所有历史数据重新训练)；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p>
<h2 id="8-推导多元函数梯度下降法的迭代公式。"><a href="#8-推导多元函数梯度下降法的迭代公式。" class="headerlink" title="8. 推导多元函数梯度下降法的迭代公式。"></a>8. 推导多元函数梯度下降法的迭代公式。</h2><p>根据多元函数泰勒公式，如果忽略一次以上的项，函数在$\mathbf{x}$点处可以展开为</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\Delta \mathbf{x})=f(\mathbf{x})+(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}+o(\|\mathbf{\Delta} \mathbf{x}\|)</script><p>对上式变形，函数的增量与自变量增量、函数梯度的关系为</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\Delta \mathbf{x})-f(\mathbf{x})=(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}+o(\|\Delta \mathbf{x}\|)</script><p>如果令$\Delta \mathbf{x}=-\nabla f(\mathbf{x})$则有</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\Delta \mathbf{x})-f(\mathbf{x}) \approx-(\nabla f(\mathbf{x}))^{\mathrm{T}} \nabla f(\mathbf{x}) \leq 0</script><p>即函数值减小。即有</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\Delta \mathbf{x}) \leq f(\mathbf{x})</script><p>梯度下降法每次的迭代增量为</p>
<script type="math/tex; mode=display">
\Delta \mathbf{x}=-\alpha \nabla f(\mathbf{x})</script><p>其中$\alpha$为人工设定的接近于的正数，称为步长或学习率。其作用是保证$\mathbf{x}+\Delta \mathbf{x}$在$\mathbf{x}$的<br>邻域内，从而可以忽略泰勒公式中的$o(|\Delta \mathbf{x}|)$项。</p>
<p>使用该增量则有</p>
<script type="math/tex; mode=display">
(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}=-\alpha(\nabla f(\mathbf{x}))^{\mathrm{T}}(\nabla f(\mathbf{x})) \leq 0</script><p>函数值下降。从初始点$\mathbf{x}_{0}$开始，反复使用如下<strong>迭代公式</strong></p>
<script type="math/tex; mode=display">
\mathbf{x}_{k+1}=\mathbf{x}_{k}-\alpha \nabla f\left(\mathbf{x}_{k}\right)</script><p>只要没有到达梯度为0的点，函数值会沿序列$\mathbf{x}<em>{k}$递减，最终收敛到梯度为0 的点。从$\mathbf{x}</em>{0}$<br>出发，用<strong>迭代公式</strong>进行迭代，会形成一个函数值递减的序列$\left{\mathbf{x}_{i}\right}$</p>
<script type="math/tex; mode=display">
f\left(\mathbf{x}_{0}\right) \geq f\left(\mathbf{x}_{1}\right) \geq f\left(\mathbf{x}_{2}\right) \geq \ldots \geq f\left(\mathbf{x}_{k}\right)</script><h2 id="9-梯度下降法如何判断是否收敛？"><a href="#9-梯度下降法如何判断是否收敛？" class="headerlink" title="9.  梯度下降法如何判断是否收敛？"></a>9.  梯度下降法如何判断是否收敛？</h2><p>迭代终止的条件是函数的梯度值为0(实际实现时是接近于0 即可)，此时认为已经达<br>到极值点。可以通过判定梯度的二范数是否充分接近于0 而实现。</p>
<h2 id="10-梯度下降法为什么要在迭代公式中使用步长系数？"><a href="#10-梯度下降法为什么要在迭代公式中使用步长系数？" class="headerlink" title="10. 梯度下降法为什么要在迭代公式中使用步长系数？"></a>10. 梯度下降法为什么要在迭代公式中使用步长系数？</h2><p>其作用是保证$\mathbf{x}+\Delta \mathbf{x}$在$\mathbf{x}$的邻域内，即控制增量的步长，从而可以忽略泰勒公式中的<br>$o(|\Delta \mathbf{x}|)$项。否则不能保证每次迭代时函数值下降。</p>
<h2 id="11-梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？"><a href="#11-梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？" class="headerlink" title="11. 梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？"></a>11. 梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？</h2><p>不能，可能收敛到鞍点，不是极值点。</p>
<h2 id="12-解释一元函数极值判别法则。"><a href="#12-解释一元函数极值判别法则。" class="headerlink" title="12. 解释一元函数极值判别法则。"></a>12. 解释一元函数极值判别法则。</h2><p>假设$x_0$为函数的驻点，可分为以下三种情况。<br>case1：在该点处的二阶导数大于0，则为函数的极小值点；<br>case2：在该点处的二阶导数小于0，则为极大值点；<br>case3：在该点处的二阶导数等于0，则情况不定，可能是极值点，也可能不是极值点。</p>
<h2 id="13-解释多元函数极值判别法则。"><a href="#13-解释多元函数极值判别法则。" class="headerlink" title="13. 解释多元函数极值判别法则。"></a>13. 解释多元函数极值判别法则。</h2><p>假设多元函数在点M的梯度为0 ，即M 是函数的驻点。其Hessian 矩阵有如下几种情<br>况。<br>case1：Hessian 矩阵正定，函数在该点有极小值。<br>case2：Hessian 矩阵负定，函数在该点有极大值。<br>case3：Hessian 矩阵不定，则不是极值点，称为鞍点。<br>Hessian 矩阵正定类似于一元函数的二阶导数大于0，负定则类似于一元函数的二阶导<br>数小于0。</p>
<h2 id="14-什么是鞍点？"><a href="#14-什么是鞍点？" class="headerlink" title="14. 什么是鞍点？"></a>14. 什么是鞍点？</h2><p><strong>Hessian 矩阵不定的点称为鞍点</strong>，它不是函数的极值点。</p>
<h2 id="15-解释什么是局部极小值，什么是全局极小值。"><a href="#15-解释什么是局部极小值，什么是全局极小值。" class="headerlink" title="15. 解释什么是局部极小值，什么是全局极小值。"></a>15. 解释什么是局部极小值，什么是全局极小值。</h2><ul>
<li>全局极小值<ul>
<li>假设$\mathbf{x}^{*}$是一个可行解，如果对可行域内所有点$\mathbf{x}$都有$f\left(\mathbf{x}^{*}\right) \leq f(\mathbf{x})$，则<br>称$\mathbf{x}^{*}$为全局极小值。</li>
</ul>
</li>
<li>局部极小值<ul>
<li>对于可行解$\mathbf{x}^{*}$，如果存在其$\delta$邻域，使得该邻域内的所有点即所有满足<br>$\left|\mathbf{x}-\mathbf{x}^{*}\right| \leq \delta$的点$\mathbf{x}$，都有$f\left(x^{*}\right) \leq f(x)$，则称$\mathbf{x}^{*}$为局部极小值。</li>
</ul>
</li>
</ul>
<h2 id="16-推导多元函数牛顿法的迭代公式。"><a href="#16-推导多元函数牛顿法的迭代公式。" class="headerlink" title="16. 推导多元函数牛顿法的迭代公式。"></a>16. 推导多元函数牛顿法的迭代公式。</h2><p>根据费马定理，函数在点$\mathbf{x}$ 处取得极值的必要条件是梯度为0</p>
<script type="math/tex; mode=display">
\nabla f(\mathbf{x})=\mathbf{0}</script><p>对于一般的函数，直接求解此方程组存在困难。对目标函数在$\mathbf{x}_{0}$ 处作二阶泰勒展开</p>
<script type="math/tex; mode=display">
f(\mathbf{x})=f\left(\mathbf{x}_{0}\right)+\nabla f\left(\mathbf{x}_{0}\right)^{\mathrm{T}}\left(\mathbf{x}-\mathbf{x}_{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{0}\right)^{\mathrm{T}} \nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)+o\left(\left\|\mathbf{k}-\mathbf{x}_{0}\right\|^{2}\right)</script><p>忽略二次以上的项，将目标函数近似成二次函数，等式两边同时对$\mathbf{x}$求梯度，可得</p>
<script type="math/tex; mode=display">
\nabla f(\mathbf{x}) \approx \nabla f\left(\mathbf{x}_{0}\right)+\nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)</script><p>其中 $\nabla^{2} f\left(\mathbf{x}<em>{0}\right)$为在$\mathbf{x}</em>{0}$ 处的Hessian 矩阵。令函数的梯度为0 ，有 </p>
<script type="math/tex; mode=display">
\nabla f\left(\mathbf{x}_{0}\right)+\nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)=\mathbf{0}</script><p>解这个线性方程组可以得到</p>
<script type="math/tex; mode=display">
\tag{1}\mathbf{x}=\mathbf{x}_{0}-\left(\nabla^{2} f\left(\mathbf{x}_{0}\right)\right)^{-1} \nabla f\left(\mathbf{x}_{0}\right)</script><p>如果将梯度向量简写为$\mathbf{g}$ ，Hessian 矩阵简记为$\mathbf{H}$ ，式(1)可以简写为</p>
<script type="math/tex; mode=display">
\tag{2}\mathbf{x}=\mathbf{x}_{0}-\mathbf{H}^{-1} \mathbf{g}</script><p>在泰勒公式中忽略了高阶项将函数做了近似，因此这个解不一定是目标函数的驻点，需要反复用式(2) 进行迭代。从初始点$\mathbf{x}_{0}$处开始，计算函数在当前点处的Hessian 矩阵和梯度向量，然后用下面的公式进行迭代</p>
<script type="math/tex; mode=display">
\tag{3} \mathbf{x}_{k+1}=\mathbf{x}_{k}-\alpha \mathbf{H}_{k}^{-1} \mathbf{g}_{k}</script><p>直至收敛到驻点处。迭代终止的条件是梯度的模接近于0 ，或达到指定的迭代次数。其中$\alpha$是人工设置的学习率。需要学习率的原因与梯度下降法相同，是为了保证能够忽略泰勒公式中的高阶无穷小项。</p>
<h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><p>深度学习500问： <a href="https://github.com/scutan90/DeepLearning-500-questions">https://github.com/scutan90/DeepLearning-500-questions</a> </p>
<p>机器学习与深度学习习题集答案-1：<a href="https://mp.weixin.qq.com/s/4kWUE8ml_o6iF0F1TREyiA">https://mp.weixin.qq.com/s/4kWUE8ml_o6iF0F1TREyiA</a></p>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DL</tag>
        <tag>GradientDescent</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-决策树</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]</p>
<h2 id="1-简单介绍决策树算法"><a href="#1-简单介绍决策树算法" class="headerlink" title="1. 简单介绍决策树算法"></a>1. 简单介绍决策树算法</h2><ul>
<li>决策树将算法组织成一颗树的形式。其实这就是将平时所说的<strong>if-then语句</strong>构建成了树的形式。决策树主要包括<strong>三个部分：内部节点、叶节点、边。内部节点是划分的特征，边代表划分的条件，叶节点表示类别。</strong></li>
<li>构建决策树 就是一个递归的选择内部节点，计算划分条件的边，最后到达叶子节点的过程。 决策树在本质上是一组嵌套的if-else判定规则，从数学上看是分段常数函数，对应于用平行于坐标轴的平面对空间的划分。判定规则是人类处理很多问题时的常用方法，这些规则是我们通过经验总结出来的，而决策树的这些规则是通过训练样本自动学习得到的。</li>
<li>训练时，通过最大化Gini或者其他指标来寻找最佳分裂。决策树可以输特征向量每个分量的重要性。</li>
<li><strong>决策树是一种判别模型，既支持分类问题，也支持回归问题，是一种非线性模型（分段线性函数不是线性的）。它天然的支持多分类问题。</strong></li>
</ul>
<a id="more"></a>
<h2 id="2-决策树和条件概率分布的关系？"><a href="#2-决策树和条件概率分布的关系？" class="headerlink" title="2. 决策树和条件概率分布的关系？"></a>2. 决策树和条件概率分布的关系？</h2><p><strong>决策树可以表示成给定条件下类的条件概率分布。</strong> </p>
<p>决策树中的每一条路径都对应是划分的一个条件概率分布. 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大。</p>
<h2 id="3-信息增益比相对信息增益有什么好处？"><a href="#3-信息增益比相对信息增益有什么好处？" class="headerlink" title="3. 信息增益比相对信息增益有什么好处？"></a>3. 信息增益比相对信息增益有什么好处？</h2><ul>
<li><p>使用信息增益时：模型<strong>偏向于选择取值较多</strong>的特征</p>
</li>
<li><p>使用信息增益比时：<strong>对取值多的特征加上的惩罚</strong>，对这个问题进行了校正。</p>
</li>
</ul>
<h2 id="4-ID3算法—-gt-C4-5算法—-gt-CART算法"><a href="#4-ID3算法—-gt-C4-5算法—-gt-CART算法" class="headerlink" title="4. ID3算法—&gt;C4.5算法—&gt; CART算法"></a>4. ID3算法—&gt;C4.5算法—&gt; CART算法</h2><ul>
<li><p>$ID3$</p>
<ul>
<li>$ID3$算法没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。</li>
<li>$ID3$算法采用信息增益大的特征优先建立决策树的节点，偏向于取值比较多的特征</li>
<li>$ID3$算法对于缺失值的情况没有做考虑</li>
<li>$ID3$算法没有考虑过拟合的问题</li>
</ul>
</li>
<li><p>$C4.5$在$ID3$算法上面的改进</p>
<ul>
<li>连续的特征离散化 </li>
<li>使用信息增益比 </li>
<li>通过剪枝算法解决过拟合</li>
</ul>
</li>
<li><p>$C4.5$的不足：</p>
<ul>
<li>$C4.5$生成的是多叉树</li>
<li>$C4.5$只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</li>
<li>$C4.5$由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算</li>
</ul>
</li>
<li><p>$CART$算法 </p>
<ul>
<li>可以做回归，也可以做分类， </li>
<li>使用基尼系数来代替信息增益比 </li>
<li>$CART$分类树离散值的处理问题，采用的思路是不停的二分离散特征。 </li>
</ul>
</li>
</ul>
<h2 id="5-决策树的缺失值是怎么处理的"><a href="#5-决策树的缺失值是怎么处理的" class="headerlink" title="5. 决策树的缺失值是怎么处理的"></a>5. 决策树的缺失值是怎么处理的</h2><ul>
<li><p>如何在特征值缺失的情况下进行划分特征的选择？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">（比如“色泽”这个特征有的样本在该特征上的值是缺失的，那么该如何计算“色泽”的信息增益？）</span><br></pre></td></tr></table></figure>
<ul>
<li><p>每个样本设置一个权重（初始可以都为1） </p>
</li>
<li><p>划分数据，一部分是有特征值$a$的数据，另一部分是没有特征值$a$的数据,记为$\tilde{D}$，</p>
</li>
<li><p><strong>对</strong>没有缺失特征值$a$的<strong>数据集$\tilde{D}$，</strong>来和对应的特征$A$的各个特征值一起<strong>计算加权重后的信息增益比</strong>，最后乘上一个系数$\rho$ 。</p>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
\rho=\frac{\sum_{x \in \tilde{D}} w_{x}}{\sum_{x \in {D}} w_{x}}</script><script type="math/tex; mode=display">
\tilde{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}} \quad(1 \leq \mathrm{k} \leq|y|)</script><script type="math/tex; mode=display">
\tilde{r}_{v}=\frac{\sum_{x \in \tilde D^{v}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}} \quad(1 \leq v \leq V)</script><p>​        假设特征$A$有$v$个取值${a_1,a_2 \dots a_v}$</p>
<p>​        $\tilde D$：该特征上没有缺失值的样本</p>
<p>​        $\tilde D_k$：$\tilde D$中属于第$k$类的样本子集</p>
<p>​        $\tilde D^v$：$\tilde D$中在特征$a$上取值为$a_v$的样本子集</p>
<p>​        $\rho$：无特征$A$缺失的样本加权后所占加权总样本的比例。</p>
<p>​        $\tilde{p}_{k}$：无缺失值样本第$k$类所占无缺失值样本的比例</p>
<p>​        $\tilde{r}_{v}$：无缺失值样本在特征$a$上取值$a^v$的样本所占无缺失值样本的比例</p>
<p>​        新的信息增益公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\operatorname{Gain}(D, a)=\rho \times \operatorname{Gain}(\tilde{D}, a)=\rho \times\left(\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right)\right)\\
&\operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k} \log _{2} \tilde{p}_{k}
\end{aligned}</script><ul>
<li><p>给定划分特征，若样本在该特征上的值是缺失的，那么该如何对这个样本进行划分？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">（即到底把这个样本划分到哪个结点里？）</span><br></pre></td></tr></table></figure>
<ul>
<li>让包含缺失值的样本以不同的概率划分到不同的子节点中去。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2&#x2F;9,3&#x2F;9, 4&#x2F;9。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="6-决策树的目标函数是什么？"><a href="#6-决策树的目标函数是什么？" class="headerlink" title="6. 决策树的目标函数是什么？"></a><strong>6. 决策树的目标函数是什么？</strong></h2><script type="math/tex; mode=display">
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+a|T|</script><script type="math/tex; mode=display">
H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}</script><p>其中$|T|$代表叶节点个数</p>
<p>$N_t$表示具体某个叶节点的样例数</p>
<p> $H_t(T)$ 表示叶节点$t$上的经验熵</p>
<p>$\alpha|T|$为正则项，$\alpha \geqslant 0 $ 为参数。</p>
<h2 id="7-决策树怎么处理连续性特征？"><a href="#7-决策树怎么处理连续性特征？" class="headerlink" title="7. 决策树怎么处理连续性特征？"></a>7. 决策树怎么处理连续性特征？</h2><p>因为连续特征的可取值数目不再有限，因此不能像前面处理离散特征枚举离散特征取值来对结点进行划分。因此需要连续特征离散化，常用的离散化策略是二分法，这个技术也是$C4.5$中采用的策略。下面来具体介绍下，如何采用二分法对连续特征离散化： </p>
<ul>
<li><p>训练集D，连续特征$A$，其中A有n个取值</p>
</li>
<li><p>对$A$的取值进行从小到大排序得到：${a_1,a_2\dots a_n}$</p>
</li>
<li><p>寻找划分点$t$，$t$将D分为子集$D<em>{t}^{-}$与$D</em>{t}^{+}$</p>
<ul>
<li>$D_{t}^{-}$：特征$A$上取值不大于$t$的样本</li>
<li>$D_{t}^{+}$：特征$A$上取值大于$t$的样本</li>
</ul>
</li>
<li><p>对相邻的特征取值$a<em>i$与$a</em>{i+1}$，t再区间$[a<em>i,a</em>{i+1})$中取值所产生的划分结果相同，因此对于连续特征$A$,包含有$n-1$个元素的后选划分点集合</p>
</li>
</ul>
<script type="math/tex; mode=display">
T_a = \{\frac{a_i + a_{i+1}}{2}|1\leq{i}\leq{n-1} \}</script><ul>
<li><p>把区间$[a<em>i,a</em>{i+1})$的中位点$\frac{a<em>i + a</em>{i+1}}{2}$作为候选划分点</p>
</li>
<li><p>按照处理离散值那样来选择最优的划分点,使用公式：</p>
<script type="math/tex; mode=display">
Gain(D,a) =\underbrace{max}_{t\in T_a}Gain(D,a,t) = \underbrace{max}_{t\in T_a}\ (Ent(D) - \sum_{\lambda \in \{-,+ \}}\frac{|D_t^{\lambda}|}{|D|}Ent(D_t^{\lambda}))</script><p>其中$Gain(D,a,t)$是样本集$D$基于划分点$t$二分之后的信息增益。划分点时候选择使用$Gain(D,a,t)$最大的划分点。</p>
</li>
</ul>
<h2 id="8-决策树怎么防止过拟合？"><a href="#8-决策树怎么防止过拟合？" class="headerlink" title="8. 决策树怎么防止过拟合？"></a><strong>8. 决策树怎么防止过拟合？</strong></h2><h3 id="构建随机森林"><a href="#构建随机森林" class="headerlink" title="构建随机森林"></a>构建随机森林</h3><ul>
<li>构建随机森林</li>
</ul>
<h3 id="控制树的结构复杂程度"><a href="#控制树的结构复杂程度" class="headerlink" title="控制树的结构复杂程度"></a>控制树的结构复杂程度</h3><ul>
<li><p>预剪枝(提前停止)：控制<strong>深度、当前的节点数、分裂对测试集的准确度提升大小</strong></p>
<ul>
<li>限制树的高度，可以利用交叉验证选择</li>
<li>利用分类指标，如果下一次切分没有降低误差，则停止切分</li>
<li>限制树的节点个数，比如某个节点小于100个样本，停止对该节点切分</li>
</ul>
</li>
<li><p>后剪枝(自底而上)：<strong>生成决策树、交叉验证剪枝：子树删除，节点代替子树、测试集准确率判断决定剪枝</strong></p>
<ul>
<li>在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作. 剪枝的目的就是防止过拟合，是模型在测试数据上变现良好，更加鲁棒。</li>
</ul>
</li>
</ul>
<h2 id="9-如果特征很多，决策树中最后没有用到的特征一定是无用吗？"><a href="#9-如果特征很多，决策树中最后没有用到的特征一定是无用吗？" class="headerlink" title="9. 如果特征很多，决策树中最后没有用到的特征一定是无用吗？"></a>9. 如果特征很多，决策树中最后没有用到的特征一定是无用吗？</h2><p>不是无用的，从两个角度考虑：</p>
<ul>
<li><p><strong>特征替代性</strong>，如果可以已经使用的特征$A$和特征$B$可以提点特征$C$，特征$C$可能就没有被使用，但是如果把特征$C$单独拿出来进行训练，依然有效</p>
</li>
<li><p>决策树的每一条路径就是<strong>计算条件概率的条件</strong>，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据.</p>
</li>
</ul>
<h2 id="10-决策树的优缺点？"><a href="#10-决策树的优缺点？" class="headerlink" title="10.决策树的优缺点？"></a>10.决策树的优缺点？</h2><ul>
<li><p>优点: </p>
<ul>
<li>简单直观，生成的决策树很直观。</li>
<li>基本不需要预处理，不需要提前归一化，处理缺失值。</li>
<li>既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</li>
<li>可以处理多维度输出的分类问题。</li>
<li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li>
<li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li>
<li>对于异常点的容错能力好，健壮性高。</li>
</ul>
</li>
<li><p>缺点:</p>
<ul>
<li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li>
<li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li>
<li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li>
<li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li>
<li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li>
</ul>
</li>
</ul>
<h2 id="11-树形结构为什么不需要归一化"><a href="#11-树形结构为什么不需要归一化" class="headerlink" title="11. 树形结构为什么不需要归一化?"></a>11. 树形结构为什么不需要归一化?</h2><ul>
<li><strong>数值缩放不影响分裂点位置，对树模型的结构不造成影响</strong>。</li>
<li><p>按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。</p>
</li>
<li><p>树模型是<strong>不能进行梯度下降</strong>的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此<strong>树模型是阶跃的，阶跃点是不可导</strong>的，并且求导没意义，也就不需要归一化。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DecisionTree</tag>
      </tags>
  </entry>
  <entry>
    <title>线性回归&amp;逻辑回归</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92+%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="1-简单介绍一下线性回归。"><a href="#1-简单介绍一下线性回归。" class="headerlink" title="1. 简单介绍一下线性回归。"></a>1. 简单介绍一下线性回归。</h2><ul>
<li>线性：两个变量之间的关系<strong>是</strong>一次函数关系的——图象<strong>是直线</strong>，叫做线性。</li>
<li>非线性：两个变量之间的关系<strong>不是</strong>一次函数关系的——图象<strong>不是直线</strong>，叫做非线性。</li>
<li>回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算<strong>回归到真实值</strong>，这就是回归的由来。</li>
<li>线性回归就是利用的样本$D=(\mathrm{x}_i, \mathrm{y}_i)<br>$，$ \mathrm{i}=1,2,3 \ldots \mathrm{N}, \mathrm{x}_i$是特征数据，可能是一个，也可能是多个，通过有监督的学习，学习到由$x$到$y$的映射$h$，利用该映射关系对未知的数据进行预估，因为$y$为连续值，所以是回归问题。</li>
</ul>
<a id="more"></a>
<h2 id="2-线性回归的假设函数是什么形式？"><a href="#2-线性回归的假设函数是什么形式？" class="headerlink" title="2. 线性回归的假设函数是什么形式？"></a>2. 线性回归的假设函数是什么形式？</h2><p>线性回归的假设函数（$\theta<em>{0}$表示截距项，$ x</em>{0} = 1$，方便矩阵表达）：</p>
<script type="math/tex; mode=display">
f(x)=\theta_{0} x_{0}+\theta_{1} x_{1}+\theta_{2} x_{2} \ldots+\theta_{n} x_{n}  = \theta ^TX</script><p>其中$\theta,x$都是列向量</p>
<h2 id="3-线性回归的代价-损失-函数是什么形式？"><a href="#3-线性回归的代价-损失-函数是什么形式？" class="headerlink" title="3. 线性回归的代价(损失)函数是什么形式？"></a>3. 线性回归的代价(损失)函数是什么形式？</h2><script type="math/tex; mode=display">
MSE: \qquad J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(y_{i}-h_{\theta}\left(x_{i}\right)\right)^{2}</script><h2 id="4-简述岭回归与Lasso回归以及使用场景。"><a href="#4-简述岭回归与Lasso回归以及使用场景。" class="headerlink" title="4. 简述岭回归与Lasso回归以及使用场景。"></a>4. 简述岭回归与Lasso回归以及使用场景。</h2><ul>
<li><p>目的：</p>
<ul>
<li><p>解决线性回归出现的过拟合的请况。</p>
</li>
<li><p>解决在通过正规方程方法求解$\theta$的过程中出现的$X^TX$不可逆的请况。</p>
</li>
</ul>
</li>
<li><p>本质：</p>
<ul>
<li>约束(限制)要优化的参数 </li>
</ul>
</li>
</ul>
<p>这两种回归均通过在损失函数中引入<strong>正则化项</strong>来达到目的：</p>
<p><strong>线性回归的损失函数：</strong></p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><ul>
<li><strong>岭回归</strong><ul>
<li>损失函数：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}</script><ul>
<li><strong>Lasso回归</strong><ul>
<li>损失函数</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} |\theta_{j}|</script><p>本来Lasso回归与岭回归的解空间是全部区域，但通过正则化添加了一些约束，使得解空间变小了，甚至在个别正则化方式下，解变得稀疏了。<br><img src="https://img-blog.csdnimg.cn/20200101212656597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlaXRhbzUyMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" /><br>如图所示，这里的$w_1$，$w_2$都是模型的参数，要优化的目标参数，那个红色边框包含的区域，其实就是解空间，正如上面所说，这个时候，解空间“缩小了”，你只能在这个缩小了的空间中，寻找使得目标函数最小的$w_1$，$w_2$。左边图的解空间是圆的，是由于采用了$L2$范数正则化项的缘故，右边的是个四边形，是由于采用了$L1$范数作为正则化项的缘故，大家可以在纸上画画，$L2$构成的区域一定是个圆，$L1$构成的区域一定是个四边形。</p>
<p>再看看那蓝色的圆圈，再次提醒大家，这个<strong>坐标轴和特征（数据）没关系</strong>，它完全是参数的坐标系，每一个圆圈上，可以取无数个$w_1$，$w_2$，这些$w_1$，$w_2$有个共同的特点，用它们计算的目标函数值是相等的！那个蓝色的圆心，就是实际最优参数，但是由于我们对解空间做了限制，所以最优解只能在“缩小的”解空间中产生。</p>
<p>蓝色的圈圈一圈又一圈，代表着参数$w_1$，$w_2$在不停的变化，并且是在解空间中进行变化（这点注意，图上面没有画出来，估计画出来就不好看了），直到脱离了解空间，也就得到了图上面的那个$w^*$，这便是目标函数的最优参数。</p>
<p>对比一下左右两幅图的$w^*$，我们明显可以发现，右图的$w^*$的$w_1$分量是0，有没有感受到一丝丝凉意？稀疏解诞生了！是的，这就是我们想要的稀疏解，我们想要的简单模型。$L1$比$L2$正则化更容易产生稀疏矩阵。</p>
<ul>
<li><p>补充</p>
<ul>
<li><p><strong>ElasticNet 回归</strong>： 线性回归 + L1正则化 + L2 正则化。</p>
<ul>
<li><p>ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。</p>
</li>
<li><p>损失函数</p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2} \sum_{i}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}+\lambda\left(\rho \sum_{j}^{n}\left|\theta_{j}\right|+(1-\rho) \sum_{j}^{n} \theta_{j}^{2}\right)</script></li>
</ul>
</li>
<li><p><strong>LWR(局部加权)回归</strong>：</p>
<ul>
<li><p>局部加权线性回归是在线性回归的基础上对每一个测试样本（训练的时候就是每一个训练样本）在其已有的样本进行一个加权拟合，<strong>权重的确定</strong>可以通过一个核来计算，常用的有<strong>高斯核</strong>（离测试样本越近，权重越大，反之越小），这样对每一个测试样本就得到了不一样的权重向量，所以最后得出的拟合曲线不再是线性的了，这样就增加的模型的复杂度来更好的拟合非线性数据。</p>
</li>
<li><p>损失函数</p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2} \sum_{i=1}^{m} w^{(i)}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="5-线性回归要求因变量服从正态分布吗？"><a href="#5-线性回归要求因变量服从正态分布吗？" class="headerlink" title="5. 线性回归要求因变量服从正态分布吗？"></a>5. 线性回归要求因变量服从正态分布吗？</h2><p><strong>线性回归的假设前提是噪声服从正态分布，即因变量服从正态分布。但实际上难以达到，因变量服从正态分布时模型拟合效果更好。</strong></p>
<p>参考资料： <a href="http://www.julyedu.com/question/big/kp_id/23/ques_id/2914">http://www.julyedu.com/question/big/kp_id/23/ques_id/2914</a> </p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="1-简单介绍一下逻辑回归"><a href="#1-简单介绍一下逻辑回归" class="headerlink" title="1. 简单介绍一下逻辑回归"></a>1. 简单介绍一下逻辑回归</h2><p>逻辑回归用来解决<strong>分类</strong>问题，线性回归的结果$Y$带入一个非线性变换的<strong>Sigmoid函数</strong>中，得到$[0,1]$之间取值范围的数$S$，$S$可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么$S$大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。</p>
<ul>
<li>逻辑回归的本质： 极大似然估计</li>
<li>逻辑回归的激活函数：Sigmoid</li>
<li>逻辑回归的代价函数：交叉熵</li>
</ul>
<h2 id="2-简单介绍一下Sigmoid函数"><a href="#2-简单介绍一下Sigmoid函数" class="headerlink" title="2. 简单介绍一下Sigmoid函数"></a>2. 简单介绍一下Sigmoid函数</h2><p>函数公式如下：</p>
<script type="math/tex; mode=display">
S(t)=\frac{1}{1+e^{-t}}</script><p>函数中$t$无论取什么值，其结果都在$[0,{1}]$的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，那又有人问了，你这不是$[0,1]$的区间吗，怎么会只有0和1呢？这个问题问得好，我们假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。</p>
<p>好了，接下来我们把$\theta^T X+b$带入$t$中就得到了我们的逻辑回归的一般模型方程：</p>
<p>逻辑回归的<strong>假设函数</strong>：</p>
<script type="math/tex; mode=display">
H(\theta, b)=\frac{1}{1+e^{(\theta^T X+b)}}</script><p>结果$P$也可以理解为概率，换句话说概率大于0.5的属于1分类，概率小于0.5的属于0分类，这就达到了分类的目的。</p>
<h2 id="3-逻辑回归的损失函数是什么"><a href="#3-逻辑回归的损失函数是什么" class="headerlink" title="3. 逻辑回归的损失函数是什么"></a>3. 逻辑回归的损失函数是什么</h2><p>逻辑回归的损失函数是<strong>对数似然函数</strong>，函数公式如下：</p>
<script type="math/tex; mode=display">
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}-\log \left(h_{\theta}(x)\right) & \qquad  y=1 \\-\log \left(1-h_{\theta}(x)\right) & \qquad  y=0 \end{aligned}\right.</script><p><strong>两式合并</strong>得到<strong>概率分布表达式</strong>：</p>
<script type="math/tex; mode=display">
(P(y|x,\theta ) = h_{\theta}(x)^y(1-h_{\theta}(x))^{1-y})</script><p> <strong>对数似然函数最大化</strong>得到<strong>似然函数的代数表达式</strong>为 ：</p>
<script type="math/tex; mode=display">
L(\theta) = \prod\limits_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}</script><p> <strong>对似然函数对数化取反</strong>得到<strong>损失函数表达式</strong> ：</p>
<script type="math/tex; mode=display">
J(\theta) = -lnL(\theta) = -\sum\limits_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+ (1-y^{(i)})log(1-h_{\theta}(x^{(i)})))</script><ul>
<li>为何不能用mse</li>
</ul>
<p><a href="http://sofasofa.io/forum_main_post.php?postid=1001792">解释</a></p>
<h2 id="4-可以进行多分类吗？"><a href="#4-可以进行多分类吗？" class="headerlink" title="4.可以进行多分类吗？"></a>4.可以进行多分类吗？</h2><p>多分类问题一般将二分类推广到多分类的方式有三种，一对一，一对多，多对多。</p>
<ul>
<li><p>一对一：</p>
<ul>
<li>将$N$个类别两两配对，产生$N(N-1)/2$个二分类任务，测试阶段新样本同时交给所有的分类器，最终结果通过投票产生。</li>
</ul>
</li>
<li><p>一对多：</p>
<ul>
<li>每一次将一个例作为正例，其他的作为反例，训练$N$个分类器，测试时如果只有一个分类器预测为正类，则对应类别为最终结果，如果有多个，则一般选择置信度最大的。从分类器角度一对一更多，但是每一次都只用了2个类别，因此当类别数很多的时候一对一开销通常更小(只要训练复杂度高于$O(N)$即可得到此结果)。</li>
</ul>
</li>
<li><p>多对多：</p>
<ul>
<li>若干各类作为正类，若干个类作为反类。注意正反类必须特殊的设计。</li>
</ul>
</li>
</ul>
<h2 id="5-逻辑回归的优缺点"><a href="#5-逻辑回归的优缺点" class="headerlink" title="5.逻辑回归的优缺点"></a>5.逻辑回归的优缺点</h2><ul>
<li><p>优点</p>
<ul>
<li>LR能以概率的形式输出结果，而非只是0,1判定。</li>
<li>LR的可解释性强、可控度高、训练速度快</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li><p>对模型中自变量多重共线性较为敏感</p>
<p>例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性；</p>
</li>
<li><p>预测结果呈$S$型，因此从$log(odds)$向概率转化的过程是非线性的，在两端随着$log(odds)$值的变化，概率变化很小，边际值太小，slope太小，而中间概率的变化很大，很敏感。 导致很多区间的变量变化对目标概率的影响没有区分度，无法确定阀值。</p>
</li>
</ul>
</li>
</ul>
<h2 id="6-逻辑斯特回归为什么要对特征进行离散化。"><a href="#6-逻辑斯特回归为什么要对特征进行离散化。" class="headerlink" title="6. 逻辑斯特回归为什么要对特征进行离散化。"></a>6. 逻辑斯特回归为什么要对特征进行离散化。</h2><ul>
<li><p>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；离散特征的增加和减少都很容易，易于模型的快速迭代； </p>
</li>
<li><p>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； </p>
</li>
<li><p>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</p>
</li>
<li><p>方便交叉与特征组合：离散化后可以进行特征交叉，由$M+N$个变量变为$M*N$个变量，进一步引入非线性，提升表达能力； </p>
</li>
</ul>
<h2 id="7-线性回归与逻辑回归的区别"><a href="#7-线性回归与逻辑回归的区别" class="headerlink" title="7.  线性回归与逻辑回归的区别"></a>7.  线性回归与逻辑回归的区别</h2><ul>
<li><p>线性回归的样本的输出，都是连续值，$ y\in (-\infty ,+\infty )$，而逻辑回归中$y\in (0,1)$，只能取0和1。</p>
</li>
<li><p>对于拟合函数也有本质上的差别： </p>
<ul>
<li>线性回归：$f(x)=\theta ^{T}x=\theta <em>{1}x </em>{1}+\theta <em>{2}x </em>{2}+…+\theta <em>{n}x </em>{n}$</li>
<li>逻辑回归：$f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)$，其中，$g(z)=\frac{1}{1+e^{-z}}$</li>
</ul>
</li>
</ul>
<p>  线性回归的拟合函数，是对$f(x)$的输出变量$y$的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合。</p>
<ul>
<li><p>为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？ </p>
<p>$\theta ^{T}x=0$就相当于是1类和0类的决策边界： </p>
<p>当$\theta ^{T}x&gt;0$，则$y&gt;0.5$；若$\theta ^{T}x\rightarrow +\infty $，则$y \rightarrow  1 $，即$y$为1类; </p>
<p>当$\theta ^{T}x&lt;0$，则$y&lt;0.5$；若$\theta ^{T}x\rightarrow -\infty $，则$y \rightarrow  0 $，即$y$为0类; </p>
</li>
</ul>
<ul>
<li><p>这个时候就能看出区别，在线性回归中$\theta ^{T}x$为预测值的拟合函数；而在逻辑回归中$\theta ^{T}x$为决策边界。下表为线性回归和逻辑回归的区别。</p>
<p><center>线性回归和逻辑回归的区别</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">线性回归</th>
<th style="text-align:center">逻辑回归</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">目的</td>
<td style="text-align:center">预测</td>
<td style="text-align:center">分类</td>
</tr>
<tr>
<td style="text-align:center">$y^{(i)}$</td>
<td style="text-align:center">未知</td>
<td style="text-align:center">（0,1）</td>
</tr>
<tr>
<td style="text-align:center">函数</td>
<td style="text-align:center">拟合函数</td>
<td style="text-align:center">预测函数</td>
</tr>
<tr>
<td style="text-align:center">参数计算方式</td>
<td style="text-align:center">最小二乘法</td>
<td style="text-align:center">极大似然估计</td>
</tr>
</tbody>
</table>
</div>
<p> 下面具体解释一下： </p>
<ol>
<li>拟合函数和预测函数什么关系呢？简单来说就是将拟合函数做了一个逻辑函数的转换，转换后使得$y^{(i)} \in (0,1)$;</li>
<li>最小二乘和最大似然估计可以相互替代吗？回答当然是不行了。我们来看看两者依仗的原理：最大似然估计是计算使得数据出现的可能性最大的参数，依仗的自然是Probability。而最小二乘是计算误差损失。</li>
</ol>
<h2 id="8-逻辑回归有哪些应用"><a href="#8-逻辑回归有哪些应用" class="headerlink" title="8. 逻辑回归有哪些应用"></a>8. 逻辑回归有哪些应用</h2><ul>
<li>CTR预估/推荐系统的learning to rank/各种分类场景。</li>
<li>某搜索引擎厂的广告CTR预估基线版是LR。</li>
<li>某电商搜索排序/广告CTR预估基线版是LR。</li>
<li>某电商的购物搭配推荐用了大量LR。</li>
<li>某现在一天广告赚1000w+的新闻app排序基线是LR。</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-贝叶斯</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<p>[TOC]</p>
<h2 id="1-简述朴素贝叶斯算法原理和工作流程"><a href="#1-简述朴素贝叶斯算法原理和工作流程" class="headerlink" title="1.简述朴素贝叶斯算法原理和工作流程"></a>1.简述朴素贝叶斯算法原理和工作流程</h2><p><strong>工作原理</strong>：</p>
<ul>
<li>假设现在有样本$x=(x_1, x_2, x_3, \dots x_n)$待分类项</li>
<li>假设样本有$m$个特征$(a_1,a_2,a_3,\dots a_m)$(特征独立)</li>
<li>再假设现在有分类目标$Y={ y_1，y_2，y_3，\dots ,y_n}$</li>
<li>那么就$\max ({ P }({ y }_1 | { x }), { P }({ y }_2 | {x}), {P}({y}_3 | {x}) ,{P}({y_n} | {x}))$是最终的分类类别。</li>
<li>而$ P(y_i | x)=\frac{P(x | y_i) * P(y_i)}{ P(x)} $，因为$x$对于每个分类目标来说都一样，所以就是求$\max({P}({x}|{y_i})*{P}({y_i}))$</li>
<li>$P(x | y_i) * P(y_i)=P(y_i) * \prod(P(a_j| y_i))$，而具体的$P(a_j|y_i)$和$P(y_i)$都是能从训练样本中统计出来</li>
<li>${P}({a_j} | {y_i})$表示该类别下该特征$a_j$出现的概率$P(y_i)$表示全部类别中这个这个类别出现的概率,这样就能找到应该属于的类别了</li>
</ul>
<a id="more"></a>
<h2 id="2-条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念"><a href="#2-条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念" class="headerlink" title="2. 条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念"></a>2. 条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念</h2><ul>
<li><p>条件概率：</p>
<ul>
<li>$P(X|Y)$含义： 表示$Y$发生的条件下$X$发生的概率。</li>
</ul>
</li>
<li><p>先验概率</p>
<ul>
<li><strong>表示事件发生前的预判概率。</strong>这个可以是基于历史数据统计，也可以由背景常识得出，也可以是主观观点得出。一般都是单独事件发生的概率，如 $P(X)$</li>
</ul>
</li>
<li><p>后验概率</p>
<ul>
<li>基于先验概率求得的<strong>反向条件概率</strong>，形式上与条件概率相同(若$P(X|Y)$ 为正向，则$P(Y|X)$ 为反向)</li>
</ul>
</li>
<li><p>联合概率：</p>
</li>
<li><p>事件$X$与事件$Y$同时发生的概率。</p>
</li>
<li><p>贝叶斯公式</p>
<ul>
<li><script type="math/tex; mode=display">
P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}  \\</script></li>
<li><p>$P(Y)$ 叫做<strong>先验概率</strong>：事件$X$发生之前，我们根据以往经验和分析对事件$Y$发生的一个概率的判断</p>
</li>
<li><p>$P(Y|X)$ 叫做<strong>后验概率</strong>：事件$X$发生之后，我们对事件$Y$发生的一个概率的重新评估</p>
</li>
<li><p>$P(Y,X)$叫做<strong>联合概率</strong>：事件$X$与事件$Y$同时发生的概率。</p>
</li>
<li><p>先验概率和后验概率是相对的。如果以后还有新的信息引入，更新了现在所谓的后验概率，得到了新的概率值，那么这个新的概率值被称为后验概率。</p>
</li>
</ul>
</li>
</ul>
<h2 id="3-为什么朴素贝叶斯如此“朴素”？"><a href="#3-为什么朴素贝叶斯如此“朴素”？" class="headerlink" title="3.为什么朴素贝叶斯如此“朴素”？"></a>3.为什么朴素贝叶斯如此“朴素”？</h2><p>因为它<strong>假定所有的特征在数据集中的作用是同样重要和独立的</strong>。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。用贝叶斯公式表达如下：</p>
<script type="math/tex; mode=display">
P(Y|X_1, X_2) = \frac{P(X_1|Y) P(X_2|Y) P(Y)}{P(X_1)P(X_2)}</script><p><strong>而在很多情况下，所有变量几乎不可能满足两两之间的条件。</strong></p>
<p>朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是<strong>“很简单很天真”</strong>地假设样本特征彼此独立.这个假设现实中基本上不存在，但特征相关性很小的实际情况还是很多的，所以这个模型仍然能够工作得很好。</p>
<h2 id="4-什么是贝叶斯决策理论？"><a href="#4-什么是贝叶斯决策理论？" class="headerlink" title="4.什么是贝叶斯决策理论？"></a>4.什么是贝叶斯决策理论？</h2><p>贝叶斯决策理论是主观贝叶斯派归纳理论的重要组成部分。贝叶斯决策就是在不完全情报下，对部分未知的状态用主观概率估计，然后用贝叶斯公式对发生概率进行修正，最后再利用期望值和修正概率做出最优决策(选择概率最大的类别)。<br>贝叶斯决策理论方法是统计模型决策中的一个基本方法，其<strong>基本思想</strong>是：</p>
<ul>
<li>已知类条件概率密度参数表达式和先验概率</li>
<li>利用贝叶斯公式转换成后验概率</li>
<li>根据后验概率大小进行决策分类</li>
</ul>
<h2 id="5-朴素贝叶斯算法的前提假设是什么？"><a href="#5-朴素贝叶斯算法的前提假设是什么？" class="headerlink" title="5.朴素贝叶斯算法的前提假设是什么？"></a>5.朴素贝叶斯算法的前提假设是什么？</h2><ul>
<li>特征之间相互独立</li>
<li>每个特征同等重要</li>
</ul>
<h2 id="6-为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果"><a href="#6-为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果" class="headerlink" title="6.为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?"></a>6.为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?</h2><ul>
<li>对于分类任务来说，只要各类别的条件概率排序正确、无需精准概率值即可导致正确分类；</li>
<li>如果属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。</li>
</ul>
<h2 id="7-什么是朴素贝叶斯中的零概率问题？如何解决？"><a href="#7-什么是朴素贝叶斯中的零概率问题？如何解决？" class="headerlink" title="7.什么是朴素贝叶斯中的零概率问题？如何解决？"></a><strong>7.什么是朴素贝叶斯中的零概率问题？如何解决？</strong></h2><p><strong>零概率问题</strong>：在计算实例的概率时，如果某个量$x$，在观察样本库(训练集)中没有出现过，会导致整个实例的概率结果是0。</p>
<p><strong>解决办法</strong>：若$P(x)$为零则无法计算。为了解决零概率的问题，法国数学家拉普拉斯最早提出用加1的方法估计没有出现过的现象的概率，所以加法平滑也叫做<strong>拉普拉斯平滑</strong>。</p>
<p><strong>举个栗子</strong>：假设在文本分类中，有3个类，$C1、C2、C3$，在指定的训练样本中，某个词语$K1$，在各个类中观测计数分别为0，990，10，$K1$的概率为0，0.99，0.01，对这三个量使用拉普拉斯平滑的计算方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1&#x2F;1003&#x3D;0.001，</span><br><span class="line">991&#x2F;1003&#x3D;0.988，</span><br><span class="line">11&#x2F;1003&#x3D;0.011</span><br><span class="line">在实际的使用中也经常使用加 lambda(1≥lambda≥0)来代替简单加1。如果对N个计数都加上lambda，这时分母也要记得加上N*lambda。</span><br></pre></td></tr></table></figure>
<p>将朴素贝叶斯中的所有概率计算<strong>应用拉普拉斯平滑即可以解决零概率问题</strong>。</p>
<h2 id="8-朴素贝叶斯中概率计算的下溢问题如何解决？"><a href="#8-朴素贝叶斯中概率计算的下溢问题如何解决？" class="headerlink" title="8.朴素贝叶斯中概率计算的下溢问题如何解决？"></a>8.朴素贝叶斯中概率计算的下溢问题如何解决？</h2><p><strong>下溢问题</strong>：在朴素贝叶斯的计算过程中，需要对特定分类中各个特征出现的<strong>概率进行连乘，小数相乘，越乘越小，这样就造成了下溢出</strong>。<br>为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误。</p>
<script type="math/tex; mode=display">
\prod_{i=x}^{n} p\left(x_{i} | y_{j}\right)</script><p><strong>解决办法</strong>：对其<strong>取对数</strong>：</p>
<script type="math/tex; mode=display">
\log \prod_{i=1}^{n} p\left(x_{i} | y_{j}\right)</script><script type="math/tex; mode=display">
=\sum_{i=1}^{n} \log p\left(x_{i} | y_{j}\right)</script><p>将小数的乘法操作转化为取对数后的加法操作，规避了变为零的风险同时并不影响分类结果。</p>
<h2 id="9-当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？"><a href="#9-当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？" class="headerlink" title="9.当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？"></a>9.当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？</h2><p>当朴素贝叶斯算法数据的属性为连续型变量时，有两种方法可以计算属性的类条件概率。</p>
<ul>
<li>第一种方法：把一个连续的属性离散化，然后用相应的离散区间替换连续属性值。但这种方法不好控制离散区间划分的粒度。如果粒度太细，就会因为每个区间内训练记录太少而不能对$P(X|Y)$<br>做出可靠的估计，如果粒度太粗，那么有些区间就会有来自不同类的记录，因此失去了正确的决策边界。</li>
<li>第二种方法：假设连续变量服从某种概率分布，然后使用训练数据估计分布的参数，例如可以使用高斯分布来表示连续属性的类条件概率分布。<ul>
<li>高斯分布有两个参数，均值$\mu$和方差$\sigma 2$，对于每个类$y_i$，属性$X_i$的类条件概率等于：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
P\left(X_{i}=x_{i} | Y=y_{j}\right)=\frac{1}{\sqrt{2 \Pi} \sigma_{i j}^{2}} e^{\frac{\left(x_{i}-\mu_{j}\right)^{2}}{2 \sigma_{i}^{2}}}</script><p>$\mu_{i j}$：类$y_j$的所有训练记录关于$X_i$的样本均值估计</p>
<p>$\sigma_{i j}^{2}$：类$y_j$的所有训练记录关于$X$的样本方差</p>
<p>通过高斯分布估计出类条件概率。</p>
<h2 id="10-朴素贝叶斯有哪几种常用的分类模型？"><a href="#10-朴素贝叶斯有哪几种常用的分类模型？" class="headerlink" title="10.朴素贝叶斯有哪几种常用的分类模型？"></a><strong>10.朴素贝叶斯有哪几种常用的分类模型？</strong></h2><p>三个常用模型：高斯、多项式、伯努利。三个模型都是为了解决连续性变量引起的问题的。</p>
<ul>
<li><p>多项式模型：</p>
<ul>
<li>思想：将一个连续值视为特征的一个类别，这样会引起很多概率为0的问题。所以用拉普拉斯平滑解决。</li>
<li>其中$\alpha$为拉普拉斯平滑，加和的是属性出现的总次数，比如文本分类问题里面，不光看词语是否在文本中出现，也得看出现的次数。如果总词数为$n$，出现词数为$m$的话，说起来有点像掷骰子$n$次出现$m$次这个词的场景。<script type="math/tex; mode=display">
P\left(x_{i} | y_{k}\right)=\frac{N_{y k_{1}}+\alpha}{N_{y_{k}}+\alpha n}</script></li>
</ul>
</li>
<li><p>高斯模型：</p>
<ul>
<li>思想：是要得到一个概率值，但是直接把一个值当做一个类别会引起零概率问题，于是假设并模拟一个分布，求其参数，进而求其概率。</li>
<li>处理包含连续型变量的数据，使用高斯分布概率密度来计算类的条件概率密度</li>
</ul>
</li>
<li><p>伯努利模型：</p>
<ul>
<li>伯努利模型特征的取值为布尔型，即出现为true没有出现为false，在文本分类中，就是一个单词有没有在一个文档中出现。</li>
<li>伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次。<script type="math/tex; mode=display">
P( '代开'， '发票'， '发票'， '我' | S) = P('代开' | S)   P( '发票' | S) P('我' | S)</script>我们看到，”发票“出现了两次，但是我们只将其算作一次。我们看到，”发票“出现了两次，但是我们只将其算作一次。</li>
</ul>
</li>
</ul>
<h2 id="11-为什么说朴素贝叶斯是高偏差低方差？"><a href="#11-为什么说朴素贝叶斯是高偏差低方差？" class="headerlink" title="11.为什么说朴素贝叶斯是高偏差低方差？"></a>11.为什么说朴素贝叶斯是高偏差低方差？</h2><p>在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error=Bias +Variance$。</p>
<ul>
<li>$Error$反映的是整个模型的准确度，</li>
<li>$Bias$反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，</li>
<li>$Variance$反映的是模型每一次输出结果与模型输出期望(平均值)之间的误差，即模型的稳定性，数据是否集中。</li>
<li>对于复杂模型，充分拟合了部分数据，使得他们的偏差较小，而由于对部分数据的过度拟合，对于部分数据预测效果不好，整体来看可能引起方差较大。</li>
<li>对于朴素贝叶斯了。它简单的假设了各个数据之间是无关的，是一个被严重简化了的模型，简单模型与复杂模型相反，大部分场合偏差部分大于方差部分，也就是说高偏差而低方差。</li>
</ul>
<h2 id="12-朴素贝叶斯为什么适合增量计算？"><a href="#12-朴素贝叶斯为什么适合增量计算？" class="headerlink" title="12.朴素贝叶斯为什么适合增量计算？"></a>12.朴素贝叶斯为什么适合增量计算？</h2><p>因为朴素贝叶斯在训练过程中实际只需要计算出各个类别的概率和各个特征的类条件概率，这些概率值可以快速的根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算，该特性可以使用在超出内存的大量数据计算和按小时级等获取的数据计算中。</p>
<h2 id="13-高度相关的特征对朴素贝叶斯有什么影响？"><a href="#13-高度相关的特征对朴素贝叶斯有什么影响？" class="headerlink" title="13.高度相关的特征对朴素贝叶斯有什么影响？"></a>13.高度相关的特征对朴素贝叶斯有什么影响？</h2><p>假设有两个特征高度相关，相当于该特征在模型中发挥了两次作用(计算两次条件概率)，使得朴素贝叶斯获得的结果向该特征所希望的方向进行了偏移，影响了最终结果的准确性，所以朴素贝叶斯算法应先处理特征，把相关特征去掉。</p>
<h2 id="14-朴素贝叶斯的应用场景有哪些？"><a href="#14-朴素贝叶斯的应用场景有哪些？" class="headerlink" title="14.朴素贝叶斯的应用场景有哪些？"></a>14.朴素贝叶斯的应用场景有哪些？</h2><ul>
<li><strong>文本分类/垃圾文本过滤/情感判别</strong>：<br>这大概是朴素贝叶斯应用最多的地方了，即使在现在这种分类器层出不穷的年代，在文本分类场景中，朴素贝叶斯依旧坚挺地占据着一席之地。因为多分类很简单，同时在文本数据中，分布独立这个假设基本是成立的。而垃圾文本过滤(比如垃圾邮件识别)和情感分析(微博上的褒贬情绪)用朴素贝叶斯也通常能取得很好的效果。</li>
<li><strong>多分类实时预测</strong>：<br>对于文本相关的多分类实时预测，它因为上面提到的优点，被广泛应用，简单又高效。</li>
<li><strong>推荐系统</strong>：<br>朴素贝叶斯和协同过滤是一对好搭档，协同过滤是强相关性，但是泛化能力略弱，朴素贝叶斯和协同过滤一起，能增强推荐的覆盖度和效果。</li>
</ul>
<h2 id="15-朴素贝叶斯有什么优缺点？"><a href="#15-朴素贝叶斯有什么优缺点？" class="headerlink" title="15.朴素贝叶斯有什么优缺点？"></a>15.朴素贝叶斯有什么优缺点？</h2><ul>
<li>优点：<ul>
<li>对数据的训练快，分类也快</li>
<li>对缺失数据不太敏感，算法也比较简单</li>
<li>对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练</li>
</ul>
</li>
<li>缺点：<ul>
<li>对输入数据的表达形式很敏感</li>
<li>由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。</li>
<li>需要计算先验概率，分类决策存在错误率。</li>
</ul>
</li>
</ul>
<h2 id="16-朴素贝叶斯与-LR-区别？"><a href="#16-朴素贝叶斯与-LR-区别？" class="headerlink" title="16.朴素贝叶斯与 LR 区别？"></a>16.朴素贝叶斯与 LR 区别？</h2><ul>
<li><strong>朴素贝叶斯是生成模型</strong>，根据已有样本进行贝叶斯估计学习出先验概率 $P(Y)$ 和条件概率 $P(X|Y)$，进而求出联合分布概率 $P(X,Y)$，最后利用贝叶斯定理求解$P(Y|X)$， 而<strong>LR是判别模型</strong>，根据极大化对数似然函数直接求出条件概率 $P(Y|X)$</li>
<li>朴素贝叶斯是基于很强的<strong>条件独立假设</strong>(在已知分类Y的条件下，各个特征变量取值是相互独立的)，而 LR 则对此没有要求</li>
<li>朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。</li>
</ul>
<h2 id="17-贝叶斯优化算法-参数调优"><a href="#17-贝叶斯优化算法-参数调优" class="headerlink" title="17. 贝叶斯优化算法(参数调优)"></a>17. 贝叶斯优化算法(参数调优)</h2><ul>
<li><p>网格搜索和随机搜索：在测试一个新点时，会忽略前一个点的信息；</p>
</li>
<li><p>贝叶斯优化算法：充分利用了之前的信息。贝叶斯优化算法通过对目标函数形式进行学习，找到使目标函数向全局最优值提升的参数。</p>
</li>
<li><p>学习目标函数形式的方法：</p>
<ul>
<li>首先根据先验分布，假设一个搜集函数；</li>
<li>每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布</li>
<li>算法测试由后验分布给出的全局最值最可能出现的位置的点。</li>
</ul>
</li>
</ul>
<p>对于贝叶斯优化算法，有一个需要注意的地方，一旦找到了一个局部最优值，它会在该区域不断采样，所以很容易陷入局部最优值。为了弥补这个缺陷，贝叶斯优化算法会在探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利用”则是根据后验分布在最可能出现全局最值的区域进行采样。</p>
<h2 id="18-朴素贝叶斯分类器对异常值敏感吗"><a href="#18-朴素贝叶斯分类器对异常值敏感吗" class="headerlink" title="18.朴素贝叶斯分类器对异常值敏感吗?"></a>18.朴素贝叶斯分类器对异常值敏感吗?</h2><p>朴素贝叶斯是一种<strong>对异常值不敏感</strong>的分类器，保留数据中的异常值，常常可以保持贝叶斯算法的整体精度，如果对原始数据进行降噪训练，分类器可能会因为失去部分异常值的信息而导致泛化能力下降。</p>
<h2 id="19-朴素贝叶斯算法对缺失值敏感吗？"><a href="#19-朴素贝叶斯算法对缺失值敏感吗？" class="headerlink" title="19.朴素贝叶斯算法对缺失值敏感吗？"></a>19.朴素贝叶斯算法对缺失值敏感吗？</h2><p>朴素贝叶斯是一种<strong>对缺失值不敏感</strong>的分类器，朴素贝叶斯算法能够处理缺失的数据，在算法的建模时和预测时数据的属性都是单独处理的。因此<strong>如果一个数据实例缺失了一个属性的数值，在建模时将被忽略</strong>，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。</p>
<h2 id="20-一句话总结贝叶斯算法"><a href="#20-一句话总结贝叶斯算法" class="headerlink" title="20. 一句话总结贝叶斯算法"></a>20. 一句话总结贝叶斯算法</h2><p><strong>贝叶斯分类器直接用贝叶斯公式解决分类问题</strong>。假设样本的特征向量为$x$，类别标签为$y$，根据贝叶斯公式，样本属于每个类的条件概率（后验概率）为： </p>
<script type="math/tex; mode=display">
p(y | \mathbf{x})=\frac{p(\mathbf{x} | y) p(y)}{p(\mathbf{x})}</script><p> 分母$p(x)$对所有类都是相同的，<strong>分类的规则是将样本归到后验概率最大的那个类</strong>，不需要计算准确的概率值，只需要知道属于哪个类的概率最大即可，这样可以忽略掉分母。分类器的判别函数为： </p>
<script type="math/tex; mode=display">
\arg \max _{y} p(\mathrm{x} | y) p(y)</script><p>在实现贝叶斯分类器时，<strong>需要知道每个类的条件概率分布$p(x|y)$即先验概率</strong>。一般假设样本服从正态分布。训练时确定先验概率分布的参数，一般用最大似然估计，即最大化对数似然函数。</p>
<p><strong>贝叶斯分类器是一种生成模型，可以处理多分类问题，是一种非线性模型。</strong></p>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>NaiveBayes</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-Random Forest</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
    <content><![CDATA[<p>[TOC]</p>
<h2 id="1-简单介绍随机森林"><a href="#1-简单介绍随机森林" class="headerlink" title="1. 简单介绍随机森林"></a>1. 简单介绍随机森林</h2><ul>
<li><strong>多次随机取样，多次随机取属性，选取最优分割点，构建多个(CART)分类器，投票表决或简单平均</strong></li>
<li><strong>算法流程：</strong><ul>
<li>输入为样本集$D={(x，y_1)，(x_2，y_2) \dots (x_m，y_m)}$，弱分类器迭代次数$T$。</li>
<li>输出为最终的强分类器$f(x)$</li>
<li>对于$t=1，2 \dots T$<ul>
<li>对训练集进行第$t$次随机采样，共采集$m$次，得到包含$m$个样本的采样集Dt</li>
<li>用采样集$D_t$训练第$t$个决策树模型$G_t(x)$，在训练决策树模型的节点的时候，在节点上所有的样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分</li>
</ul>
</li>
<li>如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="2-随机森林的随机性体现在哪里？"><a href="#2-随机森林的随机性体现在哪里？" class="headerlink" title="2. 随机森林的随机性体现在哪里？"></a>2. 随机森林的随机性体现在哪里？</h2><p><strong>多次有放回的随机取样，多次随机取属性</strong></p>
<h2 id="3-随机森林为什么不容易过拟合？"><a href="#3-随机森林为什么不容易过拟合？" class="headerlink" title="3. 随机森林为什么不容易过拟合？"></a>3. 随机森林为什么不容易过拟合？</h2><ul>
<li><p>随机森林中的每一颗树都是过拟合的，拟合到非常小的细节上</p>
</li>
<li><p>随机森林通过引入随机性，使每一颗树拟合的细节不同</p>
</li>
<li><p>所有树组合在一起，过拟合的部分就会自动被消除掉。</p>
</li>
</ul>
<p>因此随机森林出现过拟合的概率相对低。</p>
<h2 id="4-为什么不用全样本训练？"><a href="#4-为什么不用全样本训练？" class="headerlink" title="4. 为什么不用全样本训练？"></a>4. 为什么不用全样本训练？</h2><p>全样本训练忽视了局部样本的规律（各个决策树趋于相同），对于模型的泛化能力是有害的，使随机森<br>林算法在样本层面失去了随机性。</p>
<h2 id="5-为什么要随机特征？"><a href="#5-为什么要随机特征？" class="headerlink" title="5. 为什么要随机特征？"></a>5. 为什么要随机特征？</h2><p>随机特征保证基分类器的多样性（差异性），最终集成的泛化性能可通过个体学习器之间的差异度而进<br>一步提升，从而提高泛化能力和抗噪能力。</p>
<h2 id="6-RF与-GBDT-的区别？"><a href="#6-RF与-GBDT-的区别？" class="headerlink" title="6. RF与 GBDT 的区别？"></a>6. RF与 GBDT 的区别？</h2><ul>
<li>随机森林将多棵决策树的结果进行投票后得到最终的结果，对不同的树的训练结果也没有做进一步的优化提升，将其称为<strong>Bagging算法。</strong></li>
<li>GBDT用到的是<strong>Boosting算法</strong>，在迭代的每一步构建弱学习器弥补原有模型的不足。GBDT中的Gradient Boost就是通过每次迭代的时候构建一个沿梯度下降最快的方向的学习器。并且通过设置不同的损失函数可以处理各类学习任务(多分类、回归等)。</li>
</ul>
<h2 id="7-RF为什么比Bagging效率高？"><a href="#7-RF为什么比Bagging效率高？" class="headerlink" title="7. RF为什么比Bagging效率高？"></a>7. RF为什么比Bagging效率高？</h2><p>Bagging无随机特征，使得训练决策树时效率更低</p>
<h2 id="8-你已经建了一个有10000棵树的随机森林模型。在得到0-001的训练误差后，你非常高兴。但是，验证错误是34-23。到底是怎么回事？你还没有训练好你的模型吗？"><a href="#8-你已经建了一个有10000棵树的随机森林模型。在得到0-001的训练误差后，你非常高兴。但是，验证错误是34-23。到底是怎么回事？你还没有训练好你的模型吗？" class="headerlink" title="8. 你已经建了一个有10000棵树的随机森林模型。在得到0.001的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？"></a>8. 你已经建了一个有10000棵树的随机森林模型。在得到0.001的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？</h2><p>训练误差为0：<strong>模型过度拟合</strong></p>
<p>验证错误为34.23：该分类器用于<strong>未看见的样本</strong>上时，找不到已有的模式</p>
<p>因此，为了避免这些情况，要用交叉验证来调整树的数量。</p>
<h2 id="9-如何使用随机森林对特征重要性进行评估？"><a href="#9-如何使用随机森林对特征重要性进行评估？" class="headerlink" title="9. 如何使用随机森林对特征重要性进行评估？"></a>9. 如何使用随机森林对特征重要性进行评估？</h2><p><strong>袋外数据(OOB)</strong>： 大约有1/3的训练实例没有参与第k棵树的生成，它们称为第$k$棵树的袋外数据样本。 </p>
<p>在随机森林中某个特征$X$的重要性的计算方法如下：</p>
<ul>
<li>对于随机森林中的每一颗决策树，使用相应的OOB(袋外数据)来计算它的袋外数据误差，记为$errooB1$。</li>
<li>随机地对袋外数据OOB所有样本的特征$X$加入噪声干扰(就可以随机的改变样本在特征X处的值)，再次计算它的袋外数据误差，记为$errooB2$。</li>
<li>假设随机森林中有$N$棵树，那么对于特征$X$的重要性为$(errOOB2-errOOB1)/N)$，之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后，袋外的准确率大幅度降低，则说明这个特征对于样本的分类结果影响很大，也就是说它的重要程度比较高。</li>
</ul>
<h2 id="10-随机森林算法训练时主要需要调整哪些参数？"><a href="#10-随机森林算法训练时主要需要调整哪些参数？" class="headerlink" title="10. 随机森林算法训练时主要需要调整哪些参数？"></a><em>10. 随机森林算法训练时主要需要调整哪些参数？</em></h2><ul>
<li><p><strong>n_estimators:</strong>随机森林建立子树的数量。<br>较多的子树一般可以让模型有更好的性能，但同时让你的代码变慢。需要选择最佳的随机森林子树数量</p>
</li>
<li><p><strong>max_features：</strong>随机森林允许单个决策树使用特征的最大数量。<br>增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。但是，可以肯定，你通过增加max_features会降低算法的速度。因此，你需要适当的平衡和选择最佳max_features。</p>
</li>
<li><p><strong>max_depth：</strong> 决策树最大深度</p>
<p>默认决策树在建立子树的时候不会限制子树的深度</p>
</li>
<li><p><strong>max_leaf_nodes：</strong> 最大叶子节点数</p>
<p>通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。</p>
</li>
<li><p><strong>min_samples_split：</strong>内部节点再划分所需最小样本数<br>内部节点再划分所需最小样本数，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。</p>
</li>
<li><p><strong>min_samples_leaf：</strong> 叶子节点最少样本</p>
<p>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。</p>
</li>
<li><p><strong>min_impurity_split：</strong> 节点划分最小不纯度<br>这个值限制了决策树的增长，如果某节点的不纯度（基于基尼系数，均方差）小于这个阈值，则该节点不再生成子节点。即为叶子节点。一般不推荐改动默认值1e-7。</p>
</li>
</ul>
<h2 id="11-随机森林的优缺点"><a href="#11-随机森林的优缺点" class="headerlink" title="11. 随机森林的优缺点"></a>11. 随机森林的优缺点</h2><ul>
<li>优点<ul>
<li>训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。</li>
<li>由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。</li>
<li>在训练后，可以给出各个特征对于输出的重要性</li>
<li>由于采用了随机采样，训练出的模型的方差小，泛化能力强。</li>
<li>相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。</li>
<li>对部分特征缺失不敏感。</li>
</ul>
</li>
<li>缺点<ul>
<li>在某些噪音比较大的样本集上，RF模型容易陷入过拟合。</li>
<li>取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。</li>
</ul>
</li>
</ul>
<h2 id="12-简述一下Adaboost原理"><a href="#12-简述一下Adaboost原理" class="headerlink" title="12. 简述一下Adaboost原理"></a>12. 简述一下Adaboost原理</h2><p>Adaboost算法利用同一种基分类器（弱分类器），<strong>基于分类器的错误率分配不同的权重参数，最后累加加权的预测结果</strong>作为输出。</p>
<ul>
<li>Adaboost算法流程：<ul>
<li>样本赋予权重，得到第一个分类器。</li>
<li>计算该分类器的错误率，根据错误率赋予分类器权重（注意这里是<strong>分类器的权重</strong>）。</li>
<li>增加分错样本的权重，减小分对样本的权重（注意这里是<strong>样本的权重</strong>）。</li>
<li>然后再用<strong>新的样本权重</strong>训练数据，得到新的分类器。</li>
<li>多次迭代，直到分类器错误率为0或者整体弱分类器错误为0，或者到达迭代次数。</li>
<li>将所有弱分类器的结果加权求和，得到一个较为准确的分类结果。错误率低的分类器获得更高的决定系数，从而在对数据进行预测时起关键作用。</li>
</ul>
</li>
</ul>
<h2 id="13-AdaBoost的优点和缺点"><a href="#13-AdaBoost的优点和缺点" class="headerlink" title="13. AdaBoost的优点和缺点"></a>13. AdaBoost的优点和缺点</h2><ul>
<li>优点<ul>
<li>Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也不存在过拟合的现象。</li>
<li>Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。</li>
<li>Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。</li>
<li>Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。</li>
<li>Adaboost对同一个训练样本集训练不同的弱分类器，按照一定的方法把这些弱分类器集合起来，构造一个分类能力很强的强分类器，即“三个臭皮匠赛过一个诸葛亮””。</li>
</ul>
</li>
<li>缺点<ul>
<li>在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。</li>
<li>Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。</li>
</ul>
</li>
</ul>
<h2 id="14-Adaboost对噪声敏感吗？"><a href="#14-Adaboost对噪声敏感吗？" class="headerlink" title="14. Adaboost对噪声敏感吗？"></a>14. Adaboost对噪声敏感吗？</h2><p>在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。</p>
<h2 id="15-Adaboost和随机森林算法的异同点"><a href="#15-Adaboost和随机森林算法的异同点" class="headerlink" title="15. Adaboost和随机森林算法的异同点"></a>15. Adaboost和随机森林算法的异同点</h2><p>随机森林和Adaboost算法都可以用来分类，它们都是优秀的基于决策树的组合算法。</p>
<ul>
<li>相同之处<ul>
<li>二者都是Bootsrap自助法选取样本。</li>
<li>二者都是要训练很多棵决策树。</li>
</ul>
</li>
<li>不同之处<ul>
<li>Adaboost是基于Bagging的算法，随机森林是基于Boosting的算法。</li>
<li>Adaboost后面树的训练，其在变量抽样选取的时候，对于上一棵树分错的样本，抽中的概率会加大。</li>
<li>随机森林在训练每一棵树的时候，随机挑选了部分特征作为拆分特征，而不是所有的特征都去作为拆分特征。</li>
<li>在预测新数据时，Adaboost中所有的树加权投票来决定因变量的预测值，每棵树的权重和错误率有关；随机森林按照所有树中少数服从多数树的分类值来决定因变量的预测值（或者求取树预测的平均值）。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>RandomForest</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-集成学习</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>[TOC]</p>
<h2 id="1-什么是集成学习算法？"><a href="#1-什么是集成学习算法？" class="headerlink" title="1. 什么是集成学习算法？"></a>1. 什么是集成学习算法？</h2><ul>
<li><strong>集成学习算法是一种优化手段或者策略</strong>，不算是一种机器学习算法。</li>
<li>集成方法是由多个较弱的模型集成模型组，一般的弱分类器可以是决策树，SVM，KNN等构成。其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。</li>
<li>该算法主要的问题是要找出哪些较弱的模型可以结合起来，以及如何结合的方法。</li>
</ul>
<a id="more"></a>
<h2 id="2-集成学习主要有哪几种框架？"><a href="#2-集成学习主要有哪几种框架？" class="headerlink" title="2. 集成学习主要有哪几种框架？"></a>2. 集成学习主要有哪几种框架？</h2><p>集成学习从集成思想的架构分为Bagging，Boosting，Stacking三种。</p>
<h2 id="3-简单介绍一下bagging，常用bagging算法有哪些？"><a href="#3-简单介绍一下bagging，常用bagging算法有哪些？" class="headerlink" title="3. 简单介绍一下bagging，常用bagging算法有哪些？"></a>3. 简单介绍一下bagging，常用bagging算法有哪些？</h2><ul>
<li>Bagging<ul>
<li><strong>多次采样，训练多个分类器，集体投票，旨在减小方差</strong>，</li>
</ul>
</li>
<li><p>基于数据<strong>随机重抽样</strong>的分类器构建方法。从训练集中进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。</p>
</li>
<li><p>算法流程：</p>
<ul>
<li>输入为样本集$D={(x_1，y_1)，(x_2，y_2) \dots (x_m，y_m)}$，弱学习器算法，弱分类器迭代次数$T$。</li>
<li>输出为最终的强分类器$f(x)$</li>
</ul>
</li>
<li><p>对于$t=1，2 \dots T$</p>
<ul>
<li>对训练集进行第t次随机采样，共采集$T$次，得到包含$T$个样本的采样集$D_t$</li>
<li>用采样集$D_t$训练第$t$个弱学习器$G_t(x)$</li>
</ul>
</li>
<li><p>如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。<br>常用bagging算法：随机森林算法</p>
</li>
</ul>
<h2 id="4-简单介绍一下boosting，常用boosting算法有哪些？"><a href="#4-简单介绍一下boosting，常用boosting算法有哪些？" class="headerlink" title="4. 简单介绍一下boosting，常用boosting算法有哪些？"></a>4. 简单介绍一下boosting，常用boosting算法有哪些？</h2><ul>
<li>Boosting<ul>
<li><strong>基分类器层层叠加，聚焦分错的样本，旨在减小方差</strong></li>
</ul>
</li>
<li>涉及到两个部分，加法模型和前向分步算法。<ul>
<li>加法模型就是说强分类器由一系列弱分类器线性相加而成。</li>
<li>前向分步就是说在训练过程中，下一轮迭代产生的分类器是在上一轮的基础上训练得来的。</li>
</ul>
</li>
<li><p>算法流程：</p>
<ul>
<li><p>给定初始训练数据，由此训练出第一个基学习器；</p>
</li>
<li><p>根据基学习器的表现对样本进行调整，在之前学习器做错的样本上投入更多关注；</p>
</li>
<li>用调整后的样本，训练下一个基学习器；</li>
<li>重复上述过程T次，将T个学习器加权结合。</li>
</ul>
</li>
<li><p>常用boosting算法：</p>
<ul>
<li>Adaboost<ul>
<li>样本权重：思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。——会因为错误分类样本的权重不断提高而使得模型对噪声敏感。</li>
<li>弱分类器权重：采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。</li>
</ul>
</li>
<li>GBDT</li>
<li>XGBoost</li>
</ul>
</li>
</ul>
<h2 id="5-boosting思想的数学表达式是什么？"><a href="#5-boosting思想的数学表达式是什么？" class="headerlink" title="5. boosting思想的数学表达式是什么？"></a>5. boosting思想的数学表达式是什么？</h2><script type="math/tex; mode=display">
f(x)=w_{0}+\sum_{m=1}^{M} w_{m} \phi_{m}(x)</script><p>其中$w$是权重，$\phi$是弱分类器的集合，可以看出最终就是基函数的线性组合。</p>
<h2 id="6-简单介绍一下stacking，常用stacking算法有哪些？"><a href="#6-简单介绍一下stacking，常用stacking算法有哪些？" class="headerlink" title="6. 简单介绍一下stacking，常用stacking算法有哪些？"></a>6. 简单介绍一下stacking，常用stacking算法有哪些？</h2><ul>
<li>Stacking<ul>
<li><strong>多次采样，训练多个分类器，将输出作为最后的输入特征</strong></li>
</ul>
</li>
<li><p>将训练好的所有基模型对训练集进行预测，第个$i$基模型对第$i$个训练样本的预测值将作为新的训练集中第$i$个样本的第$i$个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。</p>
</li>
<li><p>stacking常见的使用方式：</p>
<ul>
<li>由k-NN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的Loqistic 回归组合。</li>
</ul>
</li>
</ul>
<h2 id="7-你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？"><a href="#7-你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？" class="headerlink" title="7. 你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？"></a>7. 你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？</h2><p>低偏差意味着模型的预测值接近实际值。换句话说，该模型有足够的灵活性，以模仿训练数据的分布。貌似很好，但是别忘了，一个灵活的模型没有泛化能力。这意味着，当这个模型用在对一个未曾见过的数据集进行测试的时候，它会令人很失望。<br>在这种情况下，我们可以使用bagging算法（如随机森林），以解决高方差问题。bagging算法把数据集分成重复随机取样形成的子集。然后，这些样本利用单个学习算法生成一组模型。接着，利用投票（分类）或平均（回归）把模型预测结合在一起。另外，为了应对大方差，我们可以：</p>
<ul>
<li>使用正则化技术，惩罚更高的模型系数，从而降低了模型的复杂性。</li>
<li>使用可变重要性图表中的前n个特征。</li>
<li>可以用于当一个算法在数据集中的所有变量里很难寻找到有意义信号的时候。</li>
</ul>
<h2 id="8-常用的基分类器是什么？"><a href="#8-常用的基分类器是什么？" class="headerlink" title="8. 常用的基分类器是什么？"></a>8. 常用的基分类器是什么？</h2><p>最常用的基分类器是决策树,原因:</p>
<ul>
<li>决策树是低偏差高方差的，适合作为基分类器</li>
<li>决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重。</li>
<li>决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。</li>
<li>数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，<strong>很好地引入了随机性。</strong></li>
</ul>
<h2 id="9-可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？"><a href="#9-可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？" class="headerlink" title="9. 可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？"></a>9. 可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？</h2><p>不能：</p>
<ul>
<li>Bagging的主要好处是集成后的分类器的方差，比基分类器的方差小。</li>
<li><p>随机森林属于Bagging类的集成学习，对样本分布较为敏感的分类器更适用于Bagging。</p>
</li>
<li><p>线性分类器或者K-近邻都是较为稳定的分类器，本身方差就不大。</p>
</li>
<li>线性分类器或者K-近邻可能会由于Bagging的采样，导致在训练中更难收敛，增大偏差。</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>优化器</title>
    <url>/2022/01/01/%E4%BC%98%E5%8C%96%E5%99%A8/</url>
    <content><![CDATA[<h1 id="GD-amp-SGD-amp-Mini-batch-GD"><a href="#GD-amp-SGD-amp-Mini-batch-GD" class="headerlink" title="GD&amp;SGD&amp;Mini-batch GD"></a>GD&amp;SGD&amp;Mini-batch GD</h1><script type="math/tex; mode=display">
\theta=\theta-\eta \cdot \nabla_{\theta} J(\theta)</script><a id="more"></a>
<ul>
<li>GD<ul>
<li>用了全量样本，凸函数可以到全局最小值，非凸函数可以到局部最小值</li>
<li>慢，不能在线serving</li>
</ul>
</li>
<li>SGD<ul>
<li>快</li>
<li>损失函数震荡</li>
</ul>
</li>
<li>Mini-batch GD<ul>
<li>GD与SGD的结合：降低了参数更新时的方差，收敛较SGD更稳定</li>
<li>速度较快：可以利用矩阵运算</li>
</ul>
</li>
</ul>
<p>两个问题</p>
<ul>
<li>收敛能力<ul>
<li>lr太大，容易震荡，甚至偏离极小值；lr太小，收敛慢</li>
<li>非凸函数，容易陷于局部最小值/鞍点</li>
</ul>
</li>
<li>自适应lr调节<ul>
<li>SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。<br>Mini-batch gradient descent 不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）<br>对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是训练集全集带入即BGD，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动。）</li>
</ul>
</li>
</ul>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><ul>
<li>加上惯性，尝试解决容易陷入局部最小值/鞍点的问题<ul>
<li>使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡<br>$ v<em>{t-1} $上加梯度，可以认为是对 $ v</em>{t-1} $ 的预判</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{array}{l}
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\
\theta=\theta-v_{t}
\end{array}</script><ul>
<li>由Polyak提出，也叫Polyak动量<br>Nesterov</li>
<li>优化Polyak动量，“预判我的预判”<script type="math/tex; mode=display">\begin{array}{ll}
v_{t} & =\beta v_{t-1}+\nabla_{\theta} J\left(\theta_{t}-\eta \beta v_{t-1}\right) \\
\theta_{t+1} & =\theta_{t}-\eta v_{t} \end{array}</script></li>
<li>从工程实现上，无法直接获得预判点的梯度，可得如下等价公式：<script type="math/tex; mode=display">\begin{array}{ll}
v_{t} & =\beta v_{t-1}+g_{t}+\beta\left(g_{t}-g_{t-1}\right) \\
\theta_{t+1} & =\theta_{t}-\eta v_{t}
\end{array}</script></li>
</ul>
<h1 id="AdaGrad-amp-RMSProp-amp-Adam"><a href="#AdaGrad-amp-RMSProp-amp-Adam" class="headerlink" title="AdaGrad&amp;RMSProp&amp;Adam"></a>AdaGrad&amp;RMSProp&amp;Adam</h1><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><script type="math/tex; mode=display">\Large \begin{cases} 
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
v_t = v_{t-1} + g_t^2 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{v_t + \epsilon}} g_t
\end{cases}</script><h2 id="RMSPropV2"><a href="#RMSPropV2" class="headerlink" title="RMSPropV2"></a>RMSPropV2</h2><script type="math/tex; mode=display">\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
v_t = \beta v_{t-1} + g_t^2 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{v_t + 1}} g_t
\end{cases}</script><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><script type="math/tex; mode=display">\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\gamma = \frac{\sqrt{1-\beta_2^{t}}}{1-\beta_1^t} \\
\theta_{t+1} = \theta_{t} - \eta \frac{\gamma}{\sqrt{v_t +  \epsilon}} m_t
\end{cases}</script><ul>
<li>AdaGrad: Adaptive Gradient<ul>
<li>学习率的自适应调节：学习率 除以 梯度平方累积项，更新次数越多（高频特征）的更新步长减小，稀疏特征步长增大</li>
<li>后期学不动</li>
</ul>
</li>
<li>RMSProp：Root Mean Square Propogation，为了解决AdaGrad后期学不动的问题<ul>
<li>累积梯度时加上衰减系数，越早的梯度会逐渐衰减</li>
</ul>
</li>
<li>Adam：Adaptive Moment Optimization, <ul>
<li>RMSProp + Momentum </li>
</ul>
</li>
</ul>
<p><strong>以下内容未解锁</strong></p>
<!-- # AdaMom: An optional optimizer for Dense Module
$$\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t = \beta_2 v_{t-1} + g_t^2 \\
c_t = \beta_2 c_{t-1} + 1 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{\frac{v_t}{c_t} +  \epsilon}} m_t
\end{cases}$$
- Adam的基础上，对二阶动量求累计更新次数的平均
- Momentum
 Adagrad仅使用二阶动量，导致累计梯度平方和单调增长，在某些维度变量频繁更新之后，会导致累计梯度平方和极大，参数几乎不再更新，也就是我们常说的学习不动了。
- 在AdaMom中，我们单纯对二阶动量做累计更新次数的均值，缓解上述学习不动的情况。同时继承Adam算法中引入一阶动量的优势。其次，为了更适应在线学习的场景，我们去除了计算二阶动量时在gradient平方上的decay。在这些调整的基础上，可以将学习率设置更小，对学习稳定性和快速收敛带来优势。同时我们观察到adamom训练过程产生的gradient远小于其他优化器，印证了对学习的稳定性和长期更新的判断。但目前没有进行数学论证。
- 和Adam的不同在于早期学习率的差距
AdaMom可以看成Adam的生产环境改进版，前期降低了m_t的方差/自带了warmup，有效缓解了Dense部分在异步更新场景初期可能出现的梯度爆炸

# AdaNest: an adaptive dense optimizer with Nesterov acceleration

- 公式
  - 1: Nesterov的工程实现
  - 2: 保证所有系数和为1，类似配分函数
  - 3: 继承于AdaMom，Vt除以梯度更新次数，借此解决AdaGrad的梯度衰减问题 
- 保留Adam/AdaMom二阶动量特性的同时，使用更加激进的Nesterov动量替换Polyak动量，提升了效果
- 从引入预判的角度看，AdaNest因为使用了预测点的梯度，对梯度的估计更加准确，总体比Adam更激进
- Adam与RMSProp的折衷，相比RMSProp引入了动量，相比Adam赋予$$g_t$$ 更大的权重


# GroupLasso
- FTRL的vector版本，为了增强vector的稀疏性，以此来减小PS内存
Summary -->
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DL</tag>
        <tag>优化器</tag>
      </tags>
  </entry>
  <entry>
    <title>学习资料-会议&amp;期刊</title>
    <url>/2021/12/10/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99-%E4%BC%9A%E8%AE%AE&amp;%E6%9C%9F%E5%88%8A/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><ul>
<li><a href="https://recsys.acm.org/">RecSys</a> -The ACM Conference Series on Recommender Systems.</li>
</ul>
<a id="more"></a>
<h2 id="信息检索"><a href="#信息检索" class="headerlink" title="信息检索"></a>信息检索</h2><ul>
<li><a href="http://sigir.org/">SIGIR</a>- The ACM International Conference on Research and Development in Information Retrieval</li>
</ul>
<h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><ul>
<li><a href="http://www.kdd.org/">KDD</a> - The ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</li>
<li><a href="http://www.wsdm-conference.org/">WSDM</a> - The International Conference on Web Search and Data Mining.</li>
<li><a href="http://cs.uvm.edu/~icdm/">ICDM</a> - The IEEE International Conference on Data Mining.</li>
<li><a href="http://www.siam.org/meetings/sdm19/">SDM</a> -The SIAM International Conference on Data Mining.</li>
<li><a href="https://ecmlpkdd2020.net/">ECML-PKDD</a> - The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</li>
</ul>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ul>
<li><a href="https://icml.cc/">ICML</a> - The International Conference on Machine Learning.</li>
<li><a href="https://nips.cc/">NIPS</a> - The Conference on Neural Information Processing Systems</li>
</ul>
<h2 id="AI"><a href="#AI" class="headerlink" title="AI"></a>AI</h2><ul>
<li><a href="https://aaai.org/Conferences/AAAI-18/">AAAI</a> - The National Conference of the American Association for Artificial Intelligence.</li>
<li><a href="http://www.ijcai.org/">IJCAI</a> - The International Joint Conference on Artificial Intelligence.</li>
<li><a href="http://ecai2020.eu/">ECAI</a> -European Conference on Artificial Intelligence</li>
<li><a href="http://www.auai.org/uai2020/">UAI</a> - The Conference on Uncertainty in Artificial Intelligence</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Resource</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Feature Engineering</title>
    <url>/2021/12/10/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="特征离散的作用"><a href="#特征离散的作用" class="headerlink" title="特征离散的作用"></a>特征离散的作用</h2><a id="more"></a>
<ul>
<li><strong>离散化后的特征对异常数据有很强的鲁棒性</strong><br>比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；离散化后年龄300岁也只对应于一个权重，如果训练数据中没有出现特征”年龄-300岁”，那么在LR模型中，其权重对应于0，所以，即使测试数据中出现特征”年龄-300岁”,也不会对预测结果产生影响。特征离散化的过程，比如特征A，如果当做连续特征使用，在LR模型中，A会对应一个权重w,如果离散化，那么A就拓展为特征A-1，A-2，A-3…,每个特征对应于一个权重，如果训练样本中没有出现特征A-4，那么训练的模型对于A-4就没有权重，如果测试样本中出现特征A-4,该特征A-4也不会起作用。相当于无效。但是，如果使用连续特征，在LR模型中，y = w*a,a是特征，w是a对应的权重,比如a代表年龄，那么a的取值范围是[0..100]，如果测试样本中,出现了一个测试用例，a的取值是300，显然a是异常值，但是w*a还是有值，而且值还非常大，所以，异常值会对最后结果产生非常大的影响。</li>
<li><strong>特征离散化后，模型会更稳定</strong><br>比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；按区间离散化，划分区间是非常关键的。</li>
<li><strong>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险</strong><br>当使用连续特征时，一个特征对应于一个权重，那么，如果这个特征权重较大，模型就会很依赖于这个特征，这个特征的一个微小变化可能会导致最终结果产生很大的变化，这样子的模型很危险，当遇到新样本的时候很可能因为对这个特征过分敏感而得到错误的分类结果，也就是泛化能力差，容易过拟合。而使用离散特征的时候，一个特征变成了多个，权重也变为多个，那么之前连续特征对模型的影响力就被分散弱化了，从而降低了过拟合的风险。</li>
<li><strong>引入非线性</strong><br>单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；在LR模型中，特征A作为连续特征对应的权重是Wa。A是线性特征，因为y = Wa*A,y对于A的导数就是Wa,如果离散化后，A按区间离散化为A_1,A_2,A_3。那么y = w_1*A_1+w_2*A_2+w_3*A_3.那么y对于A的函数就相当于分段的线性函数，y对于A的导数也随A的取值变动，所以，相当于引入了非线性。</li>
<li><strong>离散化后方便进行特征交叉</strong><br>离散化后变为了类别，方便特征交叉。进一步引入非线性，提升表达能力。</li>
<li>离散特征的增加和减少都很容易，易于模型的快速迭代。(离散特征的增加和减少，模型也不需要调整，重新训练是必须的，相比贝叶斯推断方法或者树模型方法迭代快)</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>FeatureEngineering</tag>
      </tags>
  </entry>
  <entry>
    <title>CV-SIFT</title>
    <url>/2021/12/10/CV-SIFT/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]</p>
<h2 id="SIFT-Scale-invariant-feature-transform-综述"><a href="#SIFT-Scale-invariant-feature-transform-综述" class="headerlink" title="SIFT(Scale-invariant feature transform)综述"></a>SIFT(Scale-invariant feature transform)综述</h2><ul>
<li>尺度不变特征转换：SIFT是一种在<strong>多尺度空间上提取局部特征</strong>的方法。它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由 David Lowe在1999年所发表，2004年完善总结。</li>
</ul>
<a id="more"></a>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性；</li>
<li>独特性（Distinctiveness）好，信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配；</li>
<li>多量性，即使少数的几个物体也可以产生大量的SIFT特征向量；</li>
<li>高速性，经优化的SIFT匹配算法甚至可以达到实时的要求；</li>
<li>可扩展性，可以很方便的与其他形式的特征向量进行联合。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>实时性不高</li>
<li>有时特征点数量较少</li>
<li>对边缘光滑的目标无法准确提取特征点</li>
</ul>
<h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><ol>
<li>建立尺度空间，即DoG金字塔</li>
<li>关键点检测及定位，获得每个关键点的：<strong>尺度、位置</strong></li>
<li>特征点方向赋值，获得每个特征点的：<strong>方向</strong></li>
<li>关键点特征描述</li>
</ol>
<p><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330204125547-535493662.png" alt="整体流程"></p>
<h2 id="建立DoG金字塔"><a href="#建立DoG金字塔" class="headerlink" title="建立DoG金字塔"></a>建立DoG金字塔</h2><ul>
<li>获取多尺度空间：Lindeberg等人已证明高斯卷积核是实现尺度变换的唯一变换核，并且是唯一的线性核</li>
</ul>
<h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><h4 id="二维高斯函数"><a href="#二维高斯函数" class="headerlink" title="二维高斯函数"></a>二维高斯函数</h4><ul>
<li>以(m/2, n/2)为中心的二维高斯分布<script type="math/tex; mode=display">
G(x, y)=\frac{1}{2 \pi \sigma^{2}} e^{-\frac{(x-m / 2)^{2}+(y-n / 2)^{2}}{2 \sigma^{2}}}</script></li>
<li>生成的曲面等高线是从中心开始呈正态分布的同心圆，权重值从中心向周围越来越小。这样进行模糊处理比其它的均衡模糊滤波器更高地保留了边缘效果。</li>
<li>实际应用中，在计算高斯函数的离散近似时，在大概3σ距离之外的像素都可以看作不起作用，因此图像处理程序只需要计算$<br>(6 \sigma+1) \times(6 \sigma+1)<br>$的矩阵</li>
</ul>
<h4 id="二维高斯函数的优化"><a href="#二维高斯函数的优化" class="headerlink" title="二维高斯函数的优化"></a>二维高斯函数的优化</h4><ol>
<li>若高斯模板大小为m*n，当图像大小为M*N时，计算复杂度为O(m*n*M*N)</li>
<li>$\sigma$越大，高斯模板分散性越强，处理边缘点时，丢失的图像信息越多，可能会造成黑边</li>
</ol>
<ul>
<li>可以证明，二维高斯运算可以优化为：水平方向进行一维高斯矩阵变换加上竖直方向的一维高斯矩阵变换得到。时间复杂度为：$<br>O(n \times M \times N)+O(m \times M \times N)<br>$。<a href="https://blog.csdn.net/lwx309025167/article/details/82761474">证明</a><br><img src="https://img-blog.csdn.net/20180919111255142?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3eDMwOTAyNTE2Nw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="证明"></li>
</ul>
<h4 id="高斯拉普拉斯算子-Laplacion-of-Gaussian-LoG"><a href="#高斯拉普拉斯算子-Laplacion-of-Gaussian-LoG" class="headerlink" title="高斯拉普拉斯算子(Laplacion of Gaussian, LoG)"></a>高斯拉普拉斯算子(Laplacion of Gaussian, LoG)</h4><ul>
<li>作用：突出边缘，更清晰，对比度更强</li>
<li>拉普拉斯：拉普拉斯算子是图像二阶空间导数的二维各向同性测度。<strong>拉普拉斯算子可以突出图像中强度发生快速变化的区域</strong>，因此常用在边缘检测任务当中。</li>
<li>高斯：在进行Laplacian操作之前通常需要先用高斯平滑滤波器对图像进行平滑处理，以<strong>降低Laplacian操作对于噪声的敏感性</strong>。</li>
<li>输入输出均为灰度图</li>
<li>连续拉普拉斯：<script type="math/tex; mode=display">
L(x, y)=\frac{\partial^{2} I}{\partial x^{2}}+\frac{\partial^{2} I}{\partial y^{2}}</script></li>
<li>离散拉普拉斯，例如：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>0</th>
<th>-1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>-1</td>
<td>4</td>
<td>-1</td>
</tr>
<tr>
<td>0</td>
<td>-1</td>
<td>-1</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>-1</th>
<th>-1</th>
<th>-1</th>
</tr>
</thead>
<tbody>
<tr>
<td>-1</td>
<td>8</td>
<td>-1</td>
</tr>
<tr>
<td>-1</td>
<td>-1</td>
<td>-1</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>表达：以0为中心，$\sigma$为标准差的LoG<script type="math/tex; mode=display">
L o G(x, y)=-\frac{1}{\pi \sigma^{4}}\left[1-\frac{x^{2}+y^{2}}{2 \sigma^{2}}\right] e^{-\frac{x^{2}+y^{2}}{2 \sigma^{2}}}</script><img src="https://pic1.zhimg.com/80/v2-f824bd5eae07235ebf531fa0b546ba98_1440w.jpg" alt="LoG示意图"><blockquote>
<p>推导过程：<img src="https://pic4.zhimg.com/80/v2-b220086b94625a8a00ce68eb2a4bd0e3_1440w.jpg" alt="推导过程"></p>
</blockquote>
</li>
<li>示例：一维LoG滤波器对于边缘的响应：<img src="https://pic4.zhimg.com/80/v2-12ce9caee0278bf8855efdecfcba7fc7_1440w.jpg" alt="一维LoG滤波器对于边缘的响应"></li>
</ul>
<h4 id="高斯差分算子-Difference-of-Gaussian-DOG"><a href="#高斯差分算子-Difference-of-Gaussian-DOG" class="headerlink" title="高斯差分算子(Difference of Gaussian, DOG)"></a>高斯差分算子(Difference of Gaussian, DOG)</h4><ul>
<li>可以近似LoG算子，减少运算量，由Lindeberg在1994证明</li>
<li>DoG的运算如下，即DoG为两不同尺度的高斯算子平滑后的图像之差，具体证明见：<a href="https://blog.csdn.net/kieven2008/article/details/104309440">证明</a><script type="math/tex; mode=display">
\begin{aligned}
D(x, y, \sigma) &=(G(x, y, k \sigma)-G(x, y, \sigma)) * I(x, y) \\
&=L(x, y, k \sigma)-L(x, y, \sigma)
\end{aligned}</script></li>
</ul>
<h4 id="尺度空间理论"><a href="#尺度空间理论" class="headerlink" title="尺度空间理论"></a>尺度空间理论</h4><ul>
<li>定义<ul>
<li>单尺度 —-&gt; 多尺度</li>
<li>尺度空间中各尺度图像的模糊程度逐渐变大，能够模拟人在距离目标由近到远时目标在视网膜上的形成过程。</li>
</ul>
</li>
<li>要求：满足视觉不变性<ul>
<li>亮度/灰度不变性，对比度不变性</li>
<li>平移不变性、尺度不变性、欧几里得不变性、仿射不变性</li>
</ul>
</li>
<li>方法：<ul>
<li>Tony Lindeberg指出<strong>尺度规范化的LoG(Laplacion of Gaussian)算子具有真正的尺度不变性</strong>。Lowe使用高斯差分金字塔近似LoG算子，是尺度空间检测稳定的关键点。</li>
</ul>
</li>
<li>表示：一个图像的尺度空间$L(x,y,\sigma)$，可以表示为一个变化尺度的高斯函数与原图像的卷积<script type="math/tex; mode=display">
L(x, y, \sigma)=G(x, y, \sigma) * I(x, y)</script><script type="math/tex; mode=display">
G(x, y, \sigma)=\frac{1}{2 \pi \sigma^{2}} e^{-\frac{(x-m / 2)^{2}+(y-n / 2)^{2}}{2 \sigma^{2}}}</script><ul>
<li>m，n表示高斯模板中心，维度为$<br>(6 \sigma+1) \times(6 \sigma+1)<br>$，(x, y)代表图像的像素位置。$\sigma$是尺度空间因子，值越小表示图像被平滑的越少，相应的尺度也就越小。<strong>大尺度对应于图像的概貌特征，小尺度对应于图像的细节特征。</strong></li>
</ul>
</li>
</ul>
<h3 id="建立DoG金字塔-1"><a href="#建立DoG金字塔-1" class="headerlink" title="建立DoG金字塔"></a>建立DoG金字塔</h3><ul>
<li>极值：图像域和尺度域上的极值<br><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330204227104-18141123.png" alt="步骤"></li>
</ul>
<h4 id="构建高斯金字塔"><a href="#构建高斯金字塔" class="headerlink" title="构建高斯金字塔"></a>构建高斯金字塔</h4><ol>
<li>先将原图像扩大一倍之后作为高斯金字塔的第1组第1层，将第1组第1层图像经高斯卷积（其实就是高斯平滑或称高斯滤波）之后作为第1组金字塔的第2层。对于参数σ，在Sift算子中取的是固定值1.6。</li>
<li>将σ乘以一个比例系数k，得到一个新的平滑因子σ=k*σ，用它来平滑第1组第2层图像，结果图像作为第3层。</li>
<li>如此这般重复，最后得到L层图像，在同一组中，每一层图像的尺寸都是一样的，只是平滑系数不一样。它们对应的平滑系数分别为：0，σ，kσ，k^2σ,k^3σ……k^(L-2)σ。</li>
<li>将第1组<strong>倒数第三层</strong>图像作比例因子为2的降采样，得到的图像作为第2组的第1层，然后对第2组的第1层图像做平滑因子为σ的高斯平滑，得到第2组的第2层，就像步骤2中一样，如此得到第2组的L层图像，同组内它们的尺寸是一样的，对应的平滑系数分别为：0，σ，kσ，k^2σ,k^3σ……k^(L-2)σ。但是在尺寸方面第2组是第1组图像的一半。</li>
<li>反复执行，得到一共O组，每组L层，共计O*L个图像，这些图像一起就构成了高斯金字塔，结构如下：<br><img src="https://img-blog.csdn.net/20160917212318336" alt="高斯金字塔"></li>
</ol>
<h4 id="构建高斯差分-DOG-金字塔"><a href="#构建高斯差分-DOG-金字塔" class="headerlink" title="构建高斯差分(DOG)金字塔"></a>构建高斯差分(DOG)金字塔</h4><ul>
<li>方法<br><img src="https://img-blog.csdn.net/20160917223500317" alt="高斯差分金字塔"></li>
<li>经过归一化的高斯差分金字塔<br><img src="https://img-blog.csdn.net/20160917232151713" alt="一个示例"></li>
</ul>
<h4 id="具体构建过程中的参数计算"><a href="#具体构建过程中的参数计算" class="headerlink" title="具体构建过程中的参数计算"></a>具体构建过程中的参数计算</h4><ul>
<li>极值点：每一个像素点要和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。要获取S个尺度的极值点—-&gt;需要S+2个DOG空间—&gt;S+3层高斯金字塔</li>
<li>实际计算时S在3到5之间</li>
<li>输入：组数o，层数S，$k=2^{\frac{1}{S}}$，以及计算每组每层的尺度参数$\sigma(o, s)=\sigma_{0} 2^{\circ+\frac{s}{S}} o \in[0, \ldots, O-1], s \in[0, \ldots, S+2]$<ul>
<li>$\sigma_{0}$为初始尺度</li>
</ul>
</li>
<li>相同组间不同层的尺度参数$\sigma$为k倍之差，不同组相同层之间尺度参数$\sigma$为2倍之差</li>
</ul>
<h2 id="关键点定位"><a href="#关键点定位" class="headerlink" title="关键点定位"></a>关键点定位</h2><h3 id="离散极值点检测"><a href="#离散极值点检测" class="headerlink" title="离散极值点检测"></a>离散极值点检测</h3><ul>
<li>DoG金字塔中，每个点与周围8+前后两个尺度9*2=26个点比较，来确定极值点</li>
</ul>
<h3 id="关键点（连续极值点）定位"><a href="#关键点（连续极值点）定位" class="headerlink" title="关键点（连续极值点）定位"></a>关键点（连续极值点）定位</h3><ul>
<li>动机<ul>
<li>获得连续空间的极值点（位置、尺度、极值大小），并去除低对比度的关键点</li>
<li>去除不稳定的边缘响应点（DoG算子会产生较强的边缘响应），增强匹配稳定性、提高降噪能力</li>
</ul>
</li>
</ul>
<h4 id="关键点定位-1"><a href="#关键点定位-1" class="headerlink" title="关键点定位"></a>关键点定位</h4><ul>
<li>离散空间的极值点只是局部区域<br><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330205035199-488432412.jpg" alt="离散空间极值点与连续空间极值点的区别"></li>
<li>为了获得连续空间的极值点，需要对离散空间DoG函数进行曲线插值/拟合。拟合的方法是利用DoG函数在尺度空间上的泰勒展开。</li>
</ul>
<ol>
<li>在任意一个坐标为$X_0=(x_0,y_0,\sigma_0)$的极值点处做DoG函数的泰勒展开，舍掉2阶以后的项，结果如下：<script type="math/tex; mode=display">
f\left(\left[\begin{array}{l}
x \\
y \\
\sigma
\end{array}\right]\right) \approx f\left(\left[\begin{array}{l}
x_{0} \\
y_{0} \\
\sigma_{0}
\end{array}\right]\right) \mid+\left[\begin{array}{lll}
\frac{\partial f}{\partial x} & \frac{\partial f}{\partial y} & \frac{\partial f}{\partial \sigma}
\end{array}\right]\left(\left[\begin{array}{l}
x \\
y \\
\sigma
\end{array}\right]-\left[\begin{array}{l}
x_{0} \\
y_{0} \\
\sigma_{0}
\end{array}\right]\right) + 
\frac{1}{2}\left(\left[\begin{array}{llll}
x & y & \sigma
\end{array}\right]-\left[\begin{array}{lll}
x_{0} & y_{0} & \sigma_{0}
\end{array}\right]\right)\left[\begin{array}{lll}
\frac{\partial^{2} f}{\partial x \partial x} & \frac{\partial^{2} f}{\partial x \partial y} & \frac{\partial^{2} f}{\partial x \partial \sigma} \\
\frac{\partial^{2} f}{\partial x \partial y} & \frac{\partial^{2} f}{\partial y \partial y} & \frac{\partial^{2} f}{\partial y \partial \sigma} \\
\frac{\partial^{2} f}{\partial x \partial \sigma} & \frac{\partial^{2} f}{\partial y \partial \sigma} & \frac{\partial^{2} f}{\partial \sigma \partial \sigma}
\end{array}\right]\left(\left[\begin{array}{l}
x \\
y \\
\sigma
\end{array}\right]-\left[\begin{array}{l}
x_{0} \\
y_{0} \\
\sigma_{0}
\end{array}\right]\right)</script>其向量形式如下：<script type="math/tex; mode=display">
D(X)=D+\frac{\partial D^{T}}{\partial X} X+\frac{1}{2} X^{T} \frac{\partial^{2} D}{\partial X^{2}} X</script></li>
<li>求导并让方程==0，求得极值点相对$X_0=(x_0,y_0,\sigma_0)$的偏移量为$\hat{X}=-\frac{\partial^{2} D^{-1}}{\partial X^{2}} \frac{\partial D}{\partial X}$。</li>
<li>当此偏移量在任一维度（x, y, $\sigma$）上大于0.5时，意味着此极值点已经去临近的点上了，因此改变该极值点位置。</li>
<li>不断迭代1~3。终止条件：收敛/超出迭代次数（Lowe设定为5）/超出图像边界（此时应删除该点）。记录所有点的位置（原位置+偏移量）以及尺度$\sigma(o, s)$.</li>
<li>为加强抗噪声的能力，删除$|D(x)|$过小的点（Lowe论文中使用0.03，Rob Hess等人实现时使用0.04)。</li>
</ol>
<h4 id="边缘效应消除"><a href="#边缘效应消除" class="headerlink" title="边缘效应消除"></a>边缘效应消除</h4><ul>
<li>一个定义不好的DoG算子的极值在横跨边缘的地方有较大的主曲率，而在垂直边缘的方向有较小的主曲率。即<strong>两个主曲率的比值越大</strong>。<ul>
<li>主曲率：曲面的每个方向都有法曲率，那么就有最大最小的法曲率，这个最大最小值就是主曲率，对应的曲线在这点的切线方向就是主曲率方向。这两个方向是垂直的。</li>
</ul>
</li>
</ul>
<ol>
<li>获取特征点处的Hessian矩阵，主曲率通过一个2x2的Hessian矩阵H求出：<script type="math/tex; mode=display">
H=\left[\begin{array}{ll}
D_{x x} & D_{x y} \\
D_{xy} & D_{yy}
\end{array}\right]</script></li>
<li>H的特征值$\alpha$和$\beta$代表了x方向和y方向的梯度。<ol>
<li>先求出H的对角线元素之和以及H的行列式<script type="math/tex; mode=display">
\begin{array}{l}
\operatorname{Tr}(H)=D_{x x}+D_{x}=\alpha+\beta \\
\operatorname{Det}(H)=D_{x x} D_{y}-\left(D_{x}\right)^{2}=\alpha \beta
\end{array}</script></li>
<li>设$\alpha$较大，令$\alpha=r \beta$，则$<br>\frac{T r(H)^{2}}{D e t(H)}=\frac{(\alpha+\beta)^{2}}{\alpha \beta}=\frac{(r \beta+\beta)^{2}}{r \beta^{2}}=\frac{(r+1)^{2}}{r}<br>$</li>
<li>r值越大，说明两个特征值的比值越大，即在某一个方向的梯度值越大，而在另一个方向的梯度值越小，而边缘恰恰就是这种情况。所以为了剔除边缘响应点，需要让r小于一定的阈值，因此，为了检测主曲率是否在某域值r下，只需检测$\frac{\operatorname{Tr}(H)^{2}}{\operatorname{Det}(H)}&lt;\frac{(r+1)^{2}}{r}$，若此式子成立，保留关键点，反之剔除。<ol>
<li>Lowe取r=10.</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><ul>
<li>以上的关键点定位过程中用到了离散的导数，具体求导时应用了<a href="https://blog.csdn.net/qq_41679006/article/details/80975436">有限差分法</a>求导</li>
<li>以上过程中需要用到三阶矩阵求逆，可逆矩阵A及其逆如下<script type="math/tex; mode=display">
A=\left(\begin{array}{lll}
a_{00} & a_{01} & a_{02} \\
a_{10} & a_{11} & a_{12} \\
a_{20} & a_{21} & a_{22}
\end{array}\right)</script><script type="math/tex; mode=display">
A^{-1}=\frac{1}{|A|}\left(\begin{array}{ccc}
a_{11} a_{22}-a_{21} a_{12} & -\left(a_{01} a_{22}-a_{21} a_{02}\right) & a_{01} a_{12}-a_{02} a_{11} \\
a_{12} a_{20}-a_{22} a_{10} & -\left(a_{02} a_{20}-a_{22} a_{00}\right) & a_{02} a_{10}-a_{00} a_{12} \\
a_{10} a_{21}-a_{20} a_{11} & -\left(a_{00} a_{21}-a_{20} a_{01}\right) & a_{00} a_{11}-a_{01} a_{10}
\end{array}\right)</script></li>
</ul>
<h2 id="关键点方向匹配"><a href="#关键点方向匹配" class="headerlink" title="关键点方向匹配"></a>关键点方向匹配</h2><ul>
<li>为了使最后所得的描述符具有旋转不变性，需要利用图像的局部特征为给每一个关键点分配一个基准方向。使用图像梯度的方法求取局部结构的稳定方向。</li>
</ul>
<ol>
<li>对于在DOG金字塔中检测出的关键点点，采集其所在高斯金字塔图像3σ邻域窗口内像素的梯度和方向分布特征。梯度的模值和方向如下：<script type="math/tex; mode=display">
\begin{array}{l}
m(x, y)=\sqrt{(L(x+1, y)-L(x-1, y))^{2}+(L(x, y+1)-L(x, y-1))^{2}} \\
\left.\theta(x, y)=\tan ^{-1}((L(x, y+1)-L(x, y-1)) / L(x+1, y)-L(x-1, y))\right)
\end{array}</script>L为关键点所在的尺度空间值，按Lowe的建议，梯度的模值m(x,y)按$1.5\sigma(o,s)$的尺度进行高斯加权，按照常用的$3\sigma$原则，取邻域窗口的半径为$3 \times 1.5 \sigma(o,s)$</li>
<li>求得关键点及其邻域内像素的梯度和方向，并使用直方图进行统计。梯度直方图将0~360度的方向范围分为36个柱(bins)，其中每柱10度。<img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330205657098-53794026.png" alt="如图"></li>
<li>方向直方图的峰值则代表了该特征点处邻域梯度的方向，以直方图中最大值作为该关键点的主方向。为了增强匹配的鲁棒性，只保留峰值大于主方向峰值80％的方向作为该关键点的辅方向。因此，<strong>对于同一梯度值的多个峰值的关键点位置，在相同位置和尺度将会有多个关键点被创建但方向不同</strong>。仅有15％的关键点被赋予多个方向，但可以明显的提高关键点匹配的稳定性。实际编程实现中，就是把该关键点复制成多份关键点，并将方向值分别赋给这些复制后的关键点，并且，离散的梯度方向直方图要进行插值拟合处理，来求得更精确的方向角度值。<ol>
<li>梯度直方图的平滑处理：为了避免梯度方向受噪声的影响，还可以对梯度直方图进行平滑以及进行抛物线插值处理。<a href="https://www.cnblogs.com/Alliswell-WP/p/SIFT.html">具体方法</a>。</li>
</ol>
</li>
</ol>
<h2 id="特征描述子计算"><a href="#特征描述子计算" class="headerlink" title="特征描述子计算"></a>特征描述子计算</h2><ul>
<li>作用：<strong>表示关键点邻域高斯图像梯度统计结果</strong>。</li>
<li>方法：通过对关键点周围图像区域分块，计算块内梯度直方图，生成具有独特性的向量，这个向量是该区域图像信息的一种抽象，具有唯一性&amp;独特性。<ul>
<li>Lowe建议描述子使用在关键点尺度空间内4*4的窗口中计算的8个方向的梯度信息，共4*4*8=128维向量表征。</li>
</ul>
</li>
</ul>
<h3 id="确定计算描述子所需的图像区域"><a href="#确定计算描述子所需的图像区域" class="headerlink" title="确定计算描述子所需的图像区域"></a>确定计算描述子所需的图像区域</h3><ol>
<li>将关键点附近的邻域划分为d*d(Lowe建议d=4)个子区域，每个子区域做为一个种子点，每个种子点有8个方向。</li>
<li>每个子区域的大小，也使用$3\sigma$原则确定，即子区域边长为$3\sigma$。</li>
<li>则所需图像区域边长为$3\sigma\times(d+1)$</li>
<li>考虑到旋转因素(方便下一步将坐标轴旋转到关键点的方向)，用圆代替矩阵，实际计算所需的图像区域半径为：<br>$radius=\frac{3\sigma\times\sqrt{2}\times(d+1)}{2}$。计算结果四舍五入取整。<img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330210025431-395170345.jpg" alt=""></li>
</ol>
<h3 id="坐标轴旋转至主方向"><a href="#坐标轴旋转至主方向" class="headerlink" title="坐标轴旋转至主方向"></a>坐标轴旋转至主方向</h3><ul>
<li>坐标轴旋转<br><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330210107953-2078955000.png" alt="坐标轴旋转至主方向"></li>
<li>旋转后邻域内采样点的新坐标：<script type="math/tex; mode=display">
\left(\begin{array}{l}
x^{\prime} \\
y^{\prime}
\end{array}\right)=\left(\begin{array}{ll}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right)\left(\begin{array}{l}
x \\
y
\end{array}\right)(x, y \in[-\text {radius, radius}]</script></li>
</ul>
<h3 id="梯度直方图生成"><a href="#梯度直方图生成" class="headerlink" title="梯度直方图生成"></a>梯度直方图生成</h3><ul>
<li>如下图绿色部分的坐标系中，旋转后采样点$(x^{\prime}, y^{\prime})$的新坐标为<script type="math/tex; mode=display">
\left(\begin{array}{l}
x^{n} \\
y^{n}
\end{array}\right)=\frac{1}{3 \sigma_{-} o c t}\left(\begin{array}{l}
x^{\prime} \\
y^{t}
\end{array}\right)+\frac{d}{2}</script></li>
<li>Lowe建议子区域的像素的梯度大小按$\sigma=0.5d$进行高斯加权，(a, b)为关键点在DoG图像中的位置坐标，则<script type="math/tex; mode=display">
w=m(a+x, b+y)^{*} e^{-\frac{\left(x^{\prime}\right)^{2}+\left(y^{\prime}\right)^{2}}{2 \times(0.5 d)^{2}}}</script><img src="https://niecongchong.github.io/img/2019-08-06-24.jpg" alt=""></li>
<li>与求主方向不同，此时每个种子区域的梯度直方图在0-360之间划分为8个方向区间，每个区间为45度，即每个种子点有8个方向的梯度强度信息。所以共4*4*8=128个梯度。</li>
</ul>
<h3 id="三线性插值"><a href="#三线性插值" class="headerlink" title="三线性插值"></a>三线性插值</h3><ul>
<li>三线性：x, y, 方向<br><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330210313666-1453481217.png" alt=""></li>
<li>如图中的红色点，落在第0行和第1行之间，对这两行都有贡献。对第0行第3列种子点的贡献因子为dr，对第1行第3列的贡献因子为1-dr，同理，对邻近两列的贡献因子为dc和1-dc，对邻近两个方向的贡献因子为do和1-do。k,m,n为0或1，则最终累加在每个方向上的梯度大小为：<script type="math/tex; mode=display">
\text { weight }=w^{*} d r^{k} *(1-d r)^{1-k} * d c^{m *}(1-d c)^{1-m *} d o^{n *}(1-d o)^{1-n}</script></li>
</ul>
<h3 id="特征描述子以及归一化特征描述子"><a href="#特征描述子以及归一化特征描述子" class="headerlink" title="特征描述子以及归一化特征描述子"></a>特征描述子以及归一化特征描述子</h3><ul>
<li>如上统计的4*4*8=128个梯度信息即为该关键点的特征向量。特征向量形成后，为了去除光照变化的影响，需要对它们进行归一化处理，对于图像灰度值整体漂移，图像各点的梯度是邻域像素相减得到，所以也能去除。</li>
<li>得到的描述子向量为$H=\left(h<em>{1}, h</em>{2}, \ldots, h_{128}\right)$</li>
<li>归一化后的描述子向量为$L=\left(l<em>{1}, l</em>{2}, \ldots, l_{128}\right)$</li>
</ul>
<h3 id="特征描述子门限化"><a href="#特征描述子门限化" class="headerlink" title="特征描述子门限化"></a>特征描述子门限化</h3><ul>
<li>非线性光照，相机饱和度变化对造成某些方向的梯度值过大，而对方向的影响微弱。因此设置门限值(向量归一化后，一般取0.2)截断较大的梯度值。然后，再进行一次归一化处理，提高特征的鉴别性。</li>
</ul>
<h3 id="特征描述向量排序"><a href="#特征描述向量排序" class="headerlink" title="特征描述向量排序"></a>特征描述向量排序</h3><ul>
<li>按特征点的尺度对特征描述向量进行排序。</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://www.cnblogs.com/Alliswell-WP/p/SIFT.html">https://www.cnblogs.com/Alliswell-WP/p/SIFT.html</a></li>
<li><a href="https://blog.csdn.net/zddblog/article/details/7521424">https://blog.csdn.net/zddblog/article/details/7521424</a></li>
<li><a href="https://blog.csdn.net/qq_41679006/article/details/80975436">https://blog.csdn.net/qq_41679006/article/details/80975436</a></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>CV</tag>
        <tag>SIFT</tag>
      </tags>
  </entry>
  <entry>
    <title>相似度</title>
    <url>/2022/01/02/%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    <content><![CDATA[<h1 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h1><ul>
<li>可以分为距离类、角度类、角度+距离类<a id="more"></a>
</li>
</ul>
<h1 id="距离类"><a href="#距离类" class="headerlink" title="距离类"></a>距离类</h1><h2 id="欧式距离（Euclidean-Distance）"><a href="#欧式距离（Euclidean-Distance）" class="headerlink" title="欧式距离（Euclidean Distance）"></a>欧式距离（Euclidean Distance）</h2><ul>
<li>欧式距离：欧式空间中两点间的距离公式。</li>
<li>例如平面空间内的欧式距离：<script type="math/tex; mode=display">
d=\sqrt{(x 1-x 2)^{2}+(y 1-y 2)^{2}}</script></li>
</ul>
<h2 id="曼哈顿距离-Manhattan-Distance"><a href="#曼哈顿距离-Manhattan-Distance" class="headerlink" title="曼哈顿距离(Manhattan Distance)"></a>曼哈顿距离(Manhattan Distance)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Manhattan</span>(<span class="params">dataA,dataB</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(dataA - dataB))</span><br><span class="line">print(Manhattan(dataA,dataB))</span><br></pre></td></tr></table></figure>
<h2 id="汉明距离（Hamming-distance）"><a href="#汉明距离（Hamming-distance）" class="headerlink" title="汉明距离（Hamming distance）"></a>汉明距离（Hamming distance）</h2><ul>
<li>汉明距离表示的是两个字符串（相同长度）对应位不同的数量。比如有两个等长的字符串 str1 = “11111” 和 str2 = “10001” 那么它们之间的汉明距离就是3（这样说就简单多了吧。哈哈）。</li>
<li><strong>汉明距离多用于图像像素的匹配（同图搜索）、通信领域统计错误数据位的数量等</strong>。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hammingDistance</span>(<span class="params">dataA,dataB</span>):</span></span><br><span class="line">    distanceArr = dataA - dataB</span><br><span class="line">    <span class="keyword">return</span> dataA.shape[<span class="number">0</span>] - np.<span class="built_in">sum</span>(distanceArr == <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="角度类"><a href="#角度类" class="headerlink" title="角度类"></a>角度类</h1><h2 id="余弦相似度（Cosine）"><a href="#余弦相似度（Cosine）" class="headerlink" title="余弦相似度（Cosine）"></a>余弦相似度（Cosine）</h2><ul>
<li>首先，样本数据的夹角余弦并不是真正几何意义上的夹角余弦，只不过是借了它的名字，实际是借用了它的概念变成了是代数意义上的“夹角余弦”，<strong>用来衡量样本向量间的差异</strong>。</li>
<li>二维情况下，即几何意义上的夹角余弦。夹角越小，余弦值越接近于1，反之则趋于-1。<script type="math/tex; mode=display">
\cos (\theta)=\frac{\sum_{k=1}^{n} x_{1 k} x_{2 k}}{\sqrt{\sum_{k=1}^{n} x_{1 k}^{2}} \sqrt{\sum_{k=1}^{n} x_{2 k}{ }^{2}}}</script></li>
<li>余弦相似度对夹角敏感，对数值的相对大小不敏感</li>
</ul>
<h2 id="皮尔逊相关系数（Pearson-Correlation-Coefficient）"><a href="#皮尔逊相关系数（Pearson-Correlation-Coefficient）" class="headerlink" title="皮尔逊相关系数（Pearson Correlation Coefficient）"></a>皮尔逊相关系数（Pearson Correlation Coefficient）</h2><script type="math/tex; mode=display">
\operatorname{sim}\left(x_{1}, x_{2}\right)=\frac{\sum_{k=1}^{n}\left(x_{1 k}-\overline{x_{1}}\right)\left(x_{2 k}-\overline{x_{2}}\right)}{\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-\overline{x_{1}}\right)^{2}} \sqrt{\sum_{k=1}^{n}\left(x_{2 k}-\overline{x_{2}}\right)^{2}}}</script><ul>
<li>在计算夹角余弦之前将两个向量减去各个样本的平均值，达到<strong>中心化</strong>的目的。<strong>皮尔逊相关函数是余弦相似度在维度缺失上面的一种改进方法</strong>。</li>
</ul>
<h1 id="角度-距离"><a href="#角度-距离" class="headerlink" title="角度+距离"></a>角度+距离</h1><h2 id="点积相似度（Dot）"><a href="#点积相似度（Dot）" class="headerlink" title="点积相似度（Dot）"></a>点积相似度（Dot）</h2><script type="math/tex; mode=display">
\vec{a} * \vec{b}=\overrightarrow{|a||b|} \mid \cos \theta</script><ul>
<li>常见于word2vec、deepwalk等算法中，用于作为loss（最小化向量ab的模以及两者的夹角），也可以理解为decoder（两向量的点积可以理解为原空间的两点相似度）</li>
<li>当向量模长固定时，点积相似度等同于余弦相似度</li>
</ul>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/33164335">from zhihu</a></li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-经验风险、期望风险、结构风险</title>
    <url>/2021/12/10/%E7%BB%8F%E9%AA%8C%20%E6%9C%9F%E6%9C%9B%20%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h3 id="经验风险"><a href="#经验风险" class="headerlink" title="经验风险"></a>经验风险</h3><ul>
<li>对训练样本误差的衡量，例如最典型的是每个样本损失函数的平均，Σ(loss(Yi, f(Xi))) / m</li>
<li>基于经验风险最小化的模型泛化能力差，只代表了局部最优 —&gt; 过拟合</li>
</ul>
<a id="more"></a>
<h3 id="期望风险"><a href="#期望风险" class="headerlink" title="期望风险"></a>期望风险</h3><ul>
<li>要寻找全局最优，可以考虑使用损失函数的期望，即E(loss(Yi, f(Xi))) = ∫(P(Xi,Yi)loss(Yi, f(Xi)))dxdy</li>
<li>此处假设了(x, y)的联合分布</li>
<li>问题：对于大部分样本无法获知其具体的分布 —&gt; 不现实</li>
</ul>
<h3 id="结构风险"><a href="#结构风险" class="headerlink" title="结构风险"></a>结构风险</h3><ul>
<li>在经验风险之上添加正则项，模拟期望风险</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-RF&amp;GBDT&amp;XGBoost&amp;LightGBM</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94XGBoost%20GBDT%20lightgbm/</url>
    <content><![CDATA[<p><meta name="referrer" content="no-referrer"/><br>[TOC]</p>
<h2 id="1-RF和GBDT的区别"><a href="#1-RF和GBDT的区别" class="headerlink" title="1. RF和GBDT的区别"></a>1. RF和GBDT的区别</h2><p><strong>相同点：</strong></p>
<ul>
<li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li>
</ul>
<p><strong>不同点：</strong></p>
<ul>
<li><strong>集成学习</strong>：$RF$属于$Bagging$思想，而$GBDT$是$Boosting$思想</li>
<li><strong>偏差-方差权衡</strong>：$RF$不断的降低模型的方差，而$GBDT$不断的降低模型的偏差</li>
<li><strong>训练样本</strong>：$RF$每次迭代的样本是从全部训练集中有放回抽样形成的，而$GBDT$每次使用全部样本</li>
<li><strong>并行性</strong>：$RF$的树可以并行生成，而$GBDT$只能顺序生成(需要等上一棵树完全生成)</li>
<li><strong>最终结果</strong>：$RF$最终是多棵树进行多数表决（回归问题是取平均），而$GBDT$是加权融合</li>
<li><strong>数据敏感性</strong>：$RF$对异常值不敏感，而$GBDT$对异常值比较敏感</li>
<li><strong>泛化能力</strong>：$RF$不易过拟合，而$GBDT$容易过拟合</li>
</ul>
<a id="more"></a>
<h2 id="2-比较LR和GBDT，说说什么情景下GBDT不如LR"><a href="#2-比较LR和GBDT，说说什么情景下GBDT不如LR" class="headerlink" title="2. 比较LR和GBDT，说说什么情景下GBDT不如LR"></a>2. 比较LR和GBDT，说说什么情景下GBDT不如LR</h2><p>先说说$LR$和$GBDT$的区别： </p>
<ul>
<li>$LR$是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程</li>
<li>$GBDT$是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；</li>
</ul>
<p>当在<strong>高维稀疏特征的场景下，$LR$的效果一般会比$GBDT$好</strong>。原因如下：</p>
<p>先看一个例子：</p>
<blockquote>
<p>假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。</p>
</blockquote>
<p>因为现在的模型普遍都会带着正则项，<strong>而 $LR$ 等线性模型的正则项是对权重的惩罚</strong>，也就是 $w_1$一旦过大，惩罚就会很大，进一步压缩 $w_1$的值，使他不至于过大。但是，树模型则不一样，<strong>树模型的惩罚项通常为叶子节点数和深度</strong>等，而我们都知道，对于上面这种<code>case</code>，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。</p>
<p>这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：<strong>带正则化的线性模型比较不容易对稀疏特征过拟合。</strong></p>
<h2 id="3-简单介绍一下XGBoost​"><a href="#3-简单介绍一下XGBoost​" class="headerlink" title="3. 简单介绍一下XGBoost​"></a>3. 简单介绍一下XGBoost​</h2><p>$XGBoost$是一种集成学习算法，属于3类常用的集成方法($Bagging$，$Boosting$，$Stacking$)中的$Boosting$算法类别。它是一个加法模型，基模型一般选择树模型，但也可以选择其它类型的模型如逻辑回归等。</p>
<p>$XGBoost$对$GBDT$进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行、默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。</p>
<h2 id="4-XGBoost与GBDT有什么不同"><a href="#4-XGBoost与GBDT有什么不同" class="headerlink" title="4. XGBoost与GBDT有什么不同"></a>4. XGBoost与GBDT有什么不同</h2><ul>
<li><strong>基分类器</strong>：$XGBoost$的基分类器不仅支持$CART$决策树，还支持线性分类器，此时$XGBoost$相当于带$L1$和$L2$正则化项的$LR$回归（分类问题）或者线性回归（回归问题）。</li>
<li><strong>导数信息</strong>：$XGBoost$对损失函数做了二阶泰勒展开，可以更为精准的逼近真实的损失函数，$GBDT$只用了一阶导数信息，并且$XGBoost$还支持自定义损失函数，只要损失函数一阶、二阶可导。</li>
<li><strong>正则项</strong>：$XGBoost$的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。</li>
<li><strong>列抽样</strong>：$XGBoost$支持列采样，与随机森林类似，用于防止过拟合。</li>
<li><strong>缺失值处理</strong>：对树中的每个非叶子结点，$XGBoost$可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。</li>
<li><strong>并行化</strong>：注意不是树维度的并行，而是特征维度的并行。$XGBoost$预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</li>
<li><strong>可扩展性</strong>：损失函数支持自定义，只需要新的损失函数二阶可导。</li>
</ul>
<h2 id="5-XGBoost​为什么可以并行训练"><a href="#5-XGBoost​为什么可以并行训练" class="headerlink" title="5. XGBoost​为什么可以并行训练"></a>5. XGBoost​为什么可以并行训练</h2><ul>
<li><strong>不是说每棵树可以并行训练</strong>，$XGBoost$本质上仍然采用$Boosting$思想，每棵树训练前需要等前面的树训练完成才能开始训练。</li>
<li><strong>而是特征维度的并行</strong>：1)训练之前，每个特征按特征值对样本进行预排序，并存储为<code>block</code>结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个<code>block</code>结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个<code>block</code>并行计算。2)也可以进行列的并行。</li>
</ul>
<h2 id="6-XGBoost​为什么快？"><a href="#6-XGBoost​为什么快？" class="headerlink" title="6. XGBoost​为什么快？"></a>6. XGBoost​为什么快？</h2><ul>
<li><strong>分块并行</strong>：训练前每个特征按特征值进行排序并存储为<code>block</code>结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点</li>
<li><strong><code>block</code> 处理优化</strong>：<code>block</code>预先放入内存；<code>block</code>按列进行解压缩；将<code>block</code>划分到不同硬盘来提高吞吐</li>
<li><strong>候选分位点</strong>：每个特征采用常数个分位点作为候选分割点</li>
<li><strong>CPU cache 命中优化</strong>： 使用缓存预取的方法，对每个线程分配一个连续的<code>buffer</code>，读取每个<code>block</code>中样本的梯度信息并存入连续的<code>buffer</code>中。</li>
</ul>
<h2 id="7-XGBoost​中如何处理过拟合的情况？"><a href="#7-XGBoost​中如何处理过拟合的情况？" class="headerlink" title="7. XGBoost​中如何处理过拟合的情况？"></a>7. XGBoost​中如何处理过拟合的情况？</h2><ul>
<li><strong>目标函数中增加了正则项</strong>：使用叶子结点的数目和叶子结点权重的$L2$模的平方，控制树的复杂度。</li>
<li><strong>设置目标函数的增益阈值</strong>：如果分裂后目标函数的增益小于该阈值，则不分裂。</li>
<li><strong>设置最小样本权重和的阈值</strong>：当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。</li>
<li><strong>设置树的最大深度</strong>：$XGBoost$ 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。</li>
</ul>
<ul>
<li>调参： <ul>
<li>第一类参数：用于直接控制模型的复杂度。包括<code>max_depth，min_child_weight，gamma</code> 等参数</li>
<li>第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括<code>subsample，colsample_by树</code></li>
<li>还有就是直接减小<code>learning rate</code>，但需要同时增加<code>estimator</code> 参数。</li>
</ul>
</li>
</ul>
<h2 id="8-XGBoost​如何处理缺失值？"><a href="#8-XGBoost​如何处理缺失值？" class="headerlink" title="8. XGBoost​如何处理缺失值？"></a>8. XGBoost​如何处理缺失值？</h2><p>$XGBoost$模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：</p>
<ul>
<li>在特征<code>k</code>上寻找最佳划分点时，不会对该列特征缺失的样本进行遍历，而只对该列特征值为无缺失的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找划分点的时间开销。 </li>
<li>在逻辑实现上，为了保证完备性，会<strong>将该特征值缺失的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后</strong>，<strong>选择分裂后增益最大</strong>的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。 </li>
<li>$XGBoost$<strong>在构建树的节点过程中只考虑非缺失值的数据遍历</strong>，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。 </li>
<li>如果在<strong>训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点</strong>。缺失值处理的伪代码如下：</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/8166116-d771bed713a887d8.png?imageMogr2/auto-orient/strip|imageView2/2/w/802/format/webp" alt=""></p>
<ul>
<li><p><strong>树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用</strong>。</p>
<p>原因就是：<strong>一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值）</strong>，完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。</p>
</li>
</ul>
<h2 id="9-XGBoost​如何处理不平衡数据？"><a href="#9-XGBoost​如何处理不平衡数据？" class="headerlink" title="9. XGBoost​如何处理不平衡数据？"></a>9. XGBoost​如何处理不平衡数据？</h2><ul>
<li><p>设置<code>scale_pos_weight</code>来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，<code>scale_pos_weight</code>可以取10；</p>
</li>
<li><p>你不能重新平衡数据集(会破坏数据的真实分布)的情况下，应该设置<code>max_delta_step</code>为一个有限数字来帮助收敛（基模型为$LR$时有效）。</p>
</li>
</ul>
<h2 id="10-XGBoost​如何选择最佳分裂点？"><a href="#10-XGBoost​如何选择最佳分裂点？" class="headerlink" title="10. XGBoost​如何选择最佳分裂点？"></a>10. XGBoost​如何选择最佳分裂点？</h2><ul>
<li>训练前预先将特征<strong>对特征值进行排序</strong>，存储为<code>block</code>结构，以便在结点分裂时可以重复使用</li>
<li>采用<strong>特征并行</strong>的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，<strong>最终选择增益最大的那个特征的特征值</strong>作为最佳分裂点。</li>
<li>$XGBoost$使用<strong>直方图近似算法</strong>，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。</li>
</ul>
<h2 id="11-XGBoost​的Scalable性如何体现？"><a href="#11-XGBoost​的Scalable性如何体现？" class="headerlink" title="11. XGBoost​的Scalable性如何体现？"></a>11. XGBoost​的Scalable性如何体现？</h2><ul>
<li><strong>基分类器的scalability</strong>：弱分类器可以支持$CART$决策树，也可以支持$LR$和Linear。</li>
<li><strong>目标函数的scalability</strong>：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。</li>
<li><strong>学习方法的scalability</strong>：<code>block</code>结构支持并行化，支持 Out-of-core计算。</li>
</ul>
<h2 id="12-XGBoost​如何评价特征的重要性？"><a href="#12-XGBoost​如何评价特征的重要性？" class="headerlink" title="12. XGBoost​如何评价特征的重要性？"></a>12. XGBoost​如何评价特征的重要性？</h2><p>常用的三种方法来评判模型中特征的重要程度：</p>
<ul>
<li><code>freq</code> ： 频率是表示特定特征在模型树中发生分裂的相对次数的百分比</li>
<li><code>gain</code> ： 增益意味着相应的特征对通过对模型中的每个树采取每个特征的贡献而计算出的模型的相对贡献。与其他特征相比，此度量值的较高值意味着它对于生成预测更为重要。 </li>
<li><code>cover</code> ：覆盖度量指的是与此功能相关的观测的相对数量。例如，如果您有100个观察值，4个特征和3棵树，并且假设特征1分别用于决定树1，树2和树3中10个，5个和2个观察值的叶节点;那么该度量将计算此功能的覆盖范围为$10 + 5 + 2 = 17$个观测值。这将针对所有4项功能进行计算，并将以17个百分比表示所有功能的覆盖指标。</li>
</ul>
<p><strong>$XGBoost$是根据<code>gain</code>来做重要性判断的。</strong> </p>
<h2 id="13-XGBooost​参数调优的一般步骤"><a href="#13-XGBooost​参数调优的一般步骤" class="headerlink" title="13. XGBooost​参数调优的一般步骤"></a>13. XGBooost​参数调优的一般步骤</h2><ul>
<li><p>确定<code>learning rate</code>和<code>estimator</code>的数量</p>
<p><code>learning rate</code>可以先用0.1，用cv来寻找最优的<code>estimators</code></p>
</li>
<li><p><code>max_depth</code>和 <code>min_child_weight</code></p>
<p>我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。</p>
<p><code>max_depth</code>: 每棵子树的最大深度，check from range(3，10，2)。</p>
<p><code>min_child_weight</code>: 子节点的权重阈值，check from range(1，6，2)。</p>
<p>如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。</p>
</li>
<li><p><code>gamma</code></p>
<p>也称作最小划分损失<code>min_split_loss</code>，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。</p>
<ul>
<li>如果大于该阈值，则该叶子节点值得继续划分</li>
<li>如果小于该阈值，则该叶子节点不值得继续划分</li>
</ul>
</li>
<li><p><code>subsample</code>、 <code>colsample_by tree</code></p>
<p><code>subsample</code>是对训练的采样比例</p>
<p><code>colsample_by tree</code>是对特征的采样比例both check from 0.6 to 0.9</p>
</li>
<li><p>正则化参数</p>
<p><code>alpha</code> 是$L1$正则化系数，try <code>1e-5， 1e-2， 0.1， 1， 100</code></p>
<p><code>lambda</code> 是$L2$正则化系数</p>
</li>
<li><p>降低学习率</p>
<p>降低学习率的同时增加树的数量，通常最后设置学习率为<code>0.01~0.1</code></p>
</li>
</ul>
<h2 id="14-XGBoost​的优缺点"><a href="#14-XGBoost​的优缺点" class="headerlink" title="14. XGBoost​的优缺点"></a>14. XGBoost​的优缺点</h2><ul>
<li><p>优点</p>
<ul>
<li><strong>精度更高：</strong> $GBDT$ 只用到一阶泰勒展开，而 $XGBoost$ 对损失函数进行了二阶泰勒展开。$XGBoost$ 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</li>
<li><strong>灵活性更强：</strong> $GBDT$ 以 $CART$ 作为基分类器，$XGBoost$ 不仅支持 $CART$ 还支持线性分类器，使用线性分类器的 $XGBoost$ 相当于带 和 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，$XGBoost$ 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li>
<li><strong>正则化：</strong> $XGBoost$ 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是$XGBoost$优于传统$GBDT$的一个特性。</li>
<li><strong>Shrinkage（缩减）：</strong> 相当于学习速率。$XGBoost$ 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。传统$GBDT$的实现也有学习速率；</li>
<li><strong>列抽样：</strong> $XGBoost$ 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。这也是$XGBoost$异于传统$GBDT$的一个特性；</li>
<li><strong>缺失值处理：</strong> 对于特征的值有缺失的样本，$XGBoost$ 采用的稀疏感知算法可以自动学习出它的分裂方向；</li>
<li><strong>$XGBoost$工具支持并行：</strong> $Boosting$不是一种串行的结构吗?怎么并行的？注意$XGBoost$的并行不是树粒度的并行，$XGBoost$也是一次迭代完才能进行下一次迭代的（第次迭代的代价函数里包含了前面次迭代的预测值）。$XGBoost$的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），$XGBoost$在训练之前，预先对数据进行了排序，然后保存为<code>block</code>结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个<code>block</code>结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>
<li><strong>可并行的近似算法：</strong> 树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以$XGBoost$还提出了一种可并行的近似算法，用于高效地生成候选的分割点。</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但<strong>在节点分裂过程中仍需要遍历数据集</strong>；</li>
<li><strong>预排序过程的空间复杂度过高</strong>，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于<strong>消耗了两倍的内存</strong>。</li>
</ul>
</li>
</ul>
<h2 id="15-XGBoost​和LightGBM​的区别"><a href="#15-XGBoost​和LightGBM​的区别" class="headerlink" title="15. XGBoost​和LightGBM​的区别"></a>15. XGBoost​和LightGBM​的区别</h2><p><img src="https://ask.qcloudimg.com/http-save/yehe-1622140/btc3oj2txs.jpeg?imageView2/2/w/1620" alt="img"></p>
<p>（1）树生长策略：XGB采用<code>level-wise</code>的分裂策略，LGB采用<code>leaf-wise</code>的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。 </p>
<p>（2）分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下：</p>
<ul>
<li>减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。</li>
<li>计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可。</li>
<li>LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算</li>
</ul>
<blockquote>
<p>但实际上$XGBoost$的近似直方图算法也类似于$LightGBM$这里的直方图算法，为什么$XGBoost$的近似算法比$LightGBM$还是慢很多呢？ $XGBoost$在每一层都动态构建直方图， 因为$XGBoost$的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而$LightGBM$中对每个特征都有一个直方图，所以构建一次直方图就够了。</p>
</blockquote>
<p>（3）支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而$LightGBM$可以直接处理类别型变量。</p>
<p>（4）缓存命中率：$XGBoost$使用<code>block</code>结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</p>
<p>（5）$LightGBM$ 与 $XGBoost$ 的并行策略不同：</p>
<ul>
<li><strong>特征并行</strong> ：LGB特征并行的前提是每个<code>worker</code>留有一份完整的数据集，但是每个<code>worker</code>仅在特征子集上进行最佳切分点的寻找；<code>worker</code>之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个<code>worker</code>进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个<code>worker</code>节点中仅有部分的列数据，也就是垂直切分，每个<code>worker</code>寻找局部最佳切分点，<code>worker</code>之间相互通信，然后在具有最佳切分点的<code>worker</code>上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他<code>worker</code>才能开始分裂。二者的区别就导致了LGB中<code>worker</code>间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。</li>
<li><strong>数据并行</strong> ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个<code>worker</code>上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得<code>worker</code>间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个<code>worker</code>建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个<code>worker</code>上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个<code>worker</code>间的通信量也就变得很大。</li>
<li><strong>投票并行（LGB）</strong>：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个<code>worker</code>首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。</li>
</ul>
]]></content>
      <categories>
        <category>Learning</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
        <tag>LightGBM</tag>
        <tag>RandomForset</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode-查找表</title>
    <url>/2021/12/10/LeetCode%20%E6%9F%A5%E6%89%BE%E8%A1%A8/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="一-考虑的基本数据结构"><a href="#一-考虑的基本数据结构" class="headerlink" title="一.考虑的基本数据结构"></a>一.考虑的基本数据结构</h1><p><strong>第一类： 查找有无—set</strong></p>
<p>元素’a’是否存在，通常用set：集合</p>
<p>set只存储键，而不需要对应其相应的值。 </p>
<p>set中的键不允许重复</p>
<a id="more"></a>
<p><strong>第二类： 查找对应关系(键值对应)—dict</strong></p>
<p>元素’a’出现了几次：dict—&gt;字典</p>
<p>dict中的键不允许重复</p>
<p><strong>第三类： 改变映射关系—map</strong></p>
<p>通过将原有序列的关系映射统一表示为其他</p>
<h1 id="LeetCode-349-Intersection-Of-Two-Arrays-1"><a href="#LeetCode-349-Intersection-Of-Two-Arrays-1" class="headerlink" title="LeetCode 349 Intersection Of Two Arrays 1"></a>LeetCode 349 Intersection Of Two Arrays 1</h1><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定两个数组nums,求两个数组的公共元素。</p>
<p>—如nums1 = [1,2,2,1],nums2 = [2,2]</p>
<p>—结果为[2]</p>
<p>—结果中每个元素只能出现一次</p>
<p>—出现的顺序可以是任意的</p>
<h2 id="分析实现"><a href="#分析实现" class="headerlink" title="分析实现"></a>分析实现</h2><p>由于每个元素只出现一次，因此不需要关注每个元素出现的次数，用set的数据结构就可以了。<br>记录元素的有和无。</p>
<p>把nums1记录为set，判断nums2的元素是否在set中，是的话，就放在一个公共的set中，最后公共的set就是我们要的结果。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersection</span>(<span class="params">self, nums1: List[<span class="built_in">int</span>], nums2: List[<span class="built_in">int</span>]</span>) -&gt; List[int]:</span></span><br><span class="line">        nums1 = <span class="built_in">set</span>(nums1)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">set</span>([i <span class="keyword">for</span> i <span class="keyword">in</span> nums2 <span class="keyword">if</span> i <span class="keyword">in</span> nums1])</span><br></pre></td></tr></table></figure>
<p>也可以通过set的内置方法来实现，直接求set的交集：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersection</span>(<span class="params">self, nums1: List[<span class="built_in">int</span>], nums2: List[<span class="built_in">int</span>]</span>) -&gt; List[int]:</span></span><br><span class="line">        set1 = <span class="built_in">set</span>(nums1)</span><br><span class="line">        set2 = <span class="built_in">set</span>(nums2)</span><br><span class="line">        <span class="keyword">return</span> set2 &amp; set1</span><br></pre></td></tr></table></figure></p>
<h1 id="LeetCode-350-Intersection-Of-Two-Arrays-2"><a href="#LeetCode-350-Intersection-Of-Two-Arrays-2" class="headerlink" title="LeetCode 350 Intersection Of Two Arrays 2"></a>LeetCode 350 Intersection Of Two Arrays 2</h1><h2 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定两个数组nums,求两个数组的交集。</p>
<p>— 如nums1=[1,2,2,1],nums=[2,2]</p>
<p>— 结果为[2,2]</p>
<p>— 出现的顺序可以是任意的</p>
<h2 id="分析实现-1"><a href="#分析实现-1" class="headerlink" title="分析实现"></a>分析实现</h2><p>元素出现的次数有用，那么对于存储次数就是有意义的，所以选择数据结构时，就应该选择dict的结构，通过字典的比较来判断；</p>
<p>记录每个元素的同时要记录这个元素的频次。</p>
<p>记录num1的字典，遍历nums2，比较nums1的字典的nums的key是否大于零，从而进行判断。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersect</span>(<span class="params">self, nums1: List[<span class="built_in">int</span>], nums2: List[<span class="built_in">int</span>]</span>) -&gt; List[int]:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        nums1_dict = Counter(nums1)</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums2:</span><br><span class="line">            <span class="keyword">if</span> nums1_dict[num] &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 说明找到了一个元素即在num1也在nums2</span></span><br><span class="line">                res.append(num)</span><br><span class="line">                nums1_dict[num] -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res        </span><br></pre></td></tr></table></figure>
<h1 id="LeetCode-242-Intersection-Of-Two-Arrays-2"><a href="#LeetCode-242-Intersection-Of-Two-Arrays-2" class="headerlink" title="LeetCode 242 Intersection Of Two Arrays 2"></a>LeetCode 242 Intersection Of Two Arrays 2</h1><h2 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。</p>
<p>示例1:</p>
<p>输入: s = “anagram”, t = “nagaram”<br>输出: true</p>
<p>示例 2:</p>
<p>输入: s = “rat”, t = “car”<br>输出: false</p>
<h2 id="分析实现-2"><a href="#分析实现-2" class="headerlink" title="分析实现"></a>分析实现</h2><p>判断异位词即判断变换位置后的字符串和原来是否相同，那么不仅需要存储元素，还需要记录元素的个数。可以选择dict的数据结构，将字符串s和t都用dict存储，而后直接比较两个dict是否相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isAnagram</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; bool:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        s = Counter(s)</span><br><span class="line">        t = Counter(t)</span><br><span class="line">        <span class="keyword">if</span> s == t:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h1 id="LeetCode-202-Happy-number"><a href="#LeetCode-202-Happy-number" class="headerlink" title="LeetCode 202 Happy number"></a>LeetCode 202 Happy number</h1><h2 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h2><p>编写一个算法来判断一个数是不是“快乐数”。</p>
<p>一个“快乐数”定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是无限循环但始终变不到 1。如果可以变为 1，那么这个数就是快乐数。</p>
<p>示例: </p>
<p>输入: 19</p>
<p>输出: true</p>
<p>解释:<br>1^2 + 9^2 = 82<br>8^2 + 2^2 = 68<br>6^2 + 8^2 = 100<br>1^2 + 0^2 + 0^2 = 1</p>
<h2 id="分析实现-3"><a href="#分析实现-3" class="headerlink" title="分析实现"></a>分析实现</h2><p>这道题目思路很明显，当n不等于1时就循环，每次循环时，将其最后一位到第一位的数依次平方求和，比较求和是否为1。</p>
<p>难点在于，什么时候跳出循环？</p>
<p>开始笔者的思路是，循环个100次，还没得出结果就false，但是小学在算无限循环小数时有一个特征，就是当除的数中，和之前历史的得到的数有重合时，这时就是无限循环小数。</p>
<p>那么这里也可以按此判断，因为只需要判断有或无，不需要记录次数，故用set的数据结构。每次对求和的数进行append，当新一次求和的值存在于set中时，就return false.</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isHappy</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        already = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">while</span> n != <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 取n的最后一位数</span></span><br><span class="line">                tmp = n % <span class="number">10</span>   </span><br><span class="line">                <span class="built_in">sum</span> += tmp ** <span class="number">2</span></span><br><span class="line">                <span class="comment"># 将n的最后一位截掉</span></span><br><span class="line">                n //= <span class="number">10</span></span><br><span class="line">            <span class="comment"># 如果求的和在过程中出现过</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span> <span class="keyword">in</span> already:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                already.add(<span class="built_in">sum</span>)</span><br><span class="line">            n = <span class="built_in">sum</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h2 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一般对多位数计算的套路是：</span></span><br><span class="line"><span class="comment">#循环从后向前取位数</span></span><br><span class="line"><span class="keyword">while</span> n &gt;<span class="number">0</span> :</span><br><span class="line"><span class="comment">#取最后一位： </span></span><br><span class="line">tmp = n % <span class="number">10</span></span><br><span class="line"><span class="comment">#再截掉最后一位：</span></span><br><span class="line">n = n // <span class="number">10</span></span><br></pre></td></tr></table></figure>
<h1 id="LeetCode-290-Word-Pattern"><a href="#LeetCode-290-Word-Pattern" class="headerlink" title="LeetCode 290 Word Pattern"></a>LeetCode 290 Word Pattern</h1><h2 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出一个模式(pattern)以及一个字符串，判断这个字符串是否符合模式</p>
<p>示例1:</p>
<p>输入: pattern = “abba”,<br>str = “dog cat cat dog”</p>
<p>输出: true</p>
<p>示例 2:</p>
<p>输入:pattern = “abba”,<br>str = “dog cat cat fish”</p>
<p>输出: false</p>
<p>示例 3:</p>
<p>输入: pattern = “aaaa”, str = “dog cat cat dog”</p>
<p>输出: false</p>
<p>示例 4:</p>
<p>输入: pattern = “abba”, str = “dog dog dog dog”</p>
<p>输出: false</p>
<h2 id="分析实现-4"><a href="#分析实现-4" class="headerlink" title="分析实现"></a>分析实现</h2><p>抓住变与不变，笔者开始的思路是选择了dict的数据结构，比较count值和dict对应的keys的个数是否相同，但是这样无法判断顺序的关系，如测试用例：’aba’,’cat cat dog’。</p>
<p>那么如何能<strong>既考虑顺序</strong>，也考虑<strong>键值对应的关系</strong>呢？</p>
<p>抓住变与不变，变的是键，但是不变的是各个字典中，对应的相同index下的值，如dict1[index] = dict2[index]，那么我们可以创建两个新的字典，遍历index对两个新的字典赋值，并比较value。</p>
<p>还有一个思路比较巧妙，既然不同，那么可以考虑怎么让它们相同，将原来的dict通过map映射为相同的key，再比较相同key的dict是否相同。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordPattern</span>(<span class="params">self,pattern, <span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="built_in">str</span> = <span class="built_in">str</span>.split()</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(pattern.index,pattern)) == <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>.index,<span class="built_in">str</span>))</span><br></pre></td></tr></table></figure>
<h2 id="tips-1"><a href="#tips-1" class="headerlink" title="tips"></a>tips</h2><ol>
<li><p>因为str是字符串，不是由单个字符组成，所以开始需要根据空格拆成字符list：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">str</span> = <span class="built_in">str</span>.split()</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过map将字典映射为index的list:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">map</span>(pattern.index, pattern)</span><br></pre></td></tr></table></figure></li>
<li>map是通过hash存储的，不能直接进行比较，需要转换为list比较list</li>
</ol>
<h1 id="LeetCode-205-Isomorphic-Strings"><a href="#LeetCode-205-Isomorphic-Strings" class="headerlink" title="LeetCode 205 Isomorphic Strings"></a>LeetCode 205 Isomorphic Strings</h1><h2 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定两个字符串 s 和 t，判断它们是否是同构的。</p>
<p>如果 s 中的字符可以被替换得到 t ，那么这两个字符串是同构的。</p>
<p>所有出现的字符都必须用另一个字符替换，同时保留字符的顺序。两个字符不能映射到同一个字符上，但字符可以映射自己本身。</p>
<p>示例 1:</p>
<p>输入: s = “egg”, t = “add”</p>
<p>输出: true</p>
<p>示例 2:</p>
<p>输入: s = “foo”, t = “bar”</p>
<p>输出: false</p>
<p>示例 3:</p>
<p>输入: s = “paper”, t = “title”</p>
<p>输出: true</p>
<h2 id="分析实现-5"><a href="#分析实现-5" class="headerlink" title="分析实现"></a>分析实现</h2><p>思路与上题一致，可以考虑通过建两个dict，比较怎样不同，也可以将不同转化为相同。</p>
<p>直接用上题的套路代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isIsomorphic</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; bool:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(s.index,s)) == <span class="built_in">list</span>(<span class="built_in">map</span>(t.index,t))</span><br></pre></td></tr></table></figure></p>
<h1 id="LeetCode-451-Sort-Characters-By-Frequency"><a href="#LeetCode-451-Sort-Characters-By-Frequency" class="headerlink" title="LeetCode 451 Sort Characters By Frequency"></a>LeetCode 451 Sort Characters By Frequency</h1><h2 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个字符串，请将字符串里的字符按照出现的频率降序排列。</p>
<p>示例 1:</p>
<p>输入:<br>“tree”</p>
<p>输出:<br>“eert”</p>
<p>示例 2:</p>
<p>输入:<br>“cccaaa”</p>
<p>输出:<br>“cccaaa”</p>
<p>示例 3:</p>
<p>输入:<br>“Aabb”</p>
<p>输出:<br>“bbAa”</p>
<h2 id="分析实现-6"><a href="#分析实现-6" class="headerlink" title="分析实现"></a>分析实现</h2><p>对于相同频次的字母，顺序任意，需要考虑大小写，返回的是字符串。</p>
<p>使用字典统计频率，对字典的value进行排序，最终根据key的字符串乘上value次数，组合在一起输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">frequencySort</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; str:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        s_dict = Counter(s)</span><br><span class="line">        <span class="comment"># sorted返回的是列表元组</span></span><br><span class="line">        s = <span class="built_in">sorted</span>(s_dict.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse = <span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 因为返回的是字符串</span></span><br><span class="line">        res = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> s:</span><br><span class="line">            res += key * value   </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="tips-2"><a href="#tips-2" class="headerlink" title="tips"></a>tips</h2><ol>
<li>通过sorted的方法进行value排序，对字典排序后无法直接按照字典进行返回，返回的为列表元组：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对value值由大到小排序</span></span><br><span class="line">s = <span class="built_in">sorted</span>(s_dict.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对key由小到大排序</span></span><br><span class="line">s = <span class="built_in">sorted</span>(s_dict.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></li>
<li>输出为字符串的情况下，可以由字符串直接进行拼接:<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 由key和value相乘进行拼接</span></span><br><span class="line"><span class="string">&#x27;s&#x27;</span> * <span class="number">5</span> + <span class="string">&#x27;d&#x27;</span>*<span class="number">2</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="二-对撞指针"><a href="#二-对撞指针" class="headerlink" title="二. 对撞指针"></a>二. 对撞指针</h1><h1 id="LeetCode-1-Two-Sum"><a href="#LeetCode-1-Two-Sum" class="headerlink" title="LeetCode 1 Two Sum"></a>LeetCode 1 Two Sum</h1><h2 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出一个整型数组nums，返回这个数组中两个数字的索引值i和j，使得nums[i] + nums[j]等于一个给定的target值，两个索引不能相等。</p>
<p>如：nums= [2,7,11,15],target=9<br>返回[0,1]</p>
<h2 id="审题"><a href="#审题" class="headerlink" title="审题:"></a>审题:</h2><p>需要考虑：</p>
<ol>
<li>开始数组是否有序；</li>
<li>索引从0开始计算还是1开始计算？</li>
<li>没有解该怎么办？</li>
<li>有多个解怎么办？保证有唯一解。</li>
</ol>
<h2 id="分析实现-7"><a href="#分析实现-7" class="headerlink" title="分析实现"></a>分析实现</h2><h2 id="暴力法O-n-2"><a href="#暴力法O-n-2" class="headerlink" title="暴力法O(n^2)"></a>暴力法O(n^2)</h2><p>时间复杂度为O(n^2),第一遍遍历数组，第二遍遍历当前遍历值之后的元素，其和等于target则return。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        len_nums = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_nums):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,len_nums):</span><br><span class="line">                <span class="keyword">if</span> nums[i] + nums[j] == target:</span><br><span class="line">                    <span class="keyword">return</span> [i,j]</span><br></pre></td></tr></table></figure>
<h2 id="排序-指针对撞-O-n-O-nlogn-O-n"><a href="#排序-指针对撞-O-n-O-nlogn-O-n" class="headerlink" title="排序+指针对撞(O(n)+O(nlogn)=O(n))"></a>排序+指针对撞(O(n)+O(nlogn)=O(n))</h2><p>在数组篇的LeetCode 167题中，也遇到了找到两个数使得它们相加之和等于目标数，但那是对于排序的情况，因此也可以使用上述的思路来完成。</p>
<p>因为问题本身不是有序的，因此需要对原来的数组进行一次排序，排序后就可以用O(n)的指针对撞进行解决。</p>
<p>但是问题是，返回的是数字的索引，如果只是对数组的值进行排序，那么数组原来表示的索引的信息就会丢失，所以在排序前要进行些处理。</p>
<p><strong>错误代码示例—只使用dict来进行保存：</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        record = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            record[nums[index]] = index </span><br><span class="line">        nums.sort()</span><br><span class="line">        l,r = <span class="number">0</span>,<span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            <span class="keyword">if</span> nums[l] + nums[r] == target:</span><br><span class="line">                <span class="keyword">return</span> [record[nums[l]],record[nums[r]]]</span><br><span class="line">            <span class="keyword">elif</span> nums[l] + nums[r] &lt; target:</span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                r -= <span class="number">1</span></span><br></pre></td></tr></table></figure><br>当遇到<strong>相同的元素的索引</strong>问题时，会不满足条件：</p>
<p>如：[3,3]  6</p>
<p>在排序前先使用一个额外的数组<strong>拷贝</strong>一份原来的数组，对于两个相同元素的索引问题，使用一个<strong>bool型变量</strong>辅助将两个索引都找到，总的时间复杂度为O(n)+O(nlogn) = O(nlogn)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        record = <span class="built_in">dict</span>()</span><br><span class="line">        nums_copy = nums.copy()</span><br><span class="line">        sameFlag = <span class="literal">True</span>;</span><br><span class="line">        nums.sort()</span><br><span class="line">        l,r = <span class="number">0</span>,<span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            <span class="keyword">if</span> nums[l] + nums[r] == target:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">elif</span> nums[l] + nums[r] &lt; target:</span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                r -= <span class="number">1</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums_copy[i] == nums[l] <span class="keyword">and</span> sameFlag:</span><br><span class="line">                res.append(i)</span><br><span class="line">                sameFlag = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">elif</span> nums_copy[i] == nums[r]:</span><br><span class="line">                res.append(i)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="小套路"><a href="#小套路" class="headerlink" title="小套路:"></a>小套路:</h3><p>如果只是对数组的值进行排序，那么数组原来表示的索引的信息就会丢失的情况，可以在排序前：</p>
<h3 id="更加pythonic的实现"><a href="#更加pythonic的实现" class="headerlink" title="更加pythonic的实现"></a>更加pythonic的实现</h3><p>通过list(enumerate(nums))开始实现下标和值的绑定，不用专门的再copy加bool判断。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = <span class="built_in">list</span>(<span class="built_in">enumerate</span>(nums))</span><br><span class="line">nums.sort(key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line">i,j = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> i &lt; j:</span><br><span class="line">    <span class="keyword">if</span> nums[i][<span class="number">1</span>] + nums[j][<span class="number">1</span>] &gt; target:</span><br><span class="line">        j -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> nums[i][<span class="number">1</span>] + nums[j][<span class="number">1</span>] &lt; target:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> nums[j][<span class="number">0</span>] &lt; nums[i][<span class="number">0</span>]:</span><br><span class="line">            nums[j],nums[i] = nums[i],nums[j]</span><br><span class="line">        <span class="keyword">return</span> num[i][<span class="number">0</span>],nums[j][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><strong>拷贝数组 + bool型变量辅助</strong></p>
<h2 id="查找表—O-n"><a href="#查找表—O-n" class="headerlink" title="查找表—O(n)"></a>查找表—O(n)</h2><p>遍历数组过程中，当遍历到元素v时，可以只看v前面的元素，是否含有target-v的元素存在。</p>
<ol>
<li>如果查找成功，就返回解；</li>
<li>如果没有查找成功，就把v放在查找表中，继续查找下一个解。</li>
</ol>
<p>即使v放在了之前的查找表中覆盖了v，也不影响当前v元素的查找。因为只需要找到两个元素，只需要找target-v的另一个元素即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        record = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            complement = target - nums[i]</span><br><span class="line">            <span class="comment"># 已经在之前的字典中找到这个值</span></span><br><span class="line">            <span class="keyword">if</span> record.get(complement) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                res = [i,record[complement]]</span><br><span class="line">                <span class="keyword">return</span> res</span><br><span class="line">            record[nums[i]] = i</span><br></pre></td></tr></table></figure>
<p>只进行一次循环，故时间复杂度O(n),空间复杂度为O(n)</p>
<h2 id="补充思路："><a href="#补充思路：" class="headerlink" title="补充思路："></a>补充思路：</h2><p>通过enumerate来把索引和值进行绑定，进而对value进行sort，前后对撞指针进行返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        nums = <span class="built_in">list</span>(<span class="built_in">enumerate</span>(nums))</span><br><span class="line">        <span class="comment"># 根据value来排序</span></span><br><span class="line">        nums.sort(key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line">        l,r = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            <span class="keyword">if</span> nums[l][<span class="number">1</span>] + nums[r][<span class="number">1</span>] == target:</span><br><span class="line">                <span class="keyword">return</span> nums[l][<span class="number">0</span>],nums[r][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> nums[l][<span class="number">1</span>] + nums[r][<span class="number">1</span>] &lt; target:</span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                r -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h1 id="LeetCode-15-3Sum"><a href="#LeetCode-15-3Sum" class="headerlink" title="LeetCode 15 3Sum"></a>LeetCode 15 3Sum</h1><h2 id="题目描述-8"><a href="#题目描述-8" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出一个整型数组，寻找其中的所有不同的三元组(a,b,c)，使得a+b+c=0</p>
<p>注意：答案中不可以包含重复的三元组。</p>
<p>如：nums = [-1, 0, 1, 2, -1, -4]，</p>
<p>结果为：[[-1, 0, 1],[-1, -1, 2]]</p>
<h2 id="审题-1"><a href="#审题-1" class="headerlink" title="审题"></a>审题</h2><ol>
<li>数组不是有序的；</li>
<li>返回结果为全部解，多个解的顺序是否需要考虑？—不需要考虑顺序</li>
<li>什么叫不同的三元组？索引不同即不同，还是值不同？—题目定义的是，值不同才为不同的三元组</li>
<li>没有解时怎么返回？—空列表</li>
</ol>
<h2 id="分析实现-8"><a href="#分析实现-8" class="headerlink" title="分析实现"></a>分析实现</h2><p>因为上篇中已经实现了Two Sum的问题，因此对于3Sum，首先想到的思路就是，开始固定一个k，然后在其后都当成two sum问题来进行解决，但是这样就ok了吗？</p>
<h3 id="没有考虑重复元素导致错误"><a href="#没有考虑重复元素导致错误" class="headerlink" title="没有考虑重复元素导致错误"></a>没有考虑重复元素导致错误</h3><p>直接使用Two Sum问题中的查找表的解法，根据第一层遍历的i，将i之后的数组作为two sum问题进行解决。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSum</span>(<span class="params">self, nums: [<span class="built_in">int</span>]</span>) -&gt; [[int]]:</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            num = <span class="number">0</span> - nums[i]</span><br><span class="line">            record = <span class="built_in">dict</span>()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">                complement = num - nums[j]</span><br><span class="line">                <span class="comment"># 已经在之前的字典中找到这个值</span></span><br><span class="line">                <span class="keyword">if</span> record.get(complement) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    res_lis = [nums[i], nums[j], complement]</span><br><span class="line">                    res.append(res_lis)</span><br><span class="line">                record[nums[j]] = i</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>但是这样会导致一个错误，错误用例如下:</p>
<p>输入：<br>[-1,0,1,2,-1,-4]</p>
<p>输出：<br>[[-1,1,0],[-1,-1,2],[0,-1,1]]</p>
<p>预期结果：<br>[[-1,-1,2],[-1,0,1]]</p>
<p>代码在实现的过程中没有把第一次遍历的i的索引指向相同元素的情况排除掉，于是出现了当i指针后面位置的元素有和之前访问过的相同的值，于是重复遍历。</p>
<p>那么可以考虑，开始时对nums数组进行排序，排序后，当第一次遍历的指针k遇到下一个和前一个指向的值重复时，就将其跳过。为了方便计算，在第二层循环中，可以使用<strong>对撞指针</strong>的套路：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对撞指针套路</span></span><br><span class="line">l,r = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> l &lt; r:</span><br><span class="line">    <span class="keyword">if</span> nums[l] + nums[r] == target:</span><br><span class="line">        <span class="keyword">return</span> nums[l],nums[r]</span><br><span class="line">    <span class="keyword">elif</span> nums[l] + nums[r] &lt; target:</span><br><span class="line">        l += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        r -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>其中需要注意的是，在里层循环中，也要考虑重复值的情况，因此当值相等时，再次移动指针时，需要保证其指向的值和前一次指向的值不重复，因此可以：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对撞指针套路</span></span><br><span class="line">l,r = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> l &lt; r:</span><br><span class="line">    <span class="built_in">sum</span> = nums[i] + nums[l] + nums[r]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">sum</span> == target:</span><br><span class="line">        res.append([nums[i],nums[l],nums[r])</span><br><span class="line">        l += <span class="number">1</span></span><br><span class="line">        r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[l<span class="number">-1</span>]: l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[r] == nums[r+<span class="number">1</span>]: r -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; target:</span><br><span class="line">        l += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        r -= <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>再调整下遍历的范围，因为设了3个索引：i，l，r。边界情况下，r索引指向len-1, l指向len-2，索引i遍历的边界为len-3，故for循环是从0到len-2。</p>
<p>代码实现如下：</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSum</span>(<span class="params">self, nums: [<span class="built_in">int</span>]</span>) -&gt; [[int]]:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)<span class="number">-2</span>):</span><br><span class="line">            <span class="comment"># 因为是排序好的数组，如果最小的都大于0可以直接排除</span></span><br><span class="line">            <span class="keyword">if</span> nums[i] &gt; <span class="number">0</span>: <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 排除i的重复值</span></span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line">            l,r = i+<span class="number">1</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">            <span class="keyword">while</span> l &lt; r:</span><br><span class="line">                <span class="built_in">sum</span> = nums[i] + nums[l] + nums[r]</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">sum</span> == <span class="number">0</span>:</span><br><span class="line">                    res.append([nums[i],nums[l],nums[r]])</span><br><span class="line">                    l += <span class="number">1</span></span><br><span class="line">                    r -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[l<span class="number">-1</span>]: l += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[r] == nums[r+<span class="number">1</span>]: r -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; <span class="number">0</span>:</span><br><span class="line">                    l += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="小套路-1"><a href="#小套路-1" class="headerlink" title="小套路"></a>小套路</h2><ol>
<li>采用<strong>for + while</strong>的形式来处理三索引；</li>
<li>当数组不是有序时需要注意，有序的特点在哪里，有序就可以用哪些方法解决？无序的话不便在哪里？</li>
<li>对撞指针套路：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对撞指针套路</span></span><br><span class="line">l,r = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> l &lt; r:</span><br><span class="line">    <span class="keyword">if</span> nums[l] + nums[r] == target:</span><br><span class="line">        <span class="keyword">return</span> nums[l],nums[r]</span><br><span class="line">    <span class="keyword">elif</span> nums[l] + nums[r] &lt; target:</span><br><span class="line">        l += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        r -= <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
<li>处理重复值的套路：先转换为有序数组，再循环判断其与上一次值是否重复：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line"><span class="comment"># 2.</span></span><br><span class="line"><span class="keyword">while</span> l &lt; r:</span><br><span class="line">    <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[l<span class="number">-1</span>]: l += <span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="LeetCode-18-4Sum"><a href="#LeetCode-18-4Sum" class="headerlink" title="LeetCode 18 4Sum"></a>LeetCode 18 4Sum</h1><h2 id="题目描述-9"><a href="#题目描述-9" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出一个整形数组，寻找其中的所有不同的四元组(a,b,c,d)，使得a+b+c+d等于一个给定的数字target。</p>
<p>—如:</p>
<p>nums = [1, 0, -1, 0, -2, 2]，target = 0</p>
<p>—结果为：</p>
<p>[[-1,  0, 0, 1],[-2, -1, 1, 2],[-2,  0, 0, 2]]</p>
<h2 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h2><p>4Sum可以当作是3Sum问题的扩展，注意事项仍是一样的，同样是不能返回重复值得解。首先排序。接着从[0,len-1]遍历i，跳过i的重复元素，再在[i+1,len-1]中遍历j，得到i，j后，再选择首尾的l和r，通过对撞指针的思路，四数和大的话r—，小的话l++,相等的话纳入结果list，最后返回。</p>
<p>套用3Sum得代码，在其前加一层循环，对边界情况进行改动即可:</p>
<ol>
<li>原来3个是到len-2,现在外层循环是到len-3;</li>
<li>在中间层得迭代中，当第二个遍历得值在第一个遍历得值之后且后项大于前项时，认定为重复；</li>
<li>加些边界条件判断：当len小于4时，直接返回；当只有4个值且长度等于target时，直接返回本身即可。</li>
</ol>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[List[int]]:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) &lt; <span class="number">4</span>: <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) == <span class="number">4</span> <span class="keyword">and</span> <span class="built_in">sum</span>(nums) == target:</span><br><span class="line">            res.append(nums)</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)<span class="number">-3</span>):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(nums)<span class="number">-2</span>):</span><br><span class="line">                <span class="keyword">if</span> j &gt; i+<span class="number">1</span> <span class="keyword">and</span> nums[j] == nums[j<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line">                l,r = j+<span class="number">1</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">                <span class="keyword">while</span> l &lt; r:</span><br><span class="line">                    sum_value = nums[i] + nums[j] + nums[l] + nums[r]</span><br><span class="line">                    <span class="keyword">if</span> sum_value == target:</span><br><span class="line">                        res.append([nums[i],nums[j],nums[l],nums[r]])</span><br><span class="line">                        l += <span class="number">1</span></span><br><span class="line">                        r -= <span class="number">1</span></span><br><span class="line">                        <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[l<span class="number">-1</span>]: l += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[r] == nums[r+<span class="number">1</span>]: r -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">elif</span> sum_value &lt; target:</span><br><span class="line">                        l += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>还可以使用combinations(nums, 4)来对原数组中得4个元素全排列，在开始sort后，对排列得到得元素进行set去重。但单纯利用combinations实现会超时。</p>
<h2 id="超出时间限制"><a href="#超出时间限制" class="headerlink" title="超出时间限制"></a>超出时间限制</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[List[int]]:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        <span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> combinations(nums, <span class="number">4</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span>(i) == target:</span><br><span class="line">                res.append(i)</span><br><span class="line">        res = <span class="built_in">set</span>(res)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">                </span><br></pre></td></tr></table></figure>
<h1 id="LeetCode-16-3Sum-Closest"><a href="#LeetCode-16-3Sum-Closest" class="headerlink" title="LeetCode 16 3Sum Closest"></a>LeetCode 16 3Sum Closest</h1><h2 id="题目描述-10"><a href="#题目描述-10" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出一个整形数组，寻找其中的三个元素a,b,c，使得a+b+c的值最接近另外一个给定的数字target。</p>
<p>如：给定数组 nums = [-1，2，1，-4], 和 target = 1.</p>
<p>与 target 最接近的三个数的和为 2. (-1 + 2 + 1 = 2).</p>
<h2 id="分析实现-9"><a href="#分析实现-9" class="headerlink" title="分析实现"></a>分析实现</h2><p>这道题也是2sum,3sum等题组中的，只不过变形的地方在于不是找相等的target，而是找最近的。</p>
<p>那么开始时可以随机设定一个三个数的和为结果值，在每次比较中，先判断三个数的和是否和target相等，如果相等直接返回和。如果不相等，则判断三个数的和与target的差是否小于这个结果值时，如果小于则进行则进行替换，并保存和的结果值。</p>
<h3 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先排序</span></span><br><span class="line">nums.sort()</span><br><span class="line"><span class="comment"># 随机选择一个和作为结果值</span></span><br><span class="line">res = nums[<span class="number">0</span>] + nums[<span class="number">1</span>] + nums[<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 记录这个差值</span></span><br><span class="line">diff = <span class="built_in">abs</span>(nums[<span class="number">0</span>]+nums[<span class="number">1</span>]+nums[<span class="number">2</span>]-target)</span><br><span class="line"><span class="comment"># 第一遍遍历</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="comment"># 标记好剩余元素的l和r</span></span><br><span class="line">    l,r = i+<span class="number">1</span>, <span class="built_in">len</span>(nums<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">while</span> l &lt; r:</span><br><span class="line">        <span class="keyword">if</span> 后续的值等于target:</span><br><span class="line">            <span class="keyword">return</span> 三个数值得和</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> 差值小于diff:</span><br><span class="line">                更新diff值</span><br><span class="line">                更新res值</span><br><span class="line">            <span class="keyword">if</span> 和小于target:</span><br><span class="line">                将l移动</span><br><span class="line">            <span class="keyword">else</span>:(开始已经排除了等于得情况，要判断和大于target)</span><br><span class="line">                将r移动</span><br></pre></td></tr></table></figure>
<h3 id="3Sum问题两层遍历得套路代码："><a href="#3Sum问题两层遍历得套路代码：" class="headerlink" title="3Sum问题两层遍历得套路代码："></a>3Sum问题两层遍历得套路代码：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums.sort()</span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)<span class="number">-2</span>):</span><br><span class="line">    l,r = i+<span class="number">1</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> l &lt; r:</span><br><span class="line">        <span class="built_in">sum</span> = nums[i] + nums[l] + nums[r]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span> == <span class="number">0</span>:</span><br><span class="line">            res.append([nums[i],nums[l],nums[r]])</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; <span class="number">0</span>:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            r -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSumClosest</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; int:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        diff = <span class="built_in">abs</span>(nums[<span class="number">0</span>]+nums[<span class="number">1</span>]+nums[<span class="number">2</span>]-target)</span><br><span class="line">        res = nums[<span class="number">0</span>] + nums[<span class="number">1</span>] + nums[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            l,r = i+<span class="number">1</span>,<span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">            t = target - nums[i]</span><br><span class="line">            <span class="keyword">while</span> l &lt; r:</span><br><span class="line">                <span class="keyword">if</span> nums[l] + nums[r] == t:</span><br><span class="line">                    <span class="keyword">return</span> nums[i] + t</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">abs</span>(nums[l]+nums[r]-t) &lt; diff:</span><br><span class="line">                        diff = <span class="built_in">abs</span>(nums[l]+nums[r]-t)</span><br><span class="line">                        res = nums[i]+nums[l]+nums[r]</span><br><span class="line">                    <span class="keyword">if</span> nums[l]+nums[r] &lt; t:</span><br><span class="line">                        l += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>时间复杂度为O(n^2)，空间复杂度为O(1);</p>
<h1 id="LeetCode-454-4SumⅡ"><a href="#LeetCode-454-4SumⅡ" class="headerlink" title="LeetCode 454 4SumⅡ"></a>LeetCode 454 4SumⅡ</h1><h2 id="题目描述-11"><a href="#题目描述-11" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出四个整形数组A,B,C,D,寻找有多少i,j,k,l的组合,使得A[i]+B[j]+C[k]+D[l]=0。其中,A,B,C,D中均含有相同的元素个数N，且0&lt;=N&lt;=500；</p>
<p>输入:</p>
<p>A = [ 1, 2]<br>B = [-2,-1]<br>C = [-1, 2]<br>D = [ 0, 2]</p>
<p>输出:2</p>
<h2 id="分析实现-10"><a href="#分析实现-10" class="headerlink" title="分析实现"></a>分析实现</h2><p>这个问题同样是Sum类问题得变种，其将同一个数组的条件，变为了四个数组中，依然可以用查找表的思想来实现。</p>
<p>首先可以考虑把D数组中的元素都放入查找表，然后遍历前三个数组，判断target减去每个元素后的值是否在查找表中存在，存在的话，把结果值加1。那么查找表的数据结构选择用set还是dict？考虑到数组中可能存在重复的元素，而重复的元素属于不同的情况，因此用dict存储，最后的结果值加上dict相应key的value，代码如下：</p>
<h3 id="O-n-3-代码"><a href="#O-n-3-代码" class="headerlink" title="O(n^3)代码"></a>O(n^3)代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">record = Counter()</span><br><span class="line"><span class="comment"># 先建立数组D的查找表</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(D)):</span><br><span class="line">    record[D[i]] += <span class="number">1</span></span><br><span class="line">res = <span class="number">0</span> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(B)):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C)):</span><br><span class="line">            num_find = <span class="number">0</span>-A[i]-B[j]-C[k]</span><br><span class="line">            <span class="keyword">if</span> record.get(num_find) != <span class="literal">None</span>:</span><br><span class="line">                res += record(num_find)</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>但是对于题目中给出的数据规模：N&lt;=500，如果N为500时，n^3的算法依然消耗很大，能否再进行优化呢？</p>
<p>根据之前的思路继续往前走，如果只遍历两个数组，那么就可以得到O(n^2)级别的算法，但是遍历两个数组，那么还剩下C和D两个数组，上面的值怎么放？</p>
<p>对于查找表问题而言，<strong>很多时候到底要查找什么</strong>，是解决的关键。对于C和D的数组，可以通过dict来记录其中和的个数，之后遍历结果在和中进行查找。代码如下：</p>
<h3 id="O-n-2-级代码"><a href="#O-n-2-级代码" class="headerlink" title="O(n^2)级代码"></a>O(n^2)级代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSumCount</span>(<span class="params">self, A: List[<span class="built_in">int</span>], B: List[<span class="built_in">int</span>], C: List[<span class="built_in">int</span>], D: List[<span class="built_in">int</span>]</span>) -&gt; int:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        record = Counter()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(B)):</span><br><span class="line">                record[A[i]+B[j]] += <span class="number">1</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(D)):</span><br><span class="line">                find_num = <span class="number">0</span> - C[i] - D[j]</span><br><span class="line">                <span class="keyword">if</span> record.get(find_num) != <span class="literal">None</span>:</span><br><span class="line">                    res += record[find_num]</span><br><span class="line">        <span class="keyword">return</span> res   </span><br></pre></td></tr></table></figure>
<p>再使用Pythonic的列表生成式和sum函数进行优化，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSumCount</span>(<span class="params">self, A: List[<span class="built_in">int</span>], B: List[<span class="built_in">int</span>], C: List[<span class="built_in">int</span>], D: List[<span class="built_in">int</span>]</span>) -&gt; int:</span></span><br><span class="line">        record = collections.Counter(a + b <span class="keyword">for</span> a <span class="keyword">in</span> A <span class="keyword">for</span> b <span class="keyword">in</span> B)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(record.get(- c - d, <span class="number">0</span>) <span class="keyword">for</span> c <span class="keyword">in</span> C <span class="keyword">for</span> d <span class="keyword">in</span> D)</span><br></pre></td></tr></table></figure></p>
<h1 id="LeetCode-49-Group-Anagrams"><a href="#LeetCode-49-Group-Anagrams" class="headerlink" title="LeetCode 49 Group Anagrams"></a>LeetCode 49 Group Anagrams</h1><h2 id="题目描述-12"><a href="#题目描述-12" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出一个字符串数组，将其中所有可以通过颠倒字符顺序产生相同结果的单词进行分组。</p>
<p>示例:</p>
<p>输入: [“eat”, “tea”, “tan”, “ate”, “nat”, “bat”],</p>
<p>输出:[[“ate”,”eat”,”tea”],[“nat”,”tan”],[“bat”]]</p>
<p>说明：</p>
<p>所有输入均为小写字母。<br>不考虑答案输出的顺序。</p>
<h2 id="分析实现-11"><a href="#分析实现-11" class="headerlink" title="分析实现"></a>分析实现</h2><p>在之前LeetCode 242的问题中，对字符串t和s来判断，判断t是否是s的字母异位词。当时的方法是通过构建t和s的字典，比较字典是否相同来判断是否为异位词。</p>
<p>在刚开始解决这个问题时，我也局限于了这个思路，以为是通过移动指针，来依次比较两个字符串是否对应的字典相等，进而确定异位词列表，再把异位词列表添加到结果集res中。于是有：</p>
<h3 id="错误思路"><a href="#错误思路" class="headerlink" title="错误思路"></a>错误思路</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = [<span class="string">&quot;eat&quot;</span>, <span class="string">&quot;tea&quot;</span>, <span class="string">&quot;tan&quot;</span>, <span class="string">&quot;ate&quot;</span>, <span class="string">&quot;nat&quot;</span>, <span class="string">&quot;bat&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">cum = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    l,r = i+<span class="number">1</span>,<span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    i_dict = Counter(nums[i])</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">if</span> nums[i] <span class="keyword">not</span> <span class="keyword">in</span> cum:</span><br><span class="line">        res.append(nums[i])</span><br><span class="line">    <span class="keyword">while</span> l &lt; r:</span><br><span class="line">        l_dict = Counter(nums[l])</span><br><span class="line">        r_dict = Counter(nums[r])</span><br><span class="line">        <span class="keyword">if</span> i_dict == l_dict <span class="keyword">and</span> l_dict == r_dict:</span><br><span class="line">            res.append(nums[l],nums[r])</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">            r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> i_dict == l_dict:</span><br><span class="line">            res.append(nums[l])</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> i_dict == r_dict:</span><br><span class="line">            res.append(nums[r])</span><br><span class="line">            r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">    print(res)</span><br><span class="line">    cum.append(res)</span><br><span class="line">......................................</span><br></pre></td></tr></table></figure>
<p>这时发现长长绵绵考虑不完，而且还要注意指针的条件，怎样遍历才能遍历所有的情况且判断列表是否相互间包含。。。</p>
<p>于是立即开始反思是否哪块考虑错了?回顾第一开始的选择数据结构，在dict和list中，自己错误的选择了list来当作数据结构，进而用指针移动来判断元素的情况。而<strong>没有利用题目中不变的条件</strong>。</p>
<p>题目的意思，对异位词的进行分组，同异位词的分为一组，那么考虑对这一组内什么是相同的，且这个相同的也能作为不同组的判断条件。</p>
<p>不同组的判断条件，就可以用数据结构dict中的key来代表，那么什么相同的适合当作key呢？</p>
<p>这时回顾下下LeetCode 242，当时是因为异位字符串中包含的<strong>字符串的字母个数</strong>都是相同的，故把字母当作key来进行判断是否为异位词。</p>
<p>但是对于本题，把每个字符串的字母dict，再当作字符串数组的dict的key，显然不太合适，那么对于异位词，还有什么是相同的？</p>
<p>显然，如果将字符串统一排序，<strong>异位词排序后的字符串</strong>，显然都是相同的。那么就可以把其当作key，把遍历的数组中的异位词当作value，对字典进行赋值，进而遍历字典的value，得到结果list。</p>
<p>需要注意的细节是，<strong>字符串和list之间的转换</strong>：</p>
<ol>
<li>默认构造字典需为list的字典；</li>
<li>排序使用sorted()函数，而不用list.sort()方法，因为其不返回值；</li>
<li>通过’’.join(list)，将list转换为字符串；</li>
<li>通过str.split(‘,’)将字符串整个转换为list中的一项；</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">groupAnagrams</span>(<span class="params">self, strs: List[<span class="built_in">str</span>]</span>) -&gt; List[List[str]]:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        strs_dict = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">str</span> <span class="keyword">in</span> strs:</span><br><span class="line">            key = <span class="string">&#x27;&#x27;</span>.join(<span class="built_in">sorted</span>(<span class="built_in">list</span>(<span class="built_in">str</span>)))</span><br><span class="line">            strs_dict[key] += <span class="built_in">str</span>.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> strs_dict.values():</span><br><span class="line">            res.append(v)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>再将能用列表生成式替换的地方替换掉,代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">groupAnagrams</span>(<span class="params">self, strs: List[<span class="built_in">str</span>]</span>) -&gt; List[List[str]]:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        strs_dict = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">str</span> <span class="keyword">in</span> strs:</span><br><span class="line">            key = <span class="string">&#x27;&#x27;</span>.join(<span class="built_in">sorted</span>(<span class="built_in">list</span>(<span class="built_in">str</span>)))</span><br><span class="line">            strs_dict[key] += <span class="built_in">str</span>.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> [v <span class="keyword">for</span> v <span class="keyword">in</span> strs_dict.values()]</span><br></pre></td></tr></table></figure>
<h1 id="LeetCode-447-Number-of-Boomerangs"><a href="#LeetCode-447-Number-of-Boomerangs" class="headerlink" title="LeetCode 447 Number of Boomerangs"></a>LeetCode 447 Number of Boomerangs</h1><h2 id="题目描述-13"><a href="#题目描述-13" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出一个平面上的n个点，寻找存在多少个由这些点构成的三元组(i,j,k)，<strong>使得i,j两点的距离等于i,k两点的距离</strong>。</p>
<p>其中n最多为500,且所有的点坐标的范围在[-10000,10000]之间。</p>
<p>输入:</p>
<p>[[0,0],[1,0],[2,0]]</p>
<p>输出:</p>
<p>2</p>
<p>解释:</p>
<p>两个结果为： [[1,0],[0,0],[2,0]] 和 [[1,0],[2,0],[0,0]]</p>
<h2 id="分析实现-12"><a href="#分析实现-12" class="headerlink" title="分析实现"></a>分析实现</h2><h3 id="原始思路"><a href="#原始思路" class="headerlink" title="原始思路"></a>原始思路</h3><p>题目的要求是：使得i,j两点的距离等于i,k两点的距离，那么相当于是比较三个点之间距离的，那么开始的思路就是三层遍历，i从0到len，j从i+1到len，k从j+1到len，然后比较三个点的距离，相等则结果数加一。</p>
<p>显然这样的时间复杂度为O(n^3)，对于这道题目，能否用查找表的思路进行解决优化？</p>
<h3 id="查找表"><a href="#查找表" class="headerlink" title="查找表"></a>查找表</h3><p>之前的查找表问题，大多是通过<strong>构建一个查找表</strong>，而避免了在查找中再内层嵌套循环，从而降低了时间复杂度。那么可以考虑在这道题中，可以通过查找表进行代替哪两层循环。</p>
<p>当i,j两点距离等于i,k时，用查找表的思路，等价于：对距离key(i,j或i,k的距离)，其值value(个数)为2。</p>
<p>那么就可以做一个查找表，用来查找相同距离key的个数value是多少。遍历每一个节点i，扫描得到其他点到节点i的距离，在查找表中，对应的键就是距离的值，对应的值就是距离值得个数。</p>
<p>在拿到对于元素i的距离查找表后，接下来就是排列选择问题了：</p>
<ol>
<li>如果当距离为x的值有2个时，那么选择j,k的可能情况有：第一次选择有2种，第二次选择有1种，为2*1；</li>
<li>如果当距离为x的值有3个时，那么选择j,k的可能的情况有：第一次选择有3种，第二次选择有2种，为3*2;</li>
<li>那么当距离为x的值有n个时，选择j,k的可能情况有：第一次选择有n种，第二次选择有n-1种。</li>
</ol>
<h3 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h3><p>对于距离值的求算，按照欧式距离的方法进行求算的话，容易产生浮点数，可以将根号去掉，用差的平方和来进行比较距离。</p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numberOfBoomerangs</span>(<span class="params">self, points: List[List[<span class="built_in">int</span>]]</span>) -&gt; int:</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> points:</span><br><span class="line">            record = Counter()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> points:</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    record[self.dis(i,j)] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> k,v <span class="keyword">in</span> record.items():</span><br><span class="line">                res += v*(v<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dis</span>(<span class="params">self,point1,point2</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (point1[<span class="number">0</span>]-point2[<span class="number">0</span>]) ** <span class="number">2</span> + (point1[<span class="number">1</span>]-point2[<span class="number">1</span>]) ** <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>对实现的代码进行优化：</p>
<ol>
<li>将for循环遍历改为列表生成式;</li>
<li>对sum+=的操作，考虑使用sum函数。</li>
<li>对不同的函数使用闭包的方式内嵌；</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numberOfBoomerangs</span>(<span class="params">self, points: List[List[<span class="built_in">int</span>]]</span>) -&gt; int:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x1, y1</span>):</span></span><br><span class="line">            <span class="comment"># 对一个i下j,k的距离值求和</span></span><br><span class="line">            d = Counter((x2 - x1) ** <span class="number">2</span> + (y2 - y1) ** <span class="number">2</span> <span class="keyword">for</span> x2, y2 <span class="keyword">in</span> points)</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">sum</span>(t * (t<span class="number">-1</span>) <span class="keyword">for</span> t <span class="keyword">in</span> d.values())</span><br><span class="line">        <span class="comment"># 对每个i的距离进行求和</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(f(x1, y1) <span class="keyword">for</span> x1, y1 <span class="keyword">in</span> points)</span><br></pre></td></tr></table></figure>
<h1 id="LeetCode-149-Max-Points-on-a-Line"><a href="#LeetCode-149-Max-Points-on-a-Line" class="headerlink" title="LeetCode 149 Max Points on a Line"></a>LeetCode 149 Max Points on a Line</h1><h2 id="题目描述-14"><a href="#题目描述-14" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个二维平面，平面上有 n 个点，求最多有多少个点在同一条直线上。</p>
<p>示例 1:</p>
<p>输入: [[1,1],[2,2],[3,3]]</p>
<p>输出: 3</p>
<p>示例 2:</p>
<p>输入: [[1,1],[3,2],[5,3],[4,1],[2,3],[1,4]]</p>
<p>输出: 4</p>
<h2 id="分析实现-13"><a href="#分析实现-13" class="headerlink" title="分析实现"></a>分析实现</h2><p>本道题目的要求是：看有多少个点在同一条直线上，那么判断点是否在一条直线上，其实就等价于判断i,j两点的斜率是否等于i,k两点的斜率。</p>
<p>回顾上道447题目中的要求：使得i,j两点的距离等于i,k两点的距离，那么在这里，直接考虑使用查找表实现，即<strong>查找相同斜率key的个数value是多少</strong>。</p>
<p>在上个问题中，i和j，j和i算是两种不同的情况，但是这道题目中，这是属于相同的两个点，<br>因此在对遍历每个i,查找与i相同斜率的点时，不能再对结果数res++，而应该取查找表中的最大值。如果有两个斜率相同时，返回的应该是3个点，故返回的是结果数+1。</p>
<p>查找表实现套路如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxPoints</span>(<span class="params">self,points</span>):</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(points)):</span><br><span class="line">            record = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(points)):</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    record[self.get_Slope(points,i,j)] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> record.values():</span><br><span class="line">                res = <span class="built_in">max</span>(res, v)</span><br><span class="line">        <span class="keyword">return</span> res + <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_Slope</span>(<span class="params">self,points,i,j</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (points[i][<span class="number">0</span>] - points[j][<span class="number">0</span>]) / (points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>但是这样会出现一个问题，即斜率的求算中，有时会出现直线为垂直的情况，故需要对返回的结果进行判断，如果分母为0，则返回inf，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_Slope</span>(<span class="params">self,points,i,j</span>):</span></span><br><span class="line">    <span class="keyword">if</span> points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> (points[i][<span class="number">0</span>] - points[j][<span class="number">0</span>]) / (points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>再次提交，发现对于空列表的测试用例会判断错误，于是对边界情况进行判断，如果初始长度小于等于1,则直接返回len：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(points) &lt;= <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(points)</span><br></pre></td></tr></table></figure></p>
<p>再次提交，对于相同元素的测试用例会出现错误，回想刚才的过程，当有相同元素时，题目的要求是算作两个不同的点，但是在程序运行时，会将其考虑为相同的点，return回了inf。但在实际运行时，需要对相同元素的情况单独考虑。</p>
<p>于是可以设定samepoint值，遍历时判断，如果相同时，same值++,最后取v+same的值作为结果数。</p>
<p>考虑到如果全是相同值，那么这时dict中的record为空，也要将same值当作结果数返回，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxPoints</span>(<span class="params">self,points</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(points) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(points)</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(points)):</span><br><span class="line">            record = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">            samepoint = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(points)):</span><br><span class="line">                <span class="keyword">if</span> points[i][<span class="number">0</span>] == points[j][<span class="number">0</span>] <span class="keyword">and</span> points[i][<span class="number">1</span>] == points[j][<span class="number">1</span>]:</span><br><span class="line">                    samepoint += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    record[self.get_Slope(points,i,j)] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> record.values():</span><br><span class="line">                res = <span class="built_in">max</span>(res, v+samepoint)</span><br><span class="line">            res = <span class="built_in">max</span>(res, samepoint)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_Slope</span>(<span class="params">self,points,i,j</span>):</span></span><br><span class="line">        <span class="keyword">if</span> points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (points[i][<span class="number">0</span>] - points[j][<span class="number">0</span>]) / (points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>时间复杂度为O(n^2)，空间复杂度为O(n)</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>遍历时多用索引，而不要直接用值进行遍历；</p>
<h1 id="三-滑动数组"><a href="#三-滑动数组" class="headerlink" title="三. 滑动数组"></a>三. 滑动数组</h1><h1 id="LeetCode-219-Contains-Dupliccate-Ⅱ"><a href="#LeetCode-219-Contains-Dupliccate-Ⅱ" class="headerlink" title="LeetCode 219 Contains Dupliccate Ⅱ"></a>LeetCode 219 Contains Dupliccate Ⅱ</h1><h2 id="题目描述-15"><a href="#题目描述-15" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出一个整形数组nums和一个整数k，是否存在索引i和j，使得nums[i]==nums[j]，且i和J之间的差不超过k。</p>
<p>示例1:</p>
<p>输入: nums = [1,2,3,1], k = 3</p>
<p>输出: true</p>
<p>示例 2:</p>
<p>输入: nums = [1,2,3,1,2,3], k = 2</p>
<p>输出: false</p>
<h2 id="分析实现-14"><a href="#分析实现-14" class="headerlink" title="分析实现"></a>分析实现</h2><p>翻译下这个题目：在这个数组中，如果有两个元素索引i和j，它们对应的元素是相等的，且索引j-i是小于等于k，那么就返回True，否则返回False。</p>
<p>因为对于这道题目可以用暴力解法双层循环，即：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure></p>
<p>故这道题目可以考虑使用滑动数组来解决：</p>
<p>固定滑动数组的长度为K+1，当这个滑动数组内如果能找到两个元素的值相等，就可以保证两个元素的索引的差是小于等于k的。如果当前的滑动数组中没有元素相同，就右移滑动数组的右边界r,同时将左边界l右移。查看r++的元素是否在l右移过后的数组里，如果不在就将其添加数组，在的话返回true表示两元素相等。</p>
<p>因为滑动数组中的元素是不同的，考虑用set作为数据结构：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyDuplicate</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        record = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[i] <span class="keyword">in</span> record:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            record.add(nums[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(record) == k+<span class="number">1</span>:</span><br><span class="line">                record.remove(nums[i-k])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>时间复杂度为O(n)，空间复杂度为O(n)</p>
<h1 id="LeetCode-220-Contains-Dupliccate-Ⅲ"><a href="#LeetCode-220-Contains-Dupliccate-Ⅲ" class="headerlink" title="LeetCode 220 Contains Dupliccate Ⅲ"></a>LeetCode 220 Contains Dupliccate Ⅲ</h1><h2 id="题目描述-16"><a href="#题目描述-16" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个整数数组，判断数组中是否有两个不同的索引 i 和 j，使得nums [i] 和nums [j]的差的绝对值最大为 t，并且 i 和 j 之间的差的绝对值最大为 ķ。</p>
<p>示例 1:</p>
<p>输入: nums = [1,2,3,1], k = 3, t = 0</p>
<p>输出: true</p>
<p>示例 2:</p>
<p>输入: nums = [1,0,1,1], k = 1, t = 2</p>
<p>输出: true</p>
<p>示例 3:</p>
<p>输入: nums = [1,5,9,1,5,9], k = 2, t = 3</p>
<p>输出: false</p>
<h2 id="分析实现-15"><a href="#分析实现-15" class="headerlink" title="分析实现"></a>分析实现</h2><p>相比较上一个问题，这个问题多了一个限定条件，条件不仅索引差限定k，数值差也限定为了t。</p>
<p>将索引的差值固定，于是问题和上道一样，同样转化为了固定长度K+1的滑动窗口内，是否存在两个值的差距不超过 t，考虑使用<strong>滑动窗口</strong>的思想来解决。</p>
<p>在遍历的过程中，目的是要在“已经出现、但还未滑出滑动窗口”的所有数中查找，是否有一个数与滑动数组中的数的<strong>差的绝对值</strong>最大为 t。对于差的绝对值最大为t，实际上等价于所要找的这个元素v的范围是在v-t到v+t之间，即查找“滑动数组”中的元素有没有[v-t，v+t]范围内的数存在。</p>
<p>因为只需证明是否存在即可，这时判断的逻辑是：如果在滑动数组<strong>查找比v-t大的最小的元素</strong>,如果这个元素小于等于v+t,即可以证明存在[v-t,v+t]。</p>
<p>那么实现过程其实和上题是一致的，只是上题中的判断条件是<strong>在查找表中找到和nums[i]相同的元素</strong>，而这题中的判断条件是<strong>查找比v-t大的最小的元素，判断其小于等于v+t</strong>，下面是实现的框架：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyDuplicate</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        record = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> 查找的比v-t大的最小的元素 &lt;= v+t:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            record.add(nums[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(record) == k+<span class="number">1</span>:</span><br><span class="line">                record.remove(nums[i-k])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>接下来考虑，如何查找比v-t大的最小的元素呢？</p>
<p>【注：C++中有lower_bound(v-t)的实现，py需要自己写函数】</p>
<p>当然首先考虑可以通过O(n)的解法来完成，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span>(<span class="params">self,array,v</span>):</span></span><br><span class="line">    array = <span class="built_in">list</span>(array)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(array)):</span><br><span class="line">        <span class="keyword">if</span> array[i] &gt;= v:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><br>但是滑动数组作为set，是有序的数组。对于有序的数组，应该第一反应就是<strong>二分查找</strong>，于是考虑二分查找实现，查找比v-t大的最小的元素：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span>(<span class="params">self, nums, target</span>):</span></span><br><span class="line">    low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> low&lt;high:</span><br><span class="line">        mid = <span class="built_in">int</span>((low+high)/<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid</span><br><span class="line">    <span class="keyword">return</span> low <span class="keyword">if</span> nums[low] &gt;= target <span class="keyword">else</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><br>整体代码实现如下，时间复杂度为O(nlogn),空间复杂度为O(n):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyAlmostDuplicate</span>(<span class="params">self, nums, k, t</span>) -&gt; bool:</span></span><br><span class="line">        record = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(record) != <span class="number">0</span>:</span><br><span class="line">                rec = <span class="built_in">list</span>(record)</span><br><span class="line">                find_index = self.lower_bound(rec,nums[i]-t)</span><br><span class="line">                <span class="keyword">if</span> find_index != <span class="number">-1</span> <span class="keyword">and</span> rec[find_index] &lt;= nums[i] + t:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            record.add(nums[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(record) == k + <span class="number">1</span>:</span><br><span class="line">                record.remove(nums[i - k])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span>(<span class="params">self, nums, target</span>):</span></span><br><span class="line">        low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> low&lt;high:</span><br><span class="line">            mid = <span class="built_in">int</span>((low+high)/<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">                low = mid+<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid</span><br><span class="line">        <span class="keyword">return</span> low <span class="keyword">if</span> nums[low] &gt;= target <span class="keyword">else</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<p>当然。。。在和小伙伴一起刷的时候，这样写的O(n^2)的结果会比上面要高，讨论的原因应该是上面的步骤存在着大量set和list的转换导致，对于py，仍旧是考虑算法思想实现为主，下面是O(n^2)的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyAlmostDuplicate</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], k: <span class="built_in">int</span>, t: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        <span class="keyword">if</span> t == <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(nums) == <span class="built_in">len</span>(<span class="built_in">set</span>(nums)):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,k+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> i+j &gt;= <span class="built_in">len</span>(nums): <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(nums[i+j]-nums[i]) &lt;= t: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="小套路："><a href="#小套路：" class="headerlink" title="小套路："></a>小套路：</h2><p>二分查找实现，查找比v-t大的最小的元素：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span>(<span class="params">self, nums, target</span>):</span></span><br><span class="line">    low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> low&lt;high:</span><br><span class="line">        mid = <span class="built_in">int</span>((low+high)/<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid</span><br><span class="line">    <span class="keyword">return</span> low <span class="keyword">if</span> nums[low] &gt;= target <span class="keyword">else</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<p>二分查找实现，查找比v-t大的最小的元素：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">upper_bound</span>(<span class="params">nums, target</span>):</span></span><br><span class="line">    low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> low&lt;high:</span><br><span class="line">        mid=(low+high)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid]&lt;=target:</span><br><span class="line">            low = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:<span class="comment">#&gt;</span></span><br><span class="line">            high = mid</span><br><span class="line">            pos = high</span><br><span class="line">    <span class="keyword">if</span> nums[low]&gt;target:</span><br><span class="line">        pos = low</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure></p>
<h1 id="四-二分查找"><a href="#四-二分查找" class="headerlink" title="四. 二分查找"></a>四. 二分查找</h1><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>查找在算法题中是很常见的，但是怎么最大化查找的效率和写出bugfree的代码才是难的部分。一般查找方法有顺序查找、二分查找和双指针，推荐一开始可以直接用顺序查找，如果遇到TLE的情况再考虑剩下的两种，毕竟AC是最重要的。</p>
<p>一般二分查找的对象是有序或者由有序部分变化的（可能暂时理解不了，看例题即可），但还存在一种可以运用的地方是按值二分查找，之后会介绍。</p>
<h2 id="代码模板"><a href="#代码模板" class="headerlink" title="代码模板"></a>代码模板</h2><p>总体来说二分查找是比较简单的算法，网上看到的写法也很多，掌握一种就可以了。<br>以下是我的写法，参考C++标准库里<algorithm>的写法。这种写法比较好的点在于：</p>
<ul>
<li>1.即使区间为空、答案不存在、有重复元素、搜索开/闭区间的上/下界也同样适用</li>
<li>2.+-1 的位置调整只出现了一次，而且最后返回lo还是hi都是对的，无需纠结</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">firstBadVersion</span>(<span class="params">self, arr</span>):</span></span><br><span class="line">        <span class="comment"># 第一点</span></span><br><span class="line">        lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(arr)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">            <span class="comment"># 第二点</span></span><br><span class="line">            mid = (lo+hi) // <span class="number">2</span></span><br><span class="line">            <span class="comment"># 第三点</span></span><br><span class="line">            <span class="keyword">if</span> f(x):</span><br><span class="line">                lo = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hi = mid</span><br><span class="line">        <span class="keyword">return</span> lo</span><br></pre></td></tr></table></figure>
<p><strong>解释</strong>：</p>
<ul>
<li>第一点：lo和hi分别对应搜索的上界和下界，但不一定为0和arr最后一个元素的下标。</li>
<li>第二点：因为Python没有溢出，int型不够了会自动改成long int型，所以无需担心。如果再苛求一点，可以把这一行改成<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mid = lo + (hi-lo) // <span class="number">2</span></span><br><span class="line"><span class="comment"># 之所以 //2 这部分不用位运算 &gt;&gt; 1 是因为会自动优化，效率不会提升</span></span><br></pre></td></tr></table></figure></li>
<li>第三点：<br>比较重要的就是这个f(x)，在带入模板的情况下，写对函数就完了。</li>
</ul>
<p>那么我们一步一步地揭开二分查找的神秘面纱，首先来一道简单的题。</p>
<h2 id="LeetCode-35-Search-Insert-Position"><a href="#LeetCode-35-Search-Insert-Position" class="headerlink" title="LeetCode 35. Search Insert Position"></a>LeetCode 35. Search Insert Position</h2><p>给定排序数组和目标值，如果找到目标，则返回索引。如果不是，则返回按顺序插入索引的位置的索引。 您可以假设数组中没有重复项。</p>
<p><strong>Example</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Example 1:</span><br><span class="line">Input: [1,3,5,6], 5</span><br><span class="line">Output: 2</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line">Input: [1,3,5,6], 2</span><br><span class="line">Output: 1</span><br><span class="line"></span><br><span class="line">Example 3:</span><br><span class="line">Input: [1,3,5,6], 7</span><br><span class="line">Output: 4</span><br><span class="line"></span><br><span class="line">Example 4:</span><br><span class="line">Input: [1,3,5,6], 0</span><br><span class="line">Output: 0</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong> 这里要注意的点是 high 要设置为 len(nums) 的原因是像第三个例子会超出数组的最大值，所以要让 lo 能到 这个下标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">searchInsert</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; int:</span>        </span><br><span class="line">        lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">            mid = (lo + hi) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">                lo = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hi = mid</span><br><span class="line">        <span class="keyword">return</span> lo</span><br></pre></td></tr></table></figure>
<h2 id="LeetCode540-Single-Element-in-a-Sorted-Array"><a href="#LeetCode540-Single-Element-in-a-Sorted-Array" class="headerlink" title="LeetCode540. Single Element in a Sorted Array"></a>LeetCode540. Single Element in a Sorted Array</h2><p>您将获得一个仅由整数组成的排序数组，其中每个元素精确出现两次，但一个元素仅出现一次。 找到只出现一次的单个元素。</p>
<p><strong>Example</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: [1,1,2,3,3,4,4,8,8]</span><br><span class="line">Output: 2</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: [3,3,7,7,10,11,11]</span><br><span class="line">Output: 10</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong> 异或的巧妙应用！如果mid是偶数，那么和1异或的话，那么得到的是mid+1，如果mid是奇数，得到的是mid-1。如果相等的话，那么唯一的元素还在这之后，往后找就可以了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNonDuplicate</span>(<span class="params">self, nums</span>):</span></span><br><span class="line">        lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">            mid = (lo + hi) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == nums[mid ^ <span class="number">1</span>]:</span><br><span class="line">                lo = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hi = mid</span><br><span class="line">        <span class="keyword">return</span> nums[lo]</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>是不是还挺简单哈哈，那我们来道HARD难度的题!</strong></p>
<hr>
<h2 id="LeetCode-410-Split-Array-Largest-Sum"><a href="#LeetCode-410-Split-Array-Largest-Sum" class="headerlink" title="LeetCode 410. Split Array Largest Sum"></a>LeetCode 410. Split Array Largest Sum</h2><p>给定一个由非负整数和整数m组成的数组，您可以将该数组拆分为m个非空连续子数组。编写算法以最小化这m个子数组中的最大和。</p>
<p><strong>Example</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input:</span><br><span class="line">nums &#x3D; [7,2,5,10,8]</span><br><span class="line">m &#x3D; 2</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">18</span><br><span class="line"></span><br><span class="line">Explanation:</span><br><span class="line">There are four ways to split nums into two subarrays.</span><br><span class="line">The best way is to split it into [7,2,5] and [10,8],</span><br><span class="line">where the largest sum among the two subarrays is only 18.</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>这其实就是二分查找里的按值二分了，可以看出这里的元素就无序了。但是我们的目标是找到一个合适的最小和，换个角度理解我们要找的值在最小值max(nums)和sum(nums)内，而这两个值中间是连续的。是不是有点难理解，那么看代码吧</li>
<li>辅助函数的作用是判断当前的“最小和”的情况下，区间数是多少，来和m判断</li>
<li>这里的下界是数组的最大值是因为如果比最大值小那么一个区间就装不下，数组的上界是数组和因为区间最少是一个，没必要扩大搜索的范围</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">splitArray</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], m: <span class="built_in">int</span></span>) -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span>(<span class="params">mid</span>):</span></span><br><span class="line">            res = tmp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">                <span class="keyword">if</span> tmp + num &lt;= mid:</span><br><span class="line">                    tmp += num</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res += <span class="number">1</span></span><br><span class="line">                    tmp = num</span><br><span class="line">            <span class="keyword">return</span> res + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        lo, hi = <span class="built_in">max</span>(nums), <span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">            mid = (lo + hi) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> helper(mid) &gt; m:</span><br><span class="line">                lo = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hi = mid</span><br><span class="line">        <span class="keyword">return</span> lo</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Learning</category>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>查找表</tag>
      </tags>
  </entry>
</search>
