<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>6.824Lecture1-MapReduce</title>
    <url>/2021/12/11/6-824Lecture1/</url>
    <content><![CDATA[<h1 id="Backgorund"><a href="#Backgorund" class="headerlink" title="Backgorund"></a>Backgorund</h1><ul>
<li>big data</li>
<li>lots of kv data stuctures, like inverted index</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Abstract-view"><a href="#Abstract-view" class="headerlink" title="Abstract view"></a>Abstract view</h2><img src="/2021/12/11/6-824Lecture1/workflow.png" class="" title="workflow">
<a id="more"></a> 
<ul>
<li><strong>split</strong> files from GFS to disks</li>
<li>master分配worker执行map任务，生成k,v值，存入disk，map回传disk地址给master</li>
<li>master传递地址给reduce worker，reduce worker使用RPC读disk数据</li>
<li><strong>sort by key</strong>, 并将所有values聚合</li>
<li>reduce</li>
<li>master返回reduce结果给GFS</li>
</ul>
<h2 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault tolerance"></a>Fault tolerance</h2><ul>
<li>fail -&gt; rerun: worker rerun不会产生其他问题</li>
<li>worker fault: Master 周期性的 ping 每个 Worker，如果指定时间内没回应就是挂了。将这个 Worker 标记为失效，分配给这个失效 Worker 的任务将被重新分配给其他 Worker；</li>
<li>master fault: 中止整个 MapReduce 运算，重新执行。</li>
</ul>
<h1 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h1><ul>
<li>bandwidth: 尽量保持worker和其执行的文件在同一台机器上，会相近的机器</li>
<li>slow workers: 吊起执行快的作为backup worker，取最先执行完的worker</li>
</ul>
<h1 id="Cons"><a href="#Cons" class="headerlink" title="Cons"></a>Cons</h1><ul>
<li>实时性</li>
<li>复杂需求时需要大量相互依赖的mr逻辑 -》难开发，难调试</li>
</ul>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf">paper</a></li>
<li><a href="https://www.youtube.com/watch?v=WtZ7pcRSkOA">video</a></li>
<li><a href="https://mp.weixin.qq.com/s/I0PBo_O8sl18O5cgMvQPYA">note</a></li>
<li><a href="https://www.the-paper-trail.org/post/2014-06-25-the-elephant-was-a-trojan-horse-on-the-death-of-map-reduce-at-google/">the reason why google gives up MapReduce</a></li>
</ul>
<p><strong>作为一种范式，而非产品？</strong></p>
]]></content>
      <categories>
        <category>Lecture</category>
        <category>6.824</category>
      </categories>
      <tags>
        <tag>6.824</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>6.824Lecture2-RPC&amp;Threads</title>
    <url>/2021/12/11/6-824Lecture2/</url>
    <content><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Why-Threads"><a href="#Why-Threads" class="headerlink" title="Why Threads?"></a>Why Threads?</h2><ul>
<li>I/O concurrency</li>
<li>multi core parallelism</li>
<li>convenience for, like routine</li>
</ul>
<a id="more"></a>
<h2 id="Threads-Changes"><a href="#Threads-Changes" class="headerlink" title="Threads Changes"></a>Threads Changes</h2><ul>
<li>race conditions<ul>
<li>avoid share memory</li>
<li>use locks: 并行变串行</li>
</ul>
</li>
<li>coordinations<ul>
<li>channels or condition variables</li>
</ul>
</li>
<li>dead lock</li>
</ul>
<h1 id="Go-Methods"><a href="#Go-Methods" class="headerlink" title="Go Methods"></a>Go Methods</h1><h2 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h2><ul>
<li>用来在并发中同步变量</li>
<li>pipeline with datatype, need be created, like: ch = make(chan int)</li>
<li>receivers &amp; sender.默认情况下，发送和接收操作在另一端准备好之前都会阻塞。这使得 Go 程可以在没有显式的锁或竞态变量的情况下进行同步<ul>
<li>sender: 通过close关闭信道表示没有要发送的值. <strong>向一个已经关闭的信道发送数据会引发程序panic</strong></li>
<li>receiver: v, ok := &lt;-ch, ok=false when channel is empty and is closed</li>
</ul>
</li>
</ul>
<h2 id="sync-Mutex"><a href="#sync-Mutex" class="headerlink" title="sync.Mutex"></a>sync.Mutex</h2><ul>
<li>互斥锁，代码段前后调用Lock() &amp; Unlock()实现某段代码的互斥执行，用defer保证互斥锁一定被解锁。<ul>
<li>defer func()：func会在包含defer语句的函数返回时再执行</li>
</ul>
</li>
</ul>
<h2 id="sync-Cond"><a href="#sync-Cond" class="headerlink" title="sync.Cond"></a>sync.Cond</h2><ul>
<li>Wait()/Broadcast()/Singal()</li>
</ul>
<h1 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h1><p><em>remote procedure call</em></p>
<ul>
<li>goal: rpc ≈ pc</li>
<li>pros: stub完成数据转换和解析、打开/关闭连接等细节</li>
</ul>
<h2 id="workflow"><a href="#workflow" class="headerlink" title="workflow"></a>workflow</h2><p><img src="https://pic1.zhimg.com/45366c44f775abfd0ac3b43bccc1abc3_r.jpg?source=1940ef5c" alt="workflow"></p>
<ol>
<li>客户端调用 client stub，并将调用参数 push 到栈（stack）中，这个调用是在本地的</li>
<li>client stub 将这些参数包装，并通过系统调用发送到服务端机器。打包的过程叫 marshalling（常见方式：XML、JSON、二进制编码）。</li>
<li>客户端操作系统将消息传给传输层，传输层发送信息至服务端；</li>
<li>服务端的传输层将消息传递给 server stub</li>
<li>server stub 解析信息。该过程叫 unmarshalling。</li>
<li>server stub 调用程序，并通过类似的方式返回给客户端。</li>
<li>客户端拿到数据解析后，将执行结果返回给调用者。</li>
</ol>
<h2 id="RPC-Semantics-under-failures"><a href="#RPC-Semantics-under-failures" class="headerlink" title="RPC Semantics under failures"></a>RPC Semantics under failures</h2><p><strong>server不知道client的具体情况，导致failure时难以处理</strong></p>
<ul>
<li>at-least-once: client不断重试直到ack成功。适用于幂等操作<ul>
<li>幂等操作：幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同</li>
</ul>
</li>
<li>at-most-once: <strong>最常见的</strong>，只执行0/1次。通过过滤重复项来实现。</li>
<li>exactly-once: hard</li>
</ul>
<p>go rpc: at-most-once, 由client手动决定是否重试 </p>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwODA2NjIxOA==&amp;mid=2247484193&amp;idx=1&amp;sn=693e0ff4bfcc6e02dea10ed9d639b41b&amp;chksm=970980e4a07e09f2647de63ed0bf3be98d9032a3797033af3872c692d2373f98627a63f30e22&amp;scene=178&amp;cur_album_id=1751707148520112128#rd">note</a></li>
<li><a href="https://www.bilibili.com/video/BV1e5411E7RM?p=2&amp;spm_id_from=pageDriver">video</a></li>
</ul>
]]></content>
      <categories>
        <category>Lecture</category>
        <category>6.824</category>
      </categories>
      <tags>
        <tag>6.824</tag>
        <tag>RPC</tag>
        <tag>Thread</tag>
      </tags>
  </entry>
  <entry>
    <title>C++-内存管理</title>
    <url>/2021/12/10/C++-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<h2 id="C-的内存分配方式"><a href="#C-的内存分配方式" class="headerlink" title="C++的内存分配方式"></a>C++的内存分配方式</h2><ul>
<li>全局/静态变量区</li>
<li>常量存储区</li>
<li>栈：在局部变量和函数参数在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是栈的内存容量有限。</li>
<li>堆：由new分配的内存块，需要手动通过delete来释放。如果没有delete，那么在程序结束后，操作系统会自动回收。</li>
<li>自由存储区：由malloc等分配的内存块，和堆是十分相似，不过它是用free来结束自己的生命的。</li>
</ul>
<a id="more"></a>
<h3 id="堆和栈"><a href="#堆和栈" class="headerlink" title="堆和栈"></a>堆和栈</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123; <span class="keyword">int</span>* p = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>]; &#125;</span><br><span class="line"><span class="keyword">delete</span> []p;</span><br></pre></td></tr></table></figure>
<ul>
<li>new了一个在堆上的数组，并在栈上创建一个指针p；用delete []p释放堆上的内存</li>
<li>未delete的话会造成内存泄漏。delete后的指针为野指针，</li>
<li><strong>new和delete时分别执行构造函数和析构函数</strong></li>
<li></li>
</ul>
<h4 id="管理方式-amp-申请后系统的响应"><a href="#管理方式-amp-申请后系统的响应" class="headerlink" title="管理方式&amp;申请后系统的响应"></a>管理方式&amp;申请后系统的响应</h4><ul>
<li>栈：由系统管理。不用手动申请，只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。  </li>
<li>堆：要new。<ul>
<li>首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序，另外，对于大多数系统，会在这块内存空间中的首地址处记录本次分配的大小，这样，代码中的delete语句才能正确的释放本内存空间。另外，由于找到的堆结点的大小不一定正好等于申请的大小，系统会自动的将多余的那部分重新放入空闲链表中。   </li>
</ul>
</li>
</ul>
<h4 id="空间大小"><a href="#空间大小" class="headerlink" title="空间大小"></a>空间大小</h4><ul>
<li>栈：在Windows下,<strong>栈是向低地址扩展</strong>的数据结构，是一块连续的内存的区域。这句话的意思是栈顶的地址和栈的最大容量是系统预先规定好的，在WINDOWS下，栈的大小是2M（或1M，总之是一个编译时就确定的常数），如果申请的空间超过栈的剩余空间时，将提示overflow。因此，能从栈获得的空间较小。   </li>
<li>堆：<strong>堆是向高地址扩展</strong>的数据结构，是不连续的内存区域。这是由于系统是用链表来存储的空闲内存地址的，自然是不连续的，而链表的遍历方向是由低地址向高地址。堆的大小受限于计算机系统中有效的虚拟内存。由此可见，堆获得的空间比较灵活，也比较大。   </li>
</ul>
<h4 id="申请效率的比较："><a href="#申请效率的比较：" class="headerlink" title="申请效率的比较："></a>申请效率的比较：</h4><ul>
<li>栈：机器系统提供的数据结构，计算机会在底层对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高。</li>
<li>堆：则是C/C++函数库提供的，它的机制是很复杂的，例如为了分配一块内存，库函数会按照一定的算法（具体的算法可以参考数据结构/操作系统）在堆内存中搜索可用的足够大小的空间，如果没有足够大小的空间（可能是由于内存碎片太多），就有可能调用系统功能去增加程序数据段的内存空间，这样就有机会分到足够大小的内存，然后进行返回。显然，堆的效率比栈要低得多。</li>
</ul>
<h4 id="堆和栈中的存储内容"><a href="#堆和栈中的存储内容" class="headerlink" title="堆和栈中的存储内容"></a>堆和栈中的存储内容</h4><ul>
<li>栈：在函数调用时，第一个进栈的是主函数中后的下一条指令（函数调用语句的下一条可执行语句）的地址，然后是函数的各个参数，在大多数的C编译器中，参数是由右往左入栈的，然后是函数中的局部变量。注意静态变量是不入栈的。当本次函数调用结束后，局部变量先出栈，然后是参数，最后栈顶指针指向开始存的地址，也就是主函数中的下一条指令，程序由该点继续运行。   </li>
<li>堆：一般是在堆的头部用一个字节存放堆的大小。堆中的具体内容由程序员安排。   </li>
</ul>
<h2 id="函数传递方式"><a href="#函数传递方式" class="headerlink" title="函数传递方式"></a>函数传递方式</h2><ul>
<li>值传递：调用了一个复制构造函数，相当于创建了一个拷贝，会造成更多的内存占用</li>
<li>引用传递：传递的是原参数的别名。节省内存，可对实参进行修改。</li>
<li><p>指针传递：形参是指向实参的指针，对形参指针的操作，会影响到实参。</p>
</li>
<li><p>指针传递和引用传递一般适用于：</p>
<ul>
<li>函数内部修改参数并且希望改动影响调用者。</li>
<li>当一个函数实际需要返回多个值，而只能显式返回一个值时，可以将另外需要返回的变量以指针/引用传递。</li>
</ul>
</li>
</ul>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul>
<li>引用为别名，不能有null引用</li>
<li>引用在创建的同时必须对其初始化</li>
<li>引用在初始化后，无法改变引用关系</li>
</ul>
<h3 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h3><ul>
<li>本质上还是值传递，传递的是地址</li>
</ul>
<p>&lt;未完&gt;</p>
]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>C++-自建数组及STL中的array&amp;vector</title>
    <url>/2021/12/10/C++-%E8%87%AA%E5%BB%BA%E6%95%B0%E7%BB%84%E5%8F%8ASTL%E4%B8%AD%E7%9A%84array&amp;vector/</url>
    <content><![CDATA[<h2 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h2><ul>
<li>三者皆使用连续内存，且array和vector的底层由自建数组实现</li>
<li>都可以用下标访问</li>
</ul>
<a id="more"></a>
<h2 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h2><ol>
<li>vector属于变长容器，即可以根据数据的插入删除重新构建容器容量；但array和数组属于定长容器。</li>
<li>vector提供了可以动态插入和删除元素的机制，而array和数组则无法做到，或者说array和数组需要完成该功能则需要自己实现完成。</li>
<li>由于vector的动态内存变化的机制，<strong>在插入和删除时，需要考虑迭代的是否失效的问题</strong>，vector容器增长时，不是在原有内存空间中添加，而是重新申请+分配内存。</li>
<li>vector和array提供了更好的数据访问机制，即可以使用front和back以及at访问方式，使得访问更加安全。而数组只能通过下标访问，在程序的设计过程中，更容易引发访问错误。</li>
<li>vector和array提供了更好的遍历机制，有正向迭代器和反向迭代器两种</li>
<li>vector和array提供了size和判空的获取机制，而数组只能通过遍历或者通过额外的变量记录数组的size。</li>
<li>vector和array提供了两个容器对象的内容交换，即swap的机制，而数组对于交换只能通过遍历的方式，逐个元素交换的方式使用</li>
<li>array提供了初始化所有成员的方法fill</li>
</ol>
<h2 id="自建数组"><a href="#自建数组" class="headerlink" title="自建数组"></a>自建数组</h2><h3 id="静态数组-amp-动态数组"><a href="#静态数组-amp-动态数组" class="headerlink" title="静态数组&amp;动态数组"></a>静态数组&amp;动态数组</h3><h4 id="静态数组"><a href="#静态数组" class="headerlink" title="静态数组"></a>静态数组</h4><ul>
<li><strong>在栈中</strong></li>
<li>在编译期间在栈中分配好内存的数组，在运行期间不能改变存储空间，运行后由<strong>系统自动释放</strong>。</li>
<li>必须用常量指定数组大小</li>
<li>数组名为第一个数组元素</li>
</ul>
<h4 id="动态数组"><a href="#动态数组" class="headerlink" title="动态数组"></a>动态数组</h4><ul>
<li>程序运行时才分配内存的数组</li>
<li><strong>在堆中</strong></li>
<li>数组名为指向第一个数组元素的指针</li>
</ul>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><ul>
<li>int a[3]; 局部作用域中，只做了声明，未初始化，其值未定</li>
<li>int a[3]; 静态数组以及直接声明在某个namespace中（函数外部），默认初始化为全0</li>
<li>int a[3] = {}; 初始化为0</li>
<li>int a[3] = {1}; 初始化为{1, 0, 0}</li>
<li>int a[] = {1,2,3,4}; 声明大小为4的数组，并初始化为{1,2,3,4}</li>
<li>int a[] {1,2,3,4}; <strong>通用初始化</strong>方法，声明和初始化程序之间不用等号</li>
<li>int* a = new int[10]; //new 分配了一个大小为10的未初始化的int数组，并返回指向该数组第一个元素的指针，此指针初始化了指针a<ul>
<li>a = {4, -3, 5, -2, -1, 2, 6, -2, 3}; // 错误，注意这种用大括号的数组赋值只能用在声明时，此处已经不是声明，所以出错。</li>
<li>int *a = new int[10] ();  // 默认初始化，每个元素初始化为0,括号内不能写其他值，只能初始化为0</li>
<li>int* a = new int[n];// t</li>
<li>string* Dynamic_Arr4 = new string[size]{“aa”, “bb”,”cc”, “dd”, string(2, ‘e’) };      //显式的初始化</li>
<li>delete [ ] Dynamic_Arr4；//动态数组的释放</li>
</ul>
</li>
<li>维度为变量时，不能在声明时同时初始化，即int a[b]={1};是不合法的。 </li>
<li>多维数组为1维数组的特殊形式，定义多维数组时，如果不对它进行初始化，必须标明每个维度的大小；如果进行了显式的初始化，可以不标明最高维度的大小，（也就是第一个维度，当第一个维度不标明大小，则不需进行初始化）</li>
</ul>
<h2 id="array"><a href="#array" class="headerlink" title="array"></a>array</h2><ul>
<li><a href="https://www.cplusplus.com/reference/array/array/">ref</a></li>
<li>大小固定</li>
</ul>
<h2 id="vector"><a href="#vector" class="headerlink" title="vector"></a>vector</h2><ul>
<li><a href="https://www.cplusplus.com/reference/vector/vector/">ref</a></li>
<li>动态数组</li>
</ul>
<h3 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化"></a>初始化</h3><ul>
<li>构造函数<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>():创建一个空<span class="built_in">vector</span></span><br><span class="line"><span class="built_in">vector</span>(<span class="keyword">int</span> nSize):创建一个<span class="built_in">vector</span>,元素个数为nSize</span><br><span class="line"><span class="built_in">vector</span>(<span class="keyword">int</span> nSize,<span class="keyword">const</span> t&amp; t):创建一个<span class="built_in">vector</span>，元素个数为nSize,且值均为t</span><br><span class="line"><span class="built_in">vector</span>(<span class="keyword">const</span> <span class="built_in">vector</span>&amp;):复制构造函数</span><br><span class="line"><span class="built_in">vector</span>(begin,end):复制[begin,end)区间内另一个数组的元素到<span class="built_in">vector</span>中</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; first;                                <span class="comment">// empty vector of ints</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">second</span> <span class="params">(<span class="number">4</span>,<span class="number">100</span>)</span></span>;                       <span class="comment">// four ints with value 100</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">third</span> <span class="params">(second.begin(),second.end())</span></span>;  <span class="comment">// iterating through second</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">fourth</span> <span class="params">(third)</span></span>;                       <span class="comment">// a copy of third</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// the iterator constructor can also be used to construct from arrays:</span></span><br><span class="line"><span class="keyword">int</span> myints[] = &#123;<span class="number">16</span>,<span class="number">2</span>,<span class="number">77</span>,<span class="number">29</span>&#125;;</span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">fifth</span> <span class="params">(myints, myints + <span class="keyword">sizeof</span>(myints) / <span class="keyword">sizeof</span>(<span class="keyword">int</span>) )</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>二维vector若要使用A[i].push_back(3)等方式初始化，则应事先定义好第一维的大小，例如<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">ans</span><span class="params">(<span class="number">5</span>)</span></span>;</span><br><span class="line">ans[<span class="number">3</span>].push_back(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="增加"><a href="#增加" class="headerlink" title="增加"></a>增加</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">void push_back(const T&amp; x):向量尾部增加一个元素X</span><br><span class="line">iterator insert(iterator it,const T&amp; x):向量中迭代器指向元素前增加一个元素x</span><br><span class="line">iterator insert(iterator it,int n,const T&amp; x):向量中迭代器指向元素前增加n个相同的元素x</span><br><span class="line">iterator insert(iterator it,const_iterator first,const_iterator last):向量中迭代器指向元素前插入另一个相同类型向量的[first,last)间的数据</span><br></pre></td></tr></table></figure>
<h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">iterator erase(iterator it):删除向量中迭代器指向元素</span><br><span class="line">iterator erase(iterator first,iterator last):删除向量中[first,last)中元素</span><br><span class="line">void pop_back():删除向量中最后一个元素</span><br><span class="line">void clear():清空向量中所有元素</span><br></pre></td></tr></table></figure>
<h3 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">reference at(int pos):返回pos位置元素的引用</span><br><span class="line">reference front():返回首元素的引用</span><br><span class="line">reference back():返回尾元素的引用</span><br><span class="line">iterator begin():返回向量头指针，指向第一个元素</span><br><span class="line">iterator end():返回向量尾指针，指向向量最后一个元素的下一个位置</span><br><span class="line">reverse_iterator rbegin():反向迭代器，指向最后一个元素</span><br><span class="line">reverse_iterator rend():反向迭代器，指向第一个元素之前的位置</span><br></pre></td></tr></table></figure>
<h3 id="判断"><a href="#判断" class="headerlink" title="判断"></a>判断</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">bool empty() const:判断向量是否为空，若为空，则向量中无元素</span><br></pre></td></tr></table></figure>
<h3 id="大小"><a href="#大小" class="headerlink" title="大小"></a>大小</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">int size() const:返回向量中元素的个数</span><br><span class="line">int capacity() const:返回当前向量所能容纳的最大元素值</span><br><span class="line">int max_size() const:返回最大可允许的vector元素数量值</span><br></pre></td></tr></table></figure>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">void swap(vector&amp;):交换两个同类型向量的数据</span><br><span class="line">void assign(int n,const T&amp; x):设置向量中第n个元素的值为x</span><br><span class="line">void assign(const_iterator first,const_iterator last):向量中[first,last)中元素设置成当前向量元素</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>CV-CNN</title>
    <url>/2021/12/10/CNN/</url>
    <content><![CDATA[<p>[TOC]</p>
<p><a href="https://zhuanlan.zhihu.com/p/27642620">原贴</a></p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><ul>
<li>卷积神经网络大致就是covolutional layer, pooling layer, ReLu layer, fully-connected layer的组合，例如下图所示的结构。<br><img src="https://pic4.zhimg.com/80/v2-cf87890eb8f2358f23a1ac78eb764257_hd.png" alt="ex-1"></li>
</ul>
<a id="more"></a>
<h3 id="图片的识别"><a href="#图片的识别" class="headerlink" title="图片的识别"></a>图片的识别</h3><ul>
<li>生物所看到的景象并非世界的原貌，而是长期进化出来的适合自己生存环境的一种感知方式</li>
<li>画面识别实际上是寻找/学习动物的视觉关联形式（即将能量与视觉关联在一起的方式）</li>
<li>画面的识别取决于：<ul>
<li>图片本身</li>
<li>被如何观察</li>
</ul>
</li>
<li>图像不变性：<ul>
<li>rotation</li>
<li>viewpoint</li>
<li>size</li>
<li>illumination</li>
<li>…<h3 id="前馈的不足"><a href="#前馈的不足" class="headerlink" title="前馈的不足"></a>前馈的不足</h3></li>
</ul>
</li>
<li>当出现上述variance时，前馈无法做到适应，即前馈只能对同样的内容进行识别，若出现其他情况时，只能增加样本重新训练</li>
<li>解决方法可以是让图片中不同的位置有相同的权重——<strong>共享权重</strong><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><h4 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h4></li>
<li><strong>空间共享</strong>（引入的先验知识）</li>
<li><strong>局部连接</strong>（得到的下一层节点与该层并非全连接）</li>
<li>depth上是<strong>全连接</strong>的<blockquote>
<p>每个filter会在width维, height维上，以局部连接和空间共享，并贯串整个depth维的方式得到一个Feature Map。</p>
</blockquote>
</li>
</ul>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p><img src="https://pic3.zhimg.com/80/v2-23db15ec3f783bbb5cf811711e46dbba_hd.png" alt="cnn_example"></p>
<ul>
<li>在输入depth为1时：被filter size为2x2所圈中的4个输入节点连接到1个输出节点上。</li>
<li>在输入depth为3时：被filter size为2x2，但是贯串3个channels后，所圈中的12个输入节点连接到1个输出节点上。</li>
<li>在输入depth为n时：2x2xn个输入节点连接到1个输出节点上。<blockquote>
<p>三个channels的权重并不共享。 即当深度变为3后，权重也跟着扩增到了三组。</p>
</blockquote>
</li>
</ul>
<h5 id="zero-padding"><a href="#zero-padding" class="headerlink" title="zero padding"></a>zero padding</h5><p>有时为了保证feature map与输入层保持同样大小，会添加zero padding，一般3*3的卷积核padding为1，5*5为2</p>
<p>Feature Map的尺寸等于(input_size + 2 *padding_size − filter_size)/stride+1</p>
<h4 id="形状、概念抓取"><a href="#形状、概念抓取" class="headerlink" title="形状、概念抓取"></a>形状、概念抓取</h4><ul>
<li>卷积层可以对基础形状（包括边缘、棱角、模糊等）、对比度、颜色等概念进行抓取</li>
<li>可以通过多层卷积实现对一个较大区域的抓取</li>
<li>抓取的特征取决于卷积核的权重，而此权重由网络根据数据学习得到，即CNN会自己学习以什么样的方式观察图片</li>
<li>可以有多个filter，从而可以学习到多种特征<ul>
<li>此时卷积层的输出depth也就不是1了</li>
<li>卷积层的输入输出均为长方体：其中depth与filters个数相同<br><img src="https://pic1.zhimg.com/80/v2-a9983c3cee935b68c73965bc1abe268c_hd.png" alt="ex4"><br><img src="https://pic1.zhimg.com/80/v2-d11e1d2f2c41b6df713573f8155bc324_hd.png" alt="ex2"><h4 id="非线性（以ReLu为例）"><a href="#非线性（以ReLu为例）" class="headerlink" title="非线性（以ReLu为例）"></a>非线性（以ReLu为例）</h4>增强模型的非线性拟合能力<br><img src="https://pic3.zhimg.com/80/v2-54a469b2873542e75abf2bc5d8fcaa1a_hd.png" alt="ex3"><h4 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h4><em>比如以步长为2，2x2的 filter pool</em><br><img src="https://pic4.zhimg.com/80/v2-cd717414dcf32dac4df73c00f1e7c6c3_hd.jpg" alt="ex5"></li>
</ul>
</li>
<li>pooling的主要功能是downsamping，有助减少conv过程中的冗余<h4 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h4></li>
<li>当抓取到足以用来识别图片的特征后，接下来的就是如何进行分类。 全连接层（也叫前馈层）就可以用来将最后的输出映射到线性可分的空间。 通常卷积网络的最后会将末端得到的长方体平摊(flatten)成一个长长的向量，并送入全连接层配合输出层进行分类。<h4 id="一些变体中用到的技巧"><a href="#一些变体中用到的技巧" class="headerlink" title="一些变体中用到的技巧"></a>一些变体中用到的技巧</h4></li>
<li>1x1卷积核：选择不同的个数，用来降维或升维</li>
<li>残差<blockquote>
<p>所有的这些技巧都是对各种不变性的满足</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
        <category>CV</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>CV</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec1-Graph Intro &amp; Graph Representations</title>
    <url>/2021/12/12/CS224WLec1/</url>
    <content><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><ul>
<li>title: Machine Learning with Graphs</li>
<li>year: Fall 2021</li>
<li><a href="http://web.stanford.edu/class/cs224w/">CS224W page</a></li>
<li><a href="https://snap-stanford.github.io/cs224w-notes/">note</a>、<a href="https://yasoz.github.io/cs224w-zh/#/Introduction-and-Graph-Structure">note中文翻译</a><a id="more"></a></li>
<li>professor：<a href="https://profiles.stanford.edu/jure-leskovec">Jure Leskovec</a></li>
<li>videos: <a href="https://www.bilibili.com/video/BV1RZ4y1c7Co/?spm_id_from=333.788.recommend_more_video.0">bilibili</a></li>
<li>labs: <a href="https://docs.google.com/document/d/e/2PACX-1vRMprg-Uz9oEnjXOJpRPJ5oyEXRnJAz9qVeEB04sucx2o2RtQ-HRfom6IWS5ONhfoly0TOmJM7BxIzJ/pub">lab*5</a></li>
<li>schedule: <a href="http://web.stanford.edu/class/cs224w/index.html#schedule">schedule</a></li>
<li>goal: graph的表征学习和用于graph的机器学习算法</li>
<li>topics<ul>
<li>Traditional methods: Graphlets, Graph Kernels</li>
<li>Methods for node embeddings: DeepWalk, Node2Vec</li>
<li>Graph Neural Networks: GCN, GraphSAGE, GAT, Theory of GNNs</li>
<li>Knowledge graphs and reasoning: TransE, BetaE</li>
<li>Deep generative models for graphs: GraphRNN</li>
<li>Applications to Biomedicine, Science, Industry</li>
</ul>
</li>
</ul>
<h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><ul>
<li>graph level: graph classification,例如分子属性预测</li>
<li>node level: node classification, 比如用户/商品分类</li>
<li>edge level: link prediction，例如knowledge graph completioni、推荐系统、药物副作用预测</li>
<li>community(subgraph) level: clustering，比如social circle detections</li>
<li>others<ul>
<li>graph generation</li>
<li>graph evolution</li>
<li>…</li>
</ul>
</li>
</ul>
<h1 id="Representation"><a href="#Representation" class="headerlink" title="Representation"></a>Representation</h1><p>G=(V, E, R, T)</p>
<ul>
<li>node: V</li>
<li>edge: E</li>
<li>relation type: R</li>
<li>node type: T</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li>directed &amp; undirected</li>
<li>node degree<ul>
<li>avg degree: <script type="math/tex; mode=display">
\bar{k}=\langle k\rangle=\frac{1}{N} \sum_{i=1}^{N} k_{i}=\frac{2 E}{N}</script></li>
<li>in-degree &amp; out-degree</li>
</ul>
</li>
<li>bipartite graph二部图: 包含两种不同的node，node只和另外一部的node连接。可以通过投影的方式转化为folded/projected bipartite graphs</li>
</ul>
<h2 id="表示"><a href="#表示" class="headerlink" title="表示"></a>表示</h2><ul>
<li>Adjacency Matrix：无向图时为对称矩阵。<strong>graph大多数时为高度稀疏矩阵（degree远小于节点数），邻接矩阵会造成内存浪费</strong></li>
<li>Adjacency List: 对每一个节点存储其neighbors</li>
<li>图的附加属性：weight/ranking/type/sign/…</li>
</ul>
]]></content>
      <categories>
        <category>Lecture</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec2-Traditional Methods for ML on Graphs</title>
    <url>/2021/12/12/CS224WLec2/</url>
    <content><![CDATA[<p><a href="http://web.stanford.edu/class/cs224w/slides/02-tradition-ml.pdf">ppt</a></p>
<h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h1><p>design features for nodes/link/graphs —&gt; obtain features for all data —&gt; train an ML model —&gt; apply the model<br><a id="more"></a></p>
<h1 id="Feature-design"><a href="#Feature-design" class="headerlink" title="Feature design"></a>Feature design</h1><ul>
<li><em>focus on undirected graphs</em></li>
</ul>
<h2 id="Node-Features"><a href="#Node-Features" class="headerlink" title="Node Features"></a>Node Features</h2><h3 id="centrality中心性"><a href="#centrality中心性" class="headerlink" title="centrality中心性"></a>centrality中心性</h3><ul>
<li><strong>Degree counts #(edges) that a node touches</strong></li>
<li>degree只考虑neibors数量，不考虑neibor的不同重要性</li>
</ul>
<h4 id="Eigenvector-centrality"><a href="#Eigenvector-centrality" class="headerlink" title="Eigenvector centrality"></a>Eigenvector centrality</h4><ul>
<li>定义：节点的重要性由邻居节点的重要性决定。节点v的centrality是邻居centrality的加总，N(v)为v的neibors集合<script type="math/tex; mode=display">
c_{v}=\frac{1}{\lambda} \sum_{u \in N(v)} c_{u}</script></li>
</ul>
<p>可以将其写为矩阵形式，得到 $ \lambda c=A c $ ，A为邻接矩阵，$ \lambda{max} $ 总为正且唯一，因此可以将其对应的$ c_{max} $作为eigenvector</p>
<h4 id="betweenness-centrality"><a href="#betweenness-centrality" class="headerlink" title="betweenness centrality"></a>betweenness centrality</h4><ul>
<li>如果一个节点处在很多节点对的最短路径上，那么这个节点是重要的<script type="math/tex; mode=display">
c_{v}=\sum_{s \neq v \neq t} \frac{\#(\text { shortest paths betwen } s \text { and } t \text { that contain } v)}{\#(\text { shortest paths between } s \text { and } t)}</script></li>
</ul>
<h4 id="closeness-centrality"><a href="#closeness-centrality" class="headerlink" title="closeness centrality"></a>closeness centrality</h4><ul>
<li>一个节点距其他节点之间距离最短，那么认为这个节点是重要的。分母：该节点与其他节点的最短距离之和。<script type="math/tex; mode=display">
c_{v}=\frac{1}{\sum_{u \neq v} \text { shortest path length between } u \text { and } v}</script></li>
</ul>
<h3 id="clustering-coefficient"><a href="#clustering-coefficient" class="headerlink" title="clustering coefficient"></a>clustering coefficient</h3><ul>
<li><strong>Clustering coefficient counts #(triangles) that a node touches.</strong></li>
<li>节点的<strong>neighbors</strong>两两连接的情况 —》neighbor总跟该节点连接着，neibor两两连接就能构成三角形 —》<strong>反映了该节点和其neighbors是否能聚为一类的情况</strong><script type="math/tex; mode=display">
e_{v}=\frac{\#(\text { edges among neighboring nodes })}{\left(\begin{array}{c}
k_{v} \\
2
\end{array}\right)} \in[0,1]</script></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/2021052810445748.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BvbGFyaXNSaXNpbmdXYXI=,size_16,color_FFFFFF,t_70" alt=""></p>
<p><img src="https://img-blog.csdnimg.cn/20210528105114750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BvbGFyaXNSaXNpbmdXYXI=,size_16,color_FFFFFF,t_70" alt=""></p>
<h3 id="Graphlets"><a href="#Graphlets" class="headerlink" title="Graphlets"></a>Graphlets</h3><ul>
<li>Rooted connected induced non-isomorphic subgraphs, 有根连通异构子图<ul>
<li>graphlet为给定图的子图，需要满足<strong>四个条件</strong><ul>
<li>1.rooted: 同一个结构，指定的根节点不同，属于不同的结构</li>
<li>2.connected: 连通图</li>
<li>3.induced subgraphs: 是induced得到的subgraphs, 即该子图包括的nodes在原图中的所有link都应该包括在子图中</li>
<li>4.non-isomorphic: 异构图</li>
<li>对3的解释：</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/2021/12/12/CS224WLec2/induced_subgraph.png" class="" title="induced_subgraph">
<ul>
<li>不给定图时，<strong>节点数为2-5情况下一共能产生如图所示73种graphlet。这73个graphlet的核心概念就是不同的形状，不同的位置。</strong>图中标的数字代表graphlet的id（根节点可选的位置）。例如对于$ G_0 $，两个节点是等价的（对称的），所以只有一种graphlet；对于$ G_1 $，根节点有在中间和在边上两种选择，上下两个边上的点是等价的，所以只有两种graphlet。其他的类似。<br><img src="https://img-blog.csdnimg.cn/20210528121841575.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BvbGFyaXNSaXNpbmdXYXI=,size_16,color_FFFFFF,t_70" alt=""><ul>
<li>Graphlet Degree Vector (GDV): A count vector of graphslets rooted at a given node</li>
</ul>
</li>
</ul>
<ul>
<li><strong>GDV counts #(graphlets) that a node touches</strong><br><img src="https://img-blog.csdnimg.cn/20210528123912356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1BvbGFyaXNSaXNpbmdXYXI=,size_16,color_FFFFFF,t_70" alt=""></li>
<li>以v作为节点的有根连通异构子图共4个，分别为a b c d为节点的子图。a有两种情况；b一种情况；c不存在是因为graphlet是induced subgraph，c可以induce为b；d有两种情况。所以得到的GDV为[2, 1, 0, 2]</li>
<li>考虑2-5个节点的graphlets，我们得到一个长度为73个坐标coordinate（就前图所示一共73种graphlet）的向量GDV，描述该点的局部拓扑结构topology of node’s neighborhood，可以捕获距离为4 hops的互联性interconnectivities</li>
<li>相比节点度数或clustering coefficient，GDV能够描述两个节点之间更详细的节点局部拓扑结构相似性local topological similarity。</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>节点的重要性importance-based: node degree/centrality</li>
<li>节点邻域的拓扑结构structure-based: node degree/clustering cofficient/graphlet count vector</li>
</ul>
<h2 id="Link-features-amp-Link-level-prediction"><a href="#Link-features-amp-Link-level-prediction" class="headerlink" title="Link features &amp; Link-level prediction"></a>Link features &amp; Link-level prediction</h2><ul>
<li>diagram: predict <strong>new links</strong> based on the existing links. At test time, predict ranked <strong>top k</strong> node pairs</li>
<li>所以需要为<strong>pair of nodes</strong>设计特征</li>
</ul>
<h3 id="两类任务"><a href="#两类任务" class="headerlink" title="两类任务"></a>两类任务</h3><ul>
<li>predict缺失的links</li>
<li>给定t0时刻的links，预测t1时刻的links</li>
</ul>
<h3 id="Distance-based-features"><a href="#Distance-based-features" class="headerlink" title="Distance-based features"></a>Distance-based features</h3><ul>
<li>一个pair of node的最短距离、最长距离等</li>
<li><strong>无法得知其邻域的overlap</strong></li>
</ul>
<h3 id="Local-neighborhood-overlap"><a href="#Local-neighborhood-overlap" class="headerlink" title="Local neighborhood overlap"></a>Local neighborhood overlap</h3><ul>
<li>某两个节点其邻域的overlap -》<strong>两节点邻域没有交集其overlap为0</strong></li>
<li>1.common neighbors: <strong>求交集</strong>, <script type="math/tex">\left|N\left(v_{1}\right) \cap N\left(v_{2}\right)\right|</script></li>
<li>2.Jaccard’s coefficient: <strong>IoU</strong> <script type="math/tex">\frac{\left|N\left(v_{1}\right) \cap N\left(v_{2}\right)\right|}{\left|N\left(v_{1}\right) \cup N\left(v_{2}\right)\right|}</script></li>
<li>3.Adamic-Adar index: <strong>两节点交集的点的1/log(degree)之和</strong> <script type="math/tex">\sum_{u \in N\left(v_{1}\right) \cap N\left(v_{2}\right)} \frac{1}{\log \left(k_{u}\right)}</script></li>
</ul>
<h3 id="Global-neighborhood-overlap"><a href="#Global-neighborhood-overlap" class="headerlink" title="Global neighborhood overlap"></a>Global neighborhood overlap</h3><ul>
<li><strong>使用整张图的结构来表示两节点的关系</strong></li>
<li>Katz index: 计算两个节点间距离为i时有多少条通路，并穷举i，求通路数之和</li>
<li>邻接矩阵A的k次幂可以表示距离为k时通路的数量：$A_{u v}^{l}$即距离为l的通路数量</li>
<li>Katz index: <script type="math/tex">S_{v_{1} v_{2}}=\sum_{l=1}^{\infty} \beta^{l} A_{v_{1} v_{2}}^{l}</script></li>
<li>闭环形式：<script type="math/tex; mode=display">\boldsymbol{S}=\sum_{i=1}^{\infty} \beta^{i} \boldsymbol{A}^{i}=\underbrace{(\boldsymbol{I}-\beta \boldsymbol{A})^{-1}}_{=\sum_{i=0}^{\infty} \beta^{i} A^{i}}-\boldsymbol{I}</script></li>
</ul>
<h2 id="Graph-features"><a href="#Graph-features" class="headerlink" title="Graph features"></a>Graph features</h2><p><strong>focus on the Graph kernels</strong></p>
<ul>
<li>图G和G’，其kernel为K(G, G’)，且可以写成：<script type="math/tex">K\left(G, G^{\prime}\right)=\boldsymbol{f}_{G}{ }^{\mathrm{T}} \boldsymbol{f}_{G^{\prime}}</script> ，其中G的特征向量为$ \boldsymbol{f}_{G} $，称这种形式的特征表达为Kernel features</li>
<li>需要满足：<ul>
<li>kernel K(G, G’)能够表达G和G‘的相似性</li>
<li>kernel matrix <script type="math/tex">\boldsymbol{K}=\left(K\left(G, G^{\prime}\right)\right)_{G, G^{\prime}}</script></li>
<li>存在一种特征表达$\phi(\cdot)$满足$K\left(G, G^{\prime}\right)=\phi(\mathrm{G})^{\mathrm{T}} \phi\left(G^{\prime}\right)$</li>
</ul>
</li>
<li>最简单的，kernel可以是图不同degree的node个数组成的vector</li>
</ul>
<h3 id="Graphlet-kernel"><a href="#Graphlet-kernel" class="headerlink" title="Graphlet kernel"></a>Graphlet kernel</h3><ul>
<li>与node level的不同：<ul>
<li>不要求连通性，点个数在继续，不需要彼此能联通</li>
<li>不要求rooted，异构即可</li>
</ul>
</li>
<li>具体的：<ul>
<li>先确定graphlet的node个数k</li>
<li>求k个node的graphlets list： <script type="math/tex">\mathcal{G}_{k} = \left(g_{1}, g_{2}, \ldots, g_{n_{k}}\right)</script></li>
<li>求graphlets list中每个Graphlet在图中出现了几次，并构成vector： <script type="math/tex">\left(\boldsymbol{f}_{G}\right)_{i}=\#\left(g_{i} \subseteq G\right)</script> for <script type="math/tex">i=1,2, \ldots, n_{k}</script></li>
</ul>
</li>
</ul>
<img src="/2021/12/12/CS224WLec2/grahlet_vectors.png" class="" title="graphlet_vector">
<ul>
<li>problem1: graph不一样，kernel的值会skewd<ul>
<li>solution: normalize<script type="math/tex; mode=display">
\boldsymbol{h}_{G}=\frac{\boldsymbol{f}_{G}}{\operatorname{Sum}\left(\boldsymbol{f}_{G}\right)} \quad K\left(G, G^{\prime}\right)=\boldsymbol{h}_{G}{ }^{\mathrm{T}} \boldsymbol{h}_{G^{\prime}}</script></li>
</ul>
</li>
<li>problem2: 对size n的graph求size k的graphlet需要$n^{k}$</li>
<li>problem3: 图的degree上界是d，则需要$O\left(n d^{k-1}\right)$</li>
<li>problem4: 最糟糕的情况是判断一个图是不是另一个图的子图是<a href="https://chenzk1.github.io/2021/12/15/P&amp;NP&amp;NP-hard/">np-hard问题</a>（不确定性多项式hard问题，不能确定能不能在n的多项式内的复杂度完成）<h3 id="Weisfeiler-Lehman-Kernel"><a href="#Weisfeiler-Lehman-Kernel" class="headerlink" title="Weisfeiler-Lehman Kernel"></a>Weisfeiler-Lehman Kernel</h3></li>
<li>color refinement方法</li>
<li>也需要定义一个k，含义为step <script type="math/tex">c^{(k+1)}(v)=\operatorname{HASH}\left(\left\{c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right\}\right)</script></li>
<li>每一个step都用到了上一个step的值，而第一个step中，每个节点v的c(v)与其邻域有关，所以<strong> 第k个step能够得到v的k-hop邻域 </strong></li>
<li>example, k=1, G1和G2的vector都是[6，2，1，2，1]<img src="/2021/12/12/CS224WLec2/wl_kernel1.png" class="" title="wl_kernel1">
<img src="/2021/12/12/CS224WLec2/wl_kernel2.png" class="" title="wl_kernel2"></li>
<li>时间复杂度：#(edges)的线性</li>
<li>本质：bag of colors，colors代表了n-hop(n=1…k)的邻域结构</li>
<li>与Graph neural network很类似。？</li>
</ul>
]]></content>
      <categories>
        <category>Lecture</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec3-Node Embeddings</title>
    <url>/2021/12/15/CS224WLec3/</url>
    <content><![CDATA[<h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><ul>
<li>goal: 为node生成一个embedding</li>
<li>两个要素：<strong>encoder/相似度计量方法（encode前后都需要）</strong></li>
<li>framework: encoder 生成embedding —&gt; 相似度计量方法决定embedding学习的好坏</li>
<li><strong>unsupervised/self-supervised way</strong> based on random walks</li>
<li>task independent</li>
</ul>
<a id="more"></a>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><ul>
<li>goal: 原始graph中相似的node获得的embeddings也是相似的</li>
<li>类似于word2vec: 目标是求node u的embedding <script type="math/tex">\mathbf{z}_{u}</script>,而模型的预测目标是：<script type="math/tex">P\left(v \mid \mathbf{z}_{u}\right)</script>，即node v出现在以node u开始的walk上的概率。</li>
<li>如何获得“句子”：random walk</li>
<li>范式: <ul>
<li>encoder生成node embedding，本节的encoder为word2vec中的权重矩阵: <script type="math/tex">\operatorname{ENC}(v)=\mathbf{z}_{v}</script></li>
<li>decoder将node embedding映射回原空间，这里存在隐式的decoder，embedding空间两向量的点积可以表示原空间u,v的相似度: <script type="math/tex">\operatorname{similarity}(u, v) \approx \mathbf{z}_{v}^{\mathrm{T}} \mathbf{z}_{u}</script><ul>
<li>点击相似度：最小化两向量的模以及夹角余弦的乘积</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Deep-Walk"><a href="#Deep-Walk" class="headerlink" title="Deep Walk"></a>Deep Walk</h2><h3 id="Random-Walk"><a href="#Random-Walk" class="headerlink" title="Random Walk"></a>Random Walk</h3><ul>
<li>出发点：如果一个random walk中包括从u到v的路径，那u和v是相似的/有相似的高维的多跳信息</li>
<li>本质：DFS</li>
<li>$ N_{\mathrm{R}}(u) $为策略R下，从u出发的walk中，出现的所有nodes<script type="math/tex; mode=display">\max _{f} \sum_{u \in V} \log \mathrm{P}\left(N_{\mathrm{R}}(u) \mid \mathbf{z}_{u}\right)</script>—》<script type="math/tex; mode=display">\mathcal{L}=\sum_{u \in V} \sum_{v \in N_{R}(u)}-\log \left(P\left(v \mid \mathbf{z}_{u}\right)\right)</script></li>
<li>利用softmax求p<script type="math/tex; mode=display">P\left(v \mid \mathbf{z}_{u}\right)=\frac{\exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)}{\sum_{n \in V} \exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n}\right)}</script></li>
<li>problem: softmax分母 以及 最外层都需要|V|次遍历 —》<script type="math/tex">\mathrm{O}\left(|\mathrm{V}|^{2}\right)</script>的复杂度 —》<strong>优化</strong></li>
</ul>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><ul>
<li>使用所有样本做normalization —&gt; 只采样k个负样本做normalization<script type="math/tex; mode=display">P\left(v \mid \mathbf{z}_{u}\right)=\frac{\exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)}{\sum_{n \in V } \exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n}\right)} \approx \log \left(\sigma\left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)\right)-\sum_{i=1}^{k} \log \left(\sigma\left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n_{i}}\right)\right), n_{i} \sim P_{V}</script></li>
<li>k的选择：<ul>
<li>k越大，模型越鲁棒</li>
<li>k越大，对负样本考虑的越多</li>
<li>5~20间较常见</li>
</ul>
</li>
<li>负样本的选择：可以选择graph内任意样本，但更准确的方法是选择不在walk中的样本</li>
</ul>
<h2 id="Node2vec-better-random-walk-strategy"><a href="#Node2vec-better-random-walk-strategy" class="headerlink" title="Node2vec: better  random walk strategy"></a>Node2vec: better  random walk strategy</h2><ul>
<li>简单的random walk会限制walk中的node相似度与graph中node相似度的一致性</li>
</ul>
<h3 id="Biased-2nd-order-random-Walks"><a href="#Biased-2nd-order-random-Walks" class="headerlink" title="Biased 2nd-order random Walks"></a>Biased 2nd-order random Walks</h3><ul>
<li>trade off between local and global views of the network: BFS &amp; DFS</li>
<li>当前在w, 上一步在s的walk，有三种行走方向<ul>
<li>退后：回退到s</li>
<li>保持：走到和s距离一致的一个节点</li>
<li>前进：走到距离s更远的节点<img src="/2021/12/15/CS224WLec3/node2vec1.png" class="" title="node2vec1"></li>
</ul>
</li>
<li>实现：两个<strong>超参</strong>p/q，以及“1”来以非归一化的方法表示上述三种情况的概率<img src="/2021/12/15/CS224WLec3/node2vec2.png" class="" title="node2vec2"></li>
<li>流程<ul>
<li>Compute random walk probabilities</li>
<li>Simulate 𝑟 random walks of length 𝑙 starting from each node 𝑢</li>
<li>Optimize the node2vec objective using Stochastic Gradient Descent</li>
</ul>
</li>
<li>Linear-time complexity</li>
<li>All 3 steps are individually parallelizable</li>
</ul>
<h2 id="Embedding-entire-graphs"><a href="#Embedding-entire-graphs" class="headerlink" title="Embedding entire graphs"></a>Embedding entire graphs</h2><ul>
<li>approach1: add all node embeddings</li>
<li>approach2: introduce a “virtual node” or “super node” to represent the graph and learning embedding for this graph</li>
<li>approach3: anonymous walks embeddings</li>
</ul>
<h3 id="Anonymous-walk-embeddings"><a href="#Anonymous-walk-embeddings" class="headerlink" title="Anonymous walk embeddings"></a>Anonymous walk embeddings</h3><ul>
<li>Anonymous walk: random walk —&gt; 将node表示为距离start node的去重index。因此，确定了walk length的时候，就确定了anonymous walk中index的个数。</li>
<li>方法1：长度为l的annoy walk共有n种情况 —&gt; 做m次random walks —&gt; 统计每种情况的count，并形成一个vector</li>
<li>方法2：用Anonymous walks的概率分布，学习图的embedding<ul>
<li>Learn to predict walks that co-occur in 𝚫-size window (e.g., predict 𝑤3 given 𝑤1, 𝑤2 if Δ = 2)</li>
<li>objective:<script type="math/tex; mode=display">\max _{z_{G}} \sum_{t=\Delta+1}^{T} \log P\left(w_{t} \mid w_{t-\Delta}, \ldots, w_{t-1}, \mathbf{z}_{G}\right)</script></li>
</ul>
</li>
</ul>
<img src="/2021/12/15/CS224WLec3/annoywalks1.png" class="" title="annoywalks1">
<img src="/2021/12/15/CS224WLec3/annoywalks2.png" class="" title="annoywalks2">
<h2 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h2><ul>
<li>属于shallow encoding，有如下优缺点：<ul>
<li>需要O(|V|)的参数量，节点间的embedding不共享，每个node有独立的embedding</li>
<li>training时没有的node，不会有embedding</li>
<li>没有利用到节点的特征，只利用了graph structure</li>
</ul>
</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="http://web.stanford.edu/class/cs224w/slides/03-nodeemb.pdf">ppt</a></li>
</ul>
]]></content>
      <categories>
        <category>Lecture</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
        <tag>DeepWalk</tag>
        <tag>Node2vec</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec4-PageRank</title>
    <url>/2021/12/18/CS224WLec4/</url>
    <content><![CDATA[<h1 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h1><ul>
<li>一个node的重要性由指向它的节点的重要性决定 —&gt; <strong>指向该node的节点数越多，且这些节点重要性越高，则该节点越重要</strong>。node j的重要性（传递函数）为：<script type="math/tex; mode=display">r_{j}=\sum_{i \rightarrow j} \frac{r_{i}}{d_{i}}</script><script type="math/tex; mode=display">d_{i} \ldots out-degree of node i</script></li>
<li><p>矩阵形式：</p>
<ul>
<li>随机邻接矩阵M。j有<script type="math/tex">d_{j}</script>个出链，如果j -&gt; i，则<script type="math/tex">M_{ij}=1/d_{j}</script>，因此每一列上的值要么为0，要么为<script type="math/tex">1/d_{j}</script>，且加和为1，称为列随机矩阵</li>
<li>rank vector r: <script type="math/tex">r_{j}</script>为node j的重要性score，且<script type="math/tex">\sum_{i} r_{i}=1</script></li>
<li>n*1 = n*n * n*1:<br><script type="math/tex">\boldsymbol{r}=\boldsymbol{M} \cdot \boldsymbol{r}</script>,</li>
</ul>
<script type="math/tex; mode=display">\quad r_{j}=\sum_{i \rightarrow j} \frac{r_{i}}{d_{i}}</script></li>
<li>PageRank VS RandomWalk: 当random walk到达静态分布状态时满足<script type="math/tex">\boldsymbol{r}=\boldsymbol{M} \cdot \boldsymbol{r}</script>，即<strong>PageRank得到的重要性向量v是random walk的静态分布</strong></li>
<li>PangRank VS Eigenvector: <strong>PageRank得到的重要性向量v是当特征值为1时得到的主特征向量</strong><a id="more"></a>
</li>
</ul>
<h2 id="Solve-the-equation"><a href="#Solve-the-equation" class="headerlink" title="Solve the equation"></a>Solve the equation</h2><ul>
<li>method: power iteration<ul>
<li>initialize: 初始化，给每个node一个page rank值。e.g. <script type="math/tex; mode=display">
\boldsymbol{r}^{0}=[1 / N, \ldots, 1 / N]^{T}</script></li>
<li>iterate: 迭代<script type="math/tex; mode=display">
r_{j}^{(t+1)}=\sum_{i \rightarrow j} \frac{r_{i}^{(t)}}{d_{i}}</script>即：<script type="math/tex; mode=display">
\boldsymbol{r}^{(t+1)}=\boldsymbol{M} \cdot \boldsymbol{r}^{t}</script></li>
<li>stop: 也可以选择其他度量方式<script type="math/tex; mode=display">
\left|\boldsymbol{r}^{(t+1)}-\boldsymbol{r}^{t}\right|_{1}<\varepsilon</script></li>
</ul>
</li>
</ul>
<h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h3><ul>
<li>dead ends: 遇到死胡同 —》 某个节点所在列的M为全0 —》随机矩阵不再随机，秩 &lt; n —》pagerank会收敛为全0<ul>
<li>solution: 填充M中的该列，例如填充为1/n</li>
</ul>
</li>
<li>spider trap: 在某个节点处死循环<ul>
<li>solution: 允许random surfer跳到一个随机page。加一个超参β，表示是否沿着这条link走的概率，1-β表示随机jump的概率 —》相当于改了M</li>
</ul>
</li>
<li>Google solution: teleport rankpage。β表示page j由其他page的出链决定，1-β表示由其他随机page决定<ul>
<li>equation<script type="math/tex; mode=display">
r_{j}=\sum_{i \rightarrow j} \beta \frac{r_{i}}{d_{i}}+(1-\beta) \frac{1}{N}</script></li>
<li>matrix G<script type="math/tex; mode=display">\boldsymbol{G}=\beta {\boldsymbol{M}}+(1-\beta)[1/N]_{N*N}</script></li>
</ul>
</li>
</ul>
<img src="/2021/12/18/CS224WLec4/pagerank1.png" class="" title="pagerank1">
<h1 id="PageRank扩展"><a href="#PageRank扩展" class="headerlink" title="PageRank扩展"></a>PageRank扩展</h1><ul>
<li>Personalized PageRank: teleport到在指定的子图S</li>
<li>Random walks with restarts: teleport到start page<img src="/2021/12/18/CS224WLec4/pagerank2.png" class="" title="pagerank2">
</li>
</ul>
<h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><ul>
<li>无法获得训练集里没有的node的embedding，必须重新训练</li>
<li>无法捕获结构相似性：例如同一张图中的局部结构相似性</li>
<li>无法利用node/edge以及图的特征：例如无法利用节点的其他属性</li>
<li>解决方法：deep learning, GNN</li>
</ul>
]]></content>
      <categories>
        <category>Lecture</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
        <tag>ML</tag>
        <tag>PageRank</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec5-Label Propagation for Node Classification[LIP]</title>
    <url>/2021/12/19/CS224WLec5/</url>
    <content><![CDATA[<p><a href="http://web.stanford.edu/class/cs224w/slides/05-message.pdf">ppt</a></p>
<h1 id="Semi-supervised-binary-node-classification"><a href="#Semi-supervised-binary-node-classification" class="headerlink" title="Semi-supervised binary node classification"></a>Semi-supervised binary node classification</h1><ul>
<li>goal: 知道部分节点的label求其他节点的label</li>
<li>main assumption：网络中存在的同质性。同类的node倾向于相互连接，或者聚在一起。</li>
<li>framework<ul>
<li>初始化</li>
<li>迭代</li>
<li>收敛</li>
</ul>
</li>
<li>approaches<ul>
<li>relational classification</li>
<li>iterative classification</li>
<li>correct &amp; smooth</li>
</ul>
</li>
<li>以节点二分类为例</li>
</ul>
<a id="more"></a>
<h1 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h1><h2 id="Probabilistic-relational-classifier"><a href="#Probabilistic-relational-classifier" class="headerlink" title="Probabilistic relational classifier"></a>Probabilistic relational classifier</h2><ul>
<li><strong>只用到了邻居节点的label来做分类</strong></li>
<li>step1: 初始化所有unlabeled nodes为0.5</li>
<li>step2: update所有unlabeled nodes的预测值 <script type="math/tex">P\left(Y_{v}=c\right)=\frac{1}{\sum_{(v, u) \in E} A_{v, u}} \sum_{(v, u) \in E} A_{v, u} P\left(Y_{u}=c\right)</script><ul>
<li>这里A为邻接矩阵，若边有权重，则Au,v是带权邻接矩阵</li>
</ul>
</li>
<li>step3: 重复step2直到收敛：达到最大steps，或者所有节点label不再更新</li>
<li>challenges: <ul>
<li>不能保证收敛</li>
<li>没有用到节点特征信息</li>
</ul>
</li>
</ul>
<h2 id="Iterative-classification"><a href="#Iterative-classification" class="headerlink" title="Iterative classification"></a>Iterative classification</h2><ul>
<li>Classify node v based on its attributes <script type="math/tex">f_v</script> as well as labels <script type="math/tex">z_v</script> of neighbor set <script type="math/tex">N_v</script></li>
<li><script type="math/tex">z_v</script>: v的neibors的label特征，可以是<ul>
<li><script type="math/tex">N_v</script>中每种label的histogram</li>
<li><script type="math/tex">N_v</script>中数量最多的label</li>
<li>不同label的数量</li>
</ul>
</li>
<li>流程<img src="/2021/12/19/CS224WLec5/classifier1.png" class="" title="classifier1">
</li>
</ul>
<h2 id="Collective-classification-correct-amp-smooth"><a href="#Collective-classification-correct-amp-smooth" class="headerlink" title="Collective classification: correct &amp; smooth"></a>Collective classification: correct &amp; smooth</h2><ul>
<li>recent state-of-the-art collective classification method</li>
<li>一种后处理方法</li>
<li>基于的假设：相邻节点的误差应该接近</li>
<li>correct step<ul>
<li>初始error: labeled node: error = ground truth minus soft label; unlabeled node: error=0</li>
<li>利用扩散矩阵 <script type="math/tex">\widetilde{\boldsymbol{A}}</script>求取下一步的error<script type="math/tex; mode=display">\boldsymbol{E}^{(t+1)} \leftarrow(1-\alpha) \cdot \boldsymbol{E}^{(t)}+\alpha \cdot \widetilde{\boldsymbol{A}} \boldsymbol{E}^{(t)}</script></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Lecture</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224WLec6~Lec9-GNN</title>
    <url>/2021/12/22/CS224WLec6/</url>
    <content><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><ul>
<li>DeepWalk/Node2vec属于shallow encoding，有如下优缺点：<ul>
<li>需要O(|V|)的参数量，节点间的embedding不共享，每个node有独立的embedding</li>
<li>推导式的（transductive）：training时没有的node，不会有embedding</li>
<li>没有利用到节点的特征，只利用了graph structure</li>
</ul>
</li>
<li>范式: <ul>
<li>encoder生成node embedding，DeepWalk&amp;Node2vec中为一个|V|*D的权重矩阵: <script type="math/tex">\operatorname{ENC}(v)=\mathbf{z}_{v}</script></li>
<li>decoder将node embedding映射回原空间，这里存在隐式的decoder，embedding空间两向量的点积可以表示原空间u,v的相似度: <script type="math/tex">\operatorname{similarity}(u, v) \approx \mathbf{z}_{v}^{\mathrm{T}} \mathbf{z}_{u}</script><ul>
<li>点积相似度：最小化两向量的模以及夹角余弦的乘积</li>
</ul>
</li>
</ul>
</li>
<li>GNN: deep encoding<ul>
<li>encoder为MLP</li>
<li>decoder为某种向量相似度</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h1 id="Basics-amp-Intros"><a href="#Basics-amp-Intros" class="headerlink" title="Basics &amp; Intros"></a>Basics &amp; Intros</h1><h2 id="Insights"><a href="#Insights" class="headerlink" title="Insights"></a>Insights</h2><ul>
<li>其他NN：<strong>依赖于IID(independent and identically distributed)，不同样本间是独立的</strong>，因此其无需满足排列不变性，而<strong>GNN节点间不独立</strong> —》 每个节点的特征依赖于其他节点 —》 节点顺序改变后，输入GNN的特征也会改变 —》 需要满足排列不变性，使得不同的节点排列，也能有同样的结果</li>
<li>原则<ul>
<li>排列不变性：调整输入节点的顺序，得到的同一个节点的表达应该一致。A为邻接矩阵，X为节点特征矩阵，两种不同的节点顺序下，得到的同一个节点的表达应该一致<script type="math/tex; mode=display">f\left(\boldsymbol{A}_{1}, \boldsymbol{X}_{1}\right)=f\left(\boldsymbol{A}_{2}, \boldsymbol{X}_{2}\right)</script></li>
</ul>
</li>
<li>MLP：不满足排列不变性<img src="/2021/12/22/CS224WLec6/GNN1.png" class="" title="GNN1"></li>
<li>利用MLP实现GNN不符合预期<img src="/2021/12/22/CS224WLec6/GNN2.png" class="" title="GNN2"></li>
<li>Insights: <strong>借鉴CNN，每次卷积操作只取某个点及其邻域点</strong><ul>
<li><strong>卷积：对邻域信息的提取以及归纳</strong></li>
</ul>
</li>
</ul>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><ul>
<li>利用每个节点的邻域节点为每个节点建立计算图。nn的层数k代表用了k hop的邻域。</li>
<li>每个节点都有不同的计算图<img src="/2021/12/22/CS224WLec6/GNN4.png" class="" title="GNN4"></li>
<li>layer0: node v的<strong>输入特征</strong></li>
<li>layerk: 经过了k跳后，node v的节点信息</li>
</ul>
<h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><ul>
<li>每一层包含两个阶段：信息aggregation &amp; passing<img src="/2021/12/22/CS224WLec6/GNN5.png" class="" title="GNN5"></li>
<li>每层参数共享</li>
<li><script type="math/tex">h_{v}^{k}</script>: the hidden representation of node v at layer k</li>
<li><script type="math/tex">W_{k}</script>: weight matrix for neighborhood aggregation</li>
<li><script type="math/tex">B_{k}</script>: weight matrix for transforming hidden vector of self</li>
<li>当aggregate为简单的平均时，可以转化为稀疏矩阵表达式 —》 稀疏矩阵便于优化<img src="/2021/12/22/CS224WLec6/GNN6.png" class="" title="GNN6">
</li>
</ul>
<h1 id="General-GNN-Framework"><a href="#General-GNN-Framework" class="headerlink" title="General GNN Framework"></a>General GNN Framework</h1><h2 id="A-Single-GNN-Layer"><a href="#A-Single-GNN-Layer" class="headerlink" title="A Single GNN Layer"></a>A Single GNN Layer</h2><ul>
<li>Goal: Compress a set of vectors into a single vector</li>
<li>Two steps:<ul>
<li>Message, v, 传送信息</li>
<li>Aggregate</li>
</ul>
</li>
<li>others:<ul>
<li>Nonlinearity(activation)：增加表达能力</li>
<li>residual/attention/dropout/BatchNorm/…</li>
</ul>
</li>
</ul>
<h3 id="Message"><a href="#Message" class="headerlink" title="Message"></a>Message</h3><ul>
<li>将l-1层的vectors过一个function，得到新的vectors，也称messages。对于每个node u：<script type="math/tex; mode=display">\mathbf{m}_{u}^{(l)}=\mathrm{MSG}^{(l)}\left(\mathbf{h}_{u}^{(l-1)}\right)</script></li>
<li>比如使用线性层作为该函数：<script type="math/tex; mode=display">\mathbf{m}_{u}^{(l)}=\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}</script></li>
</ul>
<h3 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h3><ul>
<li>对于节点v，将其领域内的所有messages聚合在一起<script type="math/tex; mode=display">\mathbf{h}_{v}^{(l)}=\mathrm{AGG}^{(l)}\left(\left\{\mathbf{m}_{u}^{(l)}, u \in N(v)\right\}\right)</script></li>
<li>principle: Aggregation需要满足排列不变性</li>
</ul>
<h3 id="Issues-amp-Solutions"><a href="#Issues-amp-Solutions" class="headerlink" title="Issues &amp; Solutions"></a>Issues &amp; Solutions</h3><ul>
<li>Issue1: 只考虑了领域，节点本身的信息被丢弃了</li>
<li>Solution1: 计算<script type="math/tex">{h}_{v}^{(l)}</script>的时候，考虑 <script type="math/tex">{h}_{v}^{(l-1)}</script>，可以认为是node v有一个self-edge<ul>
<li>Message<script type="math/tex; mode=display">\mathbf{m}_{u}^{(l)}=\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}, \mathbf{m}_{v}^{(l)}=\mathbf{B}^{(l)} \mathbf{h}_{v}^{(l-1)}</script></li>
<li>Aggregation<script type="math/tex; mode=display">\mathbf{m}_{u}^{(l)}=\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)} \mathbf{m}_{v}^{(l)}=\mathbf{B}^{(l)} \mathbf{h}_{v}^{(l-1)}</script></li>
</ul>
</li>
<li>Issue2: </li>
<li>Solution2: Residual</li>
</ul>
<h2 id="Classical-GNN-Layers"><a href="#Classical-GNN-Layers" class="headerlink" title="Classical GNN Layers"></a>Classical GNN Layers</h2><h3 id="GCN-Graph-Convolutional-Networks"><a href="#GCN-Graph-Convolutional-Networks" class="headerlink" title="GCN(Graph Convolutional Networks)"></a>GCN(Graph Convolutional Networks)</h3><ul>
<li>Message：利用出度作归一化。包含了self-edge。</li>
<li>Aggregation: sum<script type="math/tex; mode=display">
\mathbf{h}_{v}^{(l)}=\sigma\left(\sum_{u \in N(v)} \mathbf{W}^{(l)} \frac{\mathbf{h}_{u}^{(l-1)}}{|N(v)|}\right)</script></li>
</ul>
<h3 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h3><ul>
<li>Message和Aggregation结合在一起，且做了多次Aggregation</li>
<li>先对neighbors做aggregate，再和node本身aggregate，之后再计算一次Message<ul>
<li>邻域的Aggregation可以是Mean, pool, LSTM等<script type="math/tex; mode=display">
\mathbf{h}_{v}^{(l)}=\sigma\left(\mathbf{w}^{(l)} \cdot \operatorname{CONCAT}\left(\mathbf{h}_{v}^{(l-1)}, \mathrm{AGG}\left(\left\{\mathbf{h}_{u}^{(l-1)}, \forall u \in N(v)\right\}\right)\right)\right)</script></li>
</ul>
</li>
<li>l2 normalization: Apply l2 normalization to <script type="math/tex">{h}_{v}^{(l)}</script> at every layer。标准化后，每个vector的l2 norm都为1<script type="math/tex; mode=display">
\mathbf{h}_{v}^{(l)} \leftarrow \frac{\mathbf{h}_{v}^{(l)}}{\left\|\mathbf{h}_{v}^{(l)}\right\|_{2}} \forall v \in V</script></li>
</ul>
<h3 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h3><ul>
<li><strong>显式地获得不同邻域节点对目标节点的重要性</strong></li>
<li><script type="math/tex; mode=display">\alpha_{v u} $$ 为u(key)对v(value)的attention. 
- 例如GCN中 $$ \alpha_{v u}=\frac{1}{|N(v)|} $$ ，每个节点u对v的attention是一样的
$$ \mathbf{h}_{v}^{(l)}=\sigma\left(\sum_{u \in N(v)}{\alpha_{v u}} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}\right)</script></li>
<li>attention计算<ul>
<li>attention系数计算：相似度/MLP<script type="math/tex; mode=display">e_{v u}=a\left(\mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}, \mathbf{W}^{(l)} \boldsymbol{h}_{v}^{(l-1)}\right)</script></li>
<li>归一化: softmax<script type="math/tex; mode=display">\alpha_{v u}=\frac{\exp \left(e_{v u}\right)}{\sum_{k \in N(v)} \exp \left(e_{v k}\right)}</script></li>
</ul>
</li>
<li>多头attention<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{h}_{v}^{(l)}[1]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^{1} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}\right) \\
\mathbf{h}_{v}^{(l)}[2]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^{2} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}\right) \\
\mathbf{h}_{v}^{(l)}[3]=\sigma\left(\sum_{u \in N(v)} \alpha_{v u}^{3} \mathbf{W}^{(l)} \mathbf{h}_{u}^{(l-1)}\right)
\end{array}</script></li>
</ul>
<script type="math/tex; mode=display">
\mathbf{h}_{v}^{(l)}=\mathrm{AGG}\left(\mathbf{h}_{v}^{(l)}[1], \mathbf{h}_{v}^{(l)}[2], \mathbf{h}_{v}^{(l)}[3]\right)</script><ul>
<li>Pros<ul>
<li>Computationally efficien: attention系数可以并行计算所有的edges；aggregation可以并行计算所有nodes</li>
<li>Storage efficient: 不超过O(V+E)</li>
</ul>
</li>
</ul>
<h2 id="Stacking-GNN-layers"><a href="#Stacking-GNN-layers" class="headerlink" title="Stacking GNN layers"></a>Stacking GNN layers</h2><h3 id="The-over-smoothing-problem"><a href="#The-over-smoothing-problem" class="headerlink" title="The over-smoothing problem"></a>The over-smoothing problem</h3><ul>
<li>all the node embeddings converge to the same value</li>
<li><strong>每个node的embedding由其感受野决定，如果两个node的感受野重合的很多，其embedding越相近。而感受野由GNN的层数决定，层数越多，over-smoothing越严重</strong></li>
<li>Solution1: GNN层数的控制。先分析感受野大小，再<strong>根据所需感受野大小决定layers number</strong></li>
<li>Solution2: skip connections/residual。<ul>
<li>相当于mixture of models. N次skip, <script type="math/tex">2^{N}</script>个可能的路径/模型</li>
<li>Option1: 连到非线性前<script type="math/tex; mode=display">\mathbf{h}_{v}^{(l)}=\sigma\left(\sum_{u \in N(v)} \mathbf{W}^{(l)} \frac{\mathbf{h}_{u}^{(l-1)}}{|N(v)|}+\mathbf{h}_{v}^{(l-1)}\right)</script></li>
<li>Option2: 连到下一层</li>
</ul>
</li>
</ul>
<h3 id="提高GNN表达能力"><a href="#提高GNN表达能力" class="headerlink" title="提高GNN表达能力"></a>提高GNN表达能力</h3><ul>
<li>Solution1: make aggregation / transformation become a deep neural network!</li>
<li>Solution2: 增加其他层（MLP/CNN等）<img src="/2021/12/22/CS224WLec6/GNN7.png" class="" title="GNN7">
</li>
</ul>
<h2 id="Graph-Augmentation"><a href="#Graph-Augmentation" class="headerlink" title="Graph Augmentation"></a>Graph Augmentation</h2><ul>
<li>idea: raw input graph ≠ computation graph: <strong>计算图不必和实际的图结构保持一致</strong></li>
</ul>
<h3 id="Reason-for-augmentation"><a href="#Reason-for-augmentation" class="headerlink" title="Reason for augmentation"></a>Reason for augmentation</h3><ul>
<li>特征：原始图的特征可能比较少 —》 feature augmentation</li>
<li>图结构<ul>
<li>图太稀疏：inefficient message passing —》 Add virtual nodes / edges</li>
<li>图太稠密：costly —》 Sample neighbors when doing message passing</li>
<li>图太大：GPU放不下 —》 Sample subgraphs to compute embeddings </li>
</ul>
</li>
</ul>
<h3 id="Feature-augementation"><a href="#Feature-augementation" class="headerlink" title="Feature augementation"></a>Feature augementation</h3><ul>
<li>one-hot/constant encoding</li>
<li>others: 一般会使用：<ul>
<li>node degree</li>
<li>clustering coefficient</li>
<li>pagerank</li>
<li>centrality</li>
</ul>
</li>
</ul>
<h3 id="Sparse-graphs-augmentation"><a href="#Sparse-graphs-augmentation" class="headerlink" title="Sparse graphs augmentation"></a>Sparse graphs augmentation</h3><ul>
<li>add virtual nodes/edges</li>
<li>Add virtual edges<ul>
<li>例如在使用邻接矩阵A的时候，改为使用 <script type="math/tex">A+A^{2}</script>，A2相当于添加了virtual edges</li>
</ul>
</li>
<li>Add vitual nodes<ul>
<li>提高稀疏图中的message passing</li>
</ul>
</li>
</ul>
<h3 id="Node-neighborhood-sampling"><a href="#Node-neighborhood-sampling" class="headerlink" title="Node neighborhood sampling"></a>Node neighborhood sampling</h3><ul>
<li>针对很稠密的图引起的costly问题</li>
<li>方案：sampling<ul>
<li>对某个target节点，sample其neighbors</li>
<li>sample target节点</li>
</ul>
</li>
</ul>
<h2 id="GNN-training-pipeline"><a href="#GNN-training-pipeline" class="headerlink" title="GNN training pipeline"></a>GNN training pipeline</h2><img src="/2021/12/22/CS224WLec6/GNN8.png" class="" title="GNN8">
<ul>
<li>以上工作将计算得到set of d-dim node embs</li>
<li>还存在两个问题：<ul>
<li>这些emb需要被应用在具体的任务中（前向过程需要完善）</li>
<li>emb需要更新（反向传播需要定义）</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
\left\{\mathbf{h}_{v}^{(L)} \in \mathbb{R}^{d}, \forall v \in G\right\}</script><h3 id="Prediction-heads"><a href="#Prediction-heads" class="headerlink" title="Prediction heads"></a>Prediction heads</h3><h4 id="node-level"><a href="#node-level" class="headerlink" title="node-level"></a>node-level</h4><ul>
<li>直接利用node embs，其中<script type="math/tex">\mathbf{W}^{(H)} \in \mathbb{R}^{k * d}</script>: map node embeddings from <script type="math/tex">\mathbf{h}_{v}^{(L)} \in \mathbb{R}^{d}</script> to <script type="math/tex">\widehat{\boldsymbol{y}}_{v} \in \mathbb{R}^{k}</script>  so that we can compute the loss</li>
</ul>
<script type="math/tex; mode=display">\widehat{\boldsymbol{y}}_{v}=\operatorname{Head}_{\text {node }}\left(\mathbf{h}_{v}^{(L)}\right)=\mathbf{W}^{(H)} \mathbf{h}_{v}^{(L)}</script><h4 id="edge-level"><a href="#edge-level" class="headerlink" title="edge-level"></a>edge-level</h4><ul>
<li>使用pair of node embs</li>
<li>option1: concat+linear(map 2d-dim to k-dim)<script type="math/tex; mode=display">
\left.\widehat{\boldsymbol{y}}_{u v}=\text { Linear(Concat }\left(\mathbf{h}_{u}^{(L)}, \mathbf{h}_{v}^{(L)}\right)\right)</script></li>
<li>option2: dot product. 得到一个连续值，只能应用于二分类预测/一维回归</li>
<li>option3: 多头的dot product，多个加权的option2<script type="math/tex; mode=display">
\begin{array}{c}
\widehat{\boldsymbol{y}}_{u v}^{(1)}=\left(\mathbf{h}_{u}^{(L)}\right)^{T} \mathbf{W}^{(1)} \mathbf{h}_{v}^{(L)} \\
\widehat{\boldsymbol{y}}_{u v}^{(k)}=\left(\mathbf{h}_{u}^{(L)}\right)^{T} \mathbf{W}^{(k)} \mathbf{h}_{v}^{(L)} \\
\widehat{\boldsymbol{y}}_{u v}=\operatorname{Concat}\left(\widehat{y}_{u v}^{(1)}, \ldots, \widehat{\boldsymbol{y}}_{u v}^{(k)}\right) \in \mathbb{R}^{k}
\end{array}</script></li>
</ul>
<h4 id="graph-level"><a href="#graph-level" class="headerlink" title="graph-level"></a>graph-level</h4><ul>
<li>option1: global pooling<ul>
<li>问题：无法区分不同scale的graph<script type="math/tex; mode=display">
\begin{array}{l}
\text { Prediction for } G_{1}: \hat{y}_{G}=\operatorname{Sum}(\{-1,-2,0,1,2\})=0 \\
\text { Prediction for } G_{2}: \hat{y}_{G}=\operatorname{Sum}(\{-10,-20,0,10,20\})=0
\end{array}</script></li>
</ul>
</li>
<li>option2: hierarchically global pooling<ul>
<li>需要同时实现两个GNN任务：GNN A：计算node embeddings; GNN B：聚类</li>
<li>两个任务可以并行训练</li>
<li>为每一个cluster创建一个新的node，为相连的nodes创建edge，并生成一个新的pooled network<img src="/2021/12/22/CS224WLec6/GNN9.png" class="" title="GNN9">
</li>
</ul>
</li>
</ul>
<h3 id="Loss-defines"><a href="#Loss-defines" class="headerlink" title="Loss defines"></a>Loss defines</h3><ul>
<li>和其他nn无区别</li>
</ul>
<h3 id="Dataset-split"><a href="#Dataset-split" class="headerlink" title="Dataset split"></a>Dataset split</h3><ul>
<li>难点：graph的各个node/edge之间不满足iid假设。random split会带来information leakage。</li>
<li>solution1(Transductive setting): 会有部分leakage。仅仅split node labels.<ul>
<li>At training time, we compute embeddings using <strong>the entire graph</strong>, and train <strong>using node 1&amp;2’s labels</strong></li>
<li>At validation time, we compute embeddings using <strong>the entire graph</strong>, and evaluate on <strong>node 3&amp;4’s labels</strong></li>
<li>training / validation / test sets都在同一个graph上。三个dataset组成一个graph。</li>
<li>可以应用在node/edge tasks。因为graph task需要在unseen graphs上做测试，而transductive方法无法满足。</li>
</ul>
</li>
<li>solution2(Inductive setting): We break the edges between splits to get multiple graphs。没有信息泄漏，图被分成了三个子图。<ul>
<li>At training time, we compute embeddings <strong>using the graph over node 1&amp;2</strong>, and train using node 1&amp;2’s labels</li>
<li>At validation time, we compute embeddings <strong>using the graph over node 3&amp;4</strong>, and evaluate on node 3&amp;4’s labels</li>
<li>training / validation / test sets不在同一个graph上。三个dataset组成是三个graph。</li>
<li>可以应用在node/edge/graph tasks</li>
</ul>
</li>
</ul>
<h4 id="link-prediction"><a href="#link-prediction" class="headerlink" title="link prediction"></a>link prediction</h4><ul>
<li>link prediction是无监督任务，需要定义label &amp; split</li>
<li>step1: 先划分message edges和supervision edges。其中supervision edges不入图，只作为label。</li>
<li>step2: split<ul>
<li>Transductive method: 有四种edge: training message edges &amp; training supervision edges &amp; validation edges &amp; predict test edges.<img src="/2021/12/22/CS224WLec6/GNN10.png" class="" title="GNN10"></li>
<li>Inductive method: In train or val or test set, each graph will have 2 types of edges。<img src="/2021/12/22/CS224WLec6/GNN11.png" class="" title="GNN11">
</li>
</ul>
</li>
</ul>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><ul>
<li>data processing: use normalization</li>
<li>optimizer: adam is relatively robust to learning rate</li>
<li>activation: relu</li>
<li>bias term in every layer</li>
<li>debug:<ul>
<li>小数据集上，loss应该很小。如果underfit, something is wrong</li>
<li>loss</li>
<li>visualizations</li>
<li>initialization</li>
<li>adjust hyperparameters such as learning rate</li>
</ul>
</li>
</ul>
<h1 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h1><ul>
<li>本质：<strong>假设将节点作为特征向量作为输入feed into MLP, 特征向量会包含邻域节点的信息，因此不能满足排列不变性，而GNN则是把节点的领域信息存在了NN结构里，因此每个节点都拥有自己的计算图/神经网络结构</strong></li>
<li>流程：1）定义neighborhood aggregation function; 2) 定义loss function; 3) train; 4) generate node embeddings</li>
<li>权重矩阵共享：参数量为|V|的次线性</li>
<li>Inductive(归纳式的)：<strong>可以为没出现过的node生成embedding</strong>。由于权重矩阵W/B的共享，即使没有出现过的node，也可以为其生成计算图，进而生成embedding</li>
<li><strong>可以为新的graph生成embedding</strong>，前提是new graph中的节点都出现在了old graph中</li>
</ul>
<h1 id="与MLP-CNN-Transformer的异同"><a href="#与MLP-CNN-Transformer的异同" class="headerlink" title="与MLP/CNN/Transformer的异同"></a>与MLP/CNN/Transformer的异同</h1><h2 id="GNN-VS-MLP"><a href="#GNN-VS-MLP" class="headerlink" title="GNN VS MLP"></a>GNN VS MLP</h2><ul>
<li>MLP假设IID，不同样本间独立，因此无需满足排列不变性</li>
<li>Graph数据node间不独立，因此需要满足排列不变性</li>
<li>将node的特征向量作为输入，使用MLP —》 将graph的结构作为MLP的输入 —》 一般一个数据集就一种nn结构</li>
<li>GNN —》 将graph的结构作为nn的结构 —》 一个graph有多种nn结构</li>
</ul>
<h2 id="GNN-VS-CNN"><a href="#GNN-VS-CNN" class="headerlink" title="GNN VS CNN"></a>GNN VS CNN</h2><ul>
<li>GNN: <script type="math/tex">\mathrm{h}_{v}^{(l+1)}=\sigma\left(\mathrm{W}_{l} \sum_{u \in \mathrm{N}(v)} \frac{\mathrm{h}_{u}^{(l)}}{|\mathrm{N}(v)|}+\mathrm{B}_{l} \mathrm{~h}_{v}^{(l)}\right), \forall l \in\{0, \ldots, L-1\}</script></li>
<li>CNN: <script type="math/tex">\mathrm{h}_{v}^{(l+1)}=\sigma\left(\sum_{u \in \mathrm{N}(v) \cup\{v\}} \mathrm{W}_{l}^{u} \mathrm{~h}_{u}^{(l)}\right), \forall l \in\{0, \ldots, L-1\}</script></li>
<li>重写CNN：<script type="math/tex">\mathrm{h}_{v}^{(l+1)}=\sigma\left(\sum_{u \in \mathrm{N}(v)} \mathrm{W}_{l}^{u} \mathrm{~h}_{u}^{(l)}+\mathrm{B}_{l} \mathrm{~h}_{v}^{(l)}\right), \forall l \in\{0, \ldots, L-1\}</script></li>
<li>从邻域选择讲，CNN是邻域确定的特殊GNN</li>
<li>CNN不满足排列不变性，改变像素排列，输出也不同</li>
</ul>
<h2 id="GNN-VS-Transformer"><a href="#GNN-VS-Transformer" class="headerlink" title="GNN VS Transformer"></a>GNN VS Transformer</h2><ul>
<li>Transformer是针对序列数据的，一个序列中两个node互为上下文，因此可以看做是所有n ode都互相连接的graph</li>
</ul>
<h1 id="GNN-Theory"><a href="#GNN-Theory" class="headerlink" title="GNN Theory"></a>GNN Theory</h1><ul>
<li>GNN表达能力的衡量，如何设计表达能力强的GNN</li>
<li>表达能力强的GNN能够为不同的节点生成不同的embedding
</li>
</ul>
<h2 id="GNN的表达能力"><a href="#GNN的表达能力" class="headerlink" title="GNN的表达能力"></a>GNN的表达能力</h2><ul>
<li>GNN的计算图：每个节点的有根子树<img src="/2021/12/22/CS224WLec6/GNN13.png" class="" title="GNN13"></li>
<li>Injective function: 内射函数。f(x)=Y，不同的X能映射为不同的Y，称之为内射函数</li>
<li>如果每一步的neighbor aggregation是内射函数，则GNN可以分辨不同的有根子树。</li>
<li>GCN(mean-pool, ex. MeanPool([1,0],[0,1])与MeanPool([1,0],[0,1],[1,0],[0,1])一样)/GraphSAGE(max-pool)都不是内射函数</li>
<li>Any injective multi-set function can be expressed as: <script type="math/tex">\Phi\left(\sum_{x \in S} f(x)\right)</script>, f和外层φ为非线性函数，<strong>中间是对multi-set做sum</strong>。而非线性函数可以用MLP来建模</li>
</ul>
<h2 id="GIN-Graph-Isomorphism-Network"><a href="#GIN-Graph-Isomorphism-Network" class="headerlink" title="GIN(Graph Isomorphism Network)"></a>GIN(Graph Isomorphism Network)</h2><ul>
<li>THE most expressive GNN in the class of message-passing GNNs</li>
<li>Apply an MLP, element-wise sum, followed by another MLP.<script type="math/tex; mode=display">\mathrm{MLP}_{\Phi}\left(\sum_{x \in S} \operatorname{MLP}_{f}(x)\right)</script></li>
</ul>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="http://web.stanford.edu/class/cs224w/slides/06-GNN1.pdf">ppt Lec6: Graph Neural Networks 1: GNN Model</a></li>
<li><a href="http://web.stanford.edu/class/cs224w/slides/07-GNN2.pdf">ppt Lec7: Graph Neural Networks 2: Design Space</a></li>
<li><a href="http://web.stanford.edu/class/cs224w/slides/08-GNN-application.pdf">ppt Lec8: Applications of Graph Neural Networks</a></li>
<li><a href="http://web.stanford.edu/class/cs224w/slides/09-theory.pdf">ppt Lec9: Theory of Graph Neural Networks</a></li>
</ul>
]]></content>
      <categories>
        <category>Lecture</category>
        <category>cs224w</category>
      </categories>
      <tags>
        <tag>cs224w</tag>
        <tag>Graph</tag>
        <tag>GCN</tag>
        <tag>GNN</tag>
        <tag>GraphSAGE</tag>
        <tag>GAT</tag>
        <tag>GIN</tag>
      </tags>
  </entry>
  <entry>
    <title>CS学习路线、资源推荐</title>
    <url>/2021/12/10/CS%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/</url>
    <content><![CDATA[<ul>
<li><a href="https://conanhujinming.github.io/post/how_to_learn_cs/">转载自胡神的“自学CS路线、资源推荐”</a></li>
<li><a href="https://conanhujinming.github.io/comments-for-awesome-courses/">名校公开课程评价网
</a></li>
</ul>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>Resource</tag>
      </tags>
  </entry>
  <entry>
    <title>CV-目标检测综述</title>
    <url>/2021/12/10/CV-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul>
<li>图像识别+定位</li>
<li>识别：分类问题，准确率</li>
<li>定位：分类/回归问题，找到一个框/4个坐标，IOU</li>
</ul>
<h2 id="传统目标检测"><a href="#传统目标检测" class="headerlink" title="传统目标检测"></a>传统目标检测</h2><h3 id="用回归做定位问题"><a href="#用回归做定位问题" class="headerlink" title="用回归做定位问题"></a>用回归做定位问题</h3><ul>
<li>训练一个cnn网络，在最后一个卷积层后分两个head，一个head做分类，另一个回归</li>
<li>先fine tuning分类任务，再fine tuning回归</li>
<li>缺点：回归问题，很难做</li>
</ul>
<h3 id="用分类做定位问题"><a href="#用分类做定位问题" class="headerlink" title="用分类做定位问题"></a>用分类做定位问题</h3><ul>
<li>滑动窗口（选择不同位置不同大小的区域），对其定位，对每个框内的图像做分类</li>
<li>缺点：窗口冗余、复杂度高、多物体多分类时复杂度更高</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
        <category>CV</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>CV</tag>
        <tag>ML</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-DBSCAN以及sklearn实现DBSCAN</title>
    <url>/2021/12/10/DBSCAN/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]<br><a href="https://www.cnblogs.com/pinard/p/6208966.html">原文1</a><br><a href="https://www.cnblogs.com/pinard/p/6217852.html">原文2</a><br>DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的<strong>密度聚类算法</strong>，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN<strong>既可以适用于凸样本集，也可以适用于非凸样本集</strong>。</p>
<a id="more"></a>
<h1 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h1><p>其原理为：同一类别的样本，其样本分布一定是紧密的；可以将各组紧密相连的样本划分为不同的类别来得到聚类类别结果。</p>
<h1 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h1><h2 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h2><p>参数(ϵ, MinPts)描述领域的样本分布紧密程度，其中ϵ描述了某一样本的领域<strong>距离阈值</strong>，MinPts描述某一样本的距离为ϵ的领域中样本<strong>个数的阈值</strong>。</p>
<p>假设样本集是D=(x1,x2,…,xm),则DBSCAN具体的密度描述定义如下：</p>
<ol>
<li>ϵ-邻域：对于xj∈D，其ϵ-邻域包含样本集D中与xj的距离不大于ϵ的子样本集，即Nϵ(xj)={xi∈D|distance(xi,xj)≤ϵ}, 这个子样本集的个数记为|Nϵ(xj)|</li>
<li>核心对象：对于任一样本xj∈D，如果其ϵ-邻域对应的Nϵ(xj)至少包含MinPts个样本，即如果|Nϵ(xj)|≥MinPts，则xj是核心对象。</li>
<li>密度直达：如果xi位于xj的ϵ-邻域中，且xj是核心对象，则称xi由xj密度直达。</li>
<li>密度可达：对于xi和xj,如果存在样本样本序列p1,p2,…,pT,满足p1=xi,pT=xj, 且pt+1由pt密度直达，则称xj由xi密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本p1,p2,…,pT−1均为核心对象，因为只有核心对象才能使其他样本密度直达。</li>
<li>密度相连：对于xi和xj,如果存在核心对象样本xk，使xi和xj均由xk密度可达，则称xi和xj密度相连。注意密度相连关系是满足对称性的。</li>
</ol>
<p>图中MinPts = 5。红点为核心对象。<br><img src="https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161222112847323-1346197243.png" alt="示意图"></p>
<h2 id="聚类思想"><a href="#聚类思想" class="headerlink" title="聚类思想"></a>聚类思想</h2><p>由密度可达关系导出的最大密度相连的样本集合，即为最终聚类的一个类别。</p>
<p>方法：任意选择一个没有类别的核心对象作为种子，然后找到该核心对象密度可达的样本集合，为一个聚类。接着选择另一个没有类比的核心对象…直到所有核心对象都有类别。</p>
<p>问题：</p>
<ol>
<li>outlier.不在任何一个核心对象周围的点定义为异常样本点或噪声点，不考虑。</li>
<li>距离。少量样本而言，搜索周围样本一般用最近邻的方法；大量样本，可以用KD树，球树等搜索最近邻。</li>
<li>若某样本到两个核心对象的距离都小于ϵ，但这两个核心对象不可达，此时采取先来后到原则，标记其为先聚类的cluster类别。</li>
</ol>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>输入：样本集D=(x1,x2,…,xm)，邻域参数(ϵ,MinPts), 样本距离度量方式</p>
<p>输出： 簇划分C.　</p>
<ol>
<li>初始化核心对象集合Ω=∅, 初始化聚类簇数k=0，初始化未访问样本集合Γ = D,  簇划分C = ∅</li>
<li>对于j=1,2,…m, 按下面的步骤找出所有的核心对象：<ul>
<li>通过距离度量方式，找到样本xj的ϵ-邻域子样本集Nϵ(xj)</li>
<li>如果子样本集样本个数满足|Nϵ(xj)|≥MinPts， 将样本xj加入核心对象样本集合：Ω=Ω∪{xj}</li>
</ul>
</li>
<li>如果核心对象集合Ω=∅，则算法结束，否则转入步骤4.</li>
<li>在核心对象集合Ω中，随机选择一个核心对象o，初始化当前簇核心对象队列Ωcur={o}, 初始化类别序号k=k+1，初始化当前簇样本集合Ck={o}, 更新未访问样本集合Γ=Γ−{o}</li>
<li>如果当前簇核心对象队列Ωcur=∅，则当前聚类簇Ck生成完毕, 更新簇划分C={C1,C2,…,Ck}, 更新核心对象集合Ω=Ω−Ck， 转入步骤3。</li>
<li>在当前簇核心对象队列Ωcur中取出一个核心对象o′,通过邻域距离阈值ϵ找出所有的ϵ-邻域子样本集Nϵ(o′)，令Δ=Nϵ(o′)∩Γ, 更新当前簇样本集合Ck=Ck∪Δ, 更新未访问样本集合Γ=Γ−Δ,  更新Ωcur=Ωcur∪(Δ∩Ω)−o′，转入步骤5.</li>
</ol>
<p>输出结果为： 簇划分C={C1,C2,…,Ck}</p>
<h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><p>对比图：<img src="https://img-blog.csdn.net/20170419143546349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="与其他算法的对比图"><br>一般用于数据集稠密时的情况，或数据集是非凸的。</p>
<p>DBSCAN的主要优点有：</p>
<ol>
<li>可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li>
<li>可以在聚类的同时发现异常点，对数据集中的异常点不敏感。</li>
<li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li>
</ol>
<p>DBSCAN的主要缺点有：</p>
<ol>
<li>如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li>
<li>如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。</li>
<li>调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。</li>
</ol>
<h1 id="sklearn-cluster-DBSCAN"><a href="#sklearn-cluster-DBSCAN" class="headerlink" title="sklearn.cluster.DBSCAN"></a>sklearn.cluster.DBSCAN</h1><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>按其算法，包括DBSCAN本身的参数，以及求取最近邻时的参数。</p>
<ol>
<li>eps：DBSCAN算法参数，ϵ-邻域的距离阈值。默认值是0.5.eps过大，则更多的点会落在核心对象的ϵ-邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。</li>
<li>min_samples：DBSCAN算法参数，上文的MinPts。默认值是5.通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。</li>
<li>metric：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。可以使用的距离度量参数有：<ul>
<li>欧式距离 “euclidean”</li>
<li>曼哈顿距离 “manhattan”</li>
<li>切比雪夫距离“chebyshev”</li>
<li>闵可夫斯基距离 “minkowski”</li>
<li>带权重闵可夫斯基距离 “wminkowski”</li>
<li>标准化欧式距离 “seuclidean”</li>
<li>马氏距离“mahalanobis”</li>
</ul>
</li>
<li>algorithm：最近邻搜索算法参数，算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现，‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。个人的经验，一般情况使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用”auto”建树时间可能会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。</li>
<li>leaf_size：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 因为这个值一般只影响算法的运行速度和使用内存大小，因此一般情况下可以不管它。</li>
<li>p: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。</li>
<li>n_jobs：使用CPU格式，-1代表全开。</li>
</ol>
<p>输出：</p>
<ul>
<li>core<em>sample_indices</em>:核心样本指数。（此参数在代码中有详细的解释）</li>
<li>labels_:数据集中每个点的集合标签给,噪声点标签为-1。</li>
<li>components_ ：核心样本的副本</li>
</ul>
<p>主要是<strong>eps和min_samples</strong>的调参。</p>
<h2 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h2><p>原文2中有。<br><a href="https://www.cnblogs.com/pinard/p/6217852.html">原文2</a></p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Clustering</tag>
        <tag>DBSCAN</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-GBDT &amp; XGBoost</title>
    <url>/2021/12/10/GBDT%20&amp;%20XGBoost/</url>
    <content><![CDATA[<p>[TOC]</p>
<p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Doc</a><br><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">Slides</a></p>
<a id="more"></a>
<h1 id="为何要推导出目标函数而不是直接增加树"><a href="#为何要推导出目标函数而不是直接增加树" class="headerlink" title="为何要推导出目标函数而不是直接增加树"></a>为何要推导出目标函数而不是直接增加树</h1><p><img src="http://i.imgur.com/quPhp1K.png" alt="Objective function"></p>
<ul>
<li>理论上：搞清楚learning的目的，以及其收敛性。</li>
<li>工程上：<ul>
<li>gi和hi是对loss function的一次、二次导</li>
<li>目标函数以及整个学习过程只依赖于gi和hi</li>
<li>可以根据实际问题，自定义loss function</li>
</ul>
</li>
</ul>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><img src="http://i.imgur.com/L7PhJwO.png" alt="Summary"></p>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><script type="math/tex; mode=display">\text{obj} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i)</script><p>l为loss，\ \Omega \ 为正则项</p>
<ul>
<li>loss：采用加法策略，第t颗树时：<script type="math/tex; mode=display">\hat{y}_i^{(0)} = 0</script><script type="math/tex; mode=display">\hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)</script><script type="math/tex; mode=display">\hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)</script><script type="math/tex; mode=display">\dots</script><script type="math/tex; mode=display">\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)</script>在添加第t颗树时，需要优化的目标函数为：<script type="math/tex; mode=display">\sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)</script>其中h和f：<script type="math/tex; mode=display">g_i = \partial_{\hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)})</script><script type="math/tex; mode=display">h_i = \partial_{\hat{y}_i^{(t-1)}}^2 l(y_i, \hat{y}_i^{(t-1)})</script>note: 是对谁的导</li>
<li>正则项：复杂度：<script type="math/tex; mode=display">\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2</script>其中w是叶子上的score vector，T是叶子数量</li>
</ul>
<h2 id="DART-Booster"><a href="#DART-Booster" class="headerlink" title="DART Booster"></a>DART Booster</h2><p>为了解决过拟合，会随机drop trees:</p>
<ul>
<li>训练速度可能慢于gbtree</li>
<li>由于随机性，早停可能不稳定</li>
</ul>
<h1 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h1><h2 id="Monotonic-Constraints单调性限制"><a href="#Monotonic-Constraints单调性限制" class="headerlink" title="Monotonic Constraints单调性限制"></a>Monotonic Constraints单调性限制</h2><ul>
<li><p>一个可选特性:<br>会限制模型的结果按照某个特征 单调的进行增减</p>
<p>也就是说可以降低模型对数据的敏感度，如果明确已知某个特征与预测结果呈单调关系时，那在生成模型的时候就会跟特征数据的单调性有关。</p>
</li>
</ul>
<h2 id="Feature-Interaction-Constraints单调性限制"><a href="#Feature-Interaction-Constraints单调性限制" class="headerlink" title="Feature Interaction Constraints单调性限制"></a>Feature Interaction Constraints单调性限制</h2><ul>
<li><p>一个可选特性：<br>不用时，在tree生成的时候，一棵树上的节点会无限制地选用多个特征</p>
<p>设置此特性时，可以规定，哪些特征可以有interaction（一般独立变量之间可以interaction，非独立变量的话可能会引入噪声）</p>
</li>
<li>好处：<ul>
<li>预测时更小的噪声</li>
<li>对模型更好地控制</li>
</ul>
</li>
</ul>
<h2 id="Instance-Weight-File"><a href="#Instance-Weight-File" class="headerlink" title="Instance Weight File"></a>Instance Weight File</h2><ul>
<li>规定了模型训练时data中每一条instance的权重</li>
<li>有些instance质量较差，或与前一示例相比变化不大，所以可以调节其所占权重</li>
</ul>
<h1 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h1><h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><p>与overfitting有关的参数：</p>
<ul>
<li>直接控制模型复杂度：max_depth, min_child_weight and gamma.</li>
<li>增加模型随机性以使得模型对噪声有更强的鲁棒性：<ul>
<li>subsample and colsample_bytree. </li>
<li>Reduce stepsize eta. Remember to increase num_round when you do so.</li>
</ul>
</li>
</ul>
<h2 id="Imbalanced-Dataset"><a href="#Imbalanced-Dataset" class="headerlink" title="Imbalanced Dataset"></a>Imbalanced Dataset</h2><ul>
<li>只关注测量指标的大小<ul>
<li>平衡数据集 via scale_pos_weight</li>
<li>使用AUC作为metric</li>
</ul>
</li>
<li>关注预测正确的概率<ul>
<li>此时不能re-balance数据集</li>
<li>Set parameter max_delta_step to a finite number (say 1) to help convergence</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-GBDT</title>
    <url>/2021/12/10/GBDT/</url>
    <content><![CDATA[<p>[TOC]</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul>
<li>GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型,所以说，在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。</li>
<li>在GradientBoosting算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。</li>
<li>GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此<strong>GBDT的树都是CART回归树</strong>，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。</li>
</ul>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ul>
<li>GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显，1、它能灵活的处理各种类型的数据；2、在相对较少的调参时间下，预测的准确度较高。</li>
<li>当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph-与图有关的资源</title>
    <url>/2021/12/22/Graph/</url>
    <content><![CDATA[<h2 id="OpenResource"><a href="#OpenResource" class="headerlink" title="OpenResource"></a>OpenResource</h2><ul>
<li><a href="https://ogb.stanford.edu/">OGB</a>: The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs.</li>
<li><a href="https://github.com/snap-stanford/GraphGym">GraphGym</a>: GraphGym is a platform for designing and evaluating Graph Neural Networks. —&gt; 和pytorch配合</li>
<li><a href="https://github.com/deepmind/graph_nets">GraphNets</a>: 配合tensorflow</li>
</ul>
<h2 id="Class"><a href="#Class" class="headerlink" title="Class"></a>Class</h2><ul>
<li><a href="http://web.stanford.edu/class/cs224w/">CS224W: Machine Learning with Graphs</a></li>
</ul>
<h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h2><ul>
<li>Tutorials and overviews:<ul>
<li>Relational inductive biases and graph networks (Battaglia et al., 2018)</li>
<li>Representation learning on graphs: Methods and applications (Hamilton et al., 2017)</li>
</ul>
</li>
<li>Attention-based neighborhood aggregation:<ul>
<li>Graph attention networks (Hoshen, 2017; Velickovic et al., 2018; Liu et al., 2018)</li>
</ul>
</li>
<li>Embedding entire graphs:<ul>
<li>Graph neural nets with edge embeddings (Battaglia et al., 2016; Gilmer et. al., 2017)</li>
<li>Embedding entire graphs (Duvenaud et al., 2015; Dai et al., 2016; Li et al., 2018) and graph pooling (Ying et al., 2018, Zhang et al., 2018)</li>
<li>Graph generation and relational inference (You et al., 2018; Kipf et al., 2018)</li>
<li>How powerful are graph neural networks(Xu et al., 2017)</li>
</ul>
</li>
<li>Embedding nodes:<ul>
<li>Varying neighborhood: Jumping knowledge networks (Xu et al., 2018), GeniePath (Liu et al., 2018)</li>
<li>Position-aware GNN (You et al. 2019)</li>
</ul>
</li>
<li>Spectral approaches to graph neural networks:<ul>
<li>Spectral graph CNN &amp; ChebNet (Bruna et al., 2015; Defferrard et al., 2016)</li>
<li>Geometric deep learning (Bronstein et al., 2017; Monti et al., 2017)</li>
</ul>
</li>
<li>Other GNN techniques:<ul>
<li>Pre-training Graph Neural Networks (Hu et al., 2019)</li>
<li>GNNExplainer: Generating Explanations for Graph Neural Networks (Ying et al., 2019)</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>ML</tag>
        <tag>Resource</tag>
      </tags>
  </entry>
  <entry>
    <title>GraphEmbedding-LINE</title>
    <url>/2021/12/20/GraphEmbedding-LINE/</url>
    <content><![CDATA[<p>paper: <a href="https://arxiv.org/pdf/1503.03578.pdf">LINE: Large-scale Information Network Embedding</a></p>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><ul>
<li>包含一阶信息（直接相连的两节点的邻近度）和二阶信息（一对顶点的邻近度用临近节点的相似性衡量）的GE学习模型：<strong>只用到了一阶邻点</strong></li>
<li>DeepWalk/node2vec: 基于高阶相似度的GE学习模型</li>
</ul>
<a id="more"></a>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><ul>
<li>边权重矩阵W：损失函数计算时用</li>
<li>节点权重矩阵：节点采样时用</li>
</ul>
<h2 id="一阶相似度"><a href="#一阶相似度" class="headerlink" title="一阶相似度"></a>一阶相似度</h2><ul>
<li><strong>只针对无向图，不支持有向图</strong></li>
<li>边(u,v)的权重Wuv表示u和v之间的一阶相似性，如果在u和v之间无边，它们的一阶相似性为0。</li>
<li>经验分布<script type="math/tex">\hat{p}_{1}(i, j)=\frac{w_{i j}}{W}</script>，这里W是W矩阵的模</li>
<li>预测分布 <script type="math/tex">p_{1}\left(v_{i}, v_{j}\right)=\frac{1}{1+\exp \left(-\vec{u}_{i}^{T} \cdot \vec{u}_{j}\right)}</script></li>
<li>目标函数 <script type="math/tex">O_{1}=d\left(\hat{p}_{1}(\cdot, \cdot), p_{1}(\cdot, \cdot)\right)</script>, d为距离函数，例如当d为KL散度且忽略常数项时（同交叉熵）：<script type="math/tex; mode=display">
O_{1}=-\sum_{(i, j) \in E} w_{i j} \log p_{1}\left(v_{i}, v_{j}\right)</script></li>
</ul>
<h2 id="二阶相似度"><a href="#二阶相似度" class="headerlink" title="二阶相似度"></a>二阶相似度</h2><ul>
<li>如果没有同样的相邻节点，二阶相似性为0</li>
<li>经验分布 <script type="math/tex">\hat{p}_{2}\left(v_{j} \mid v_{i}\right)=\frac{w_{i j}}{d_{i}}</script>，di是节点i的带权出度，<script type="math/tex">w_{i j}</script>是边(i,j)的权重，<script type="math/tex">d_{i}=\sum_{k \in N(i)} w_{i k}</script></li>
<li>预测分布 <script type="math/tex">p_{2}\left(v_{j} \mid v_{i}\right)=\frac{\exp \left(\vec{u}_{j}^{\prime T} \cdot \vec{u}_{i}\right)}{\sum_{k=1}^{|V|} \exp \left(\vec{u}_{k}^{\prime T} \cdot \vec{u}_{i}\right)}</script></li>
<li>目标函数 <script type="math/tex">O_{2}=\sum_{i \in V} \lambda_{i} d\left(\hat{p}_{2}\left(\cdot \mid v_{i}\right), p_{2}\left(\cdot \mid v_{i}\right)\right)</script>，这里<script type="math/tex">\lambda_{i}</script>为对节点i的加权，一般取 <script type="math/tex">\lambda_{i}</script> 为 <script type="math/tex">d_{i}</script>，并取KL散度为距离函数时：<script type="math/tex">O_{2}=-\sum_{(i, j) \in E} w_{i j} \log p_{2}\left(v_{j} \mid v_{i}\right)</script></li>
</ul>
<h2 id="结合一阶和二阶"><a href="#结合一阶和二阶" class="headerlink" title="结合一阶和二阶"></a>结合一阶和二阶</h2><ul>
<li>method1: 训完一阶训二阶，并将两者embedding concat</li>
<li>method2: 一起训</li>
</ul>
<h2 id="其他损失函数"><a href="#其他损失函数" class="headerlink" title="其他损失函数"></a>其他损失函数</h2><ul>
<li>pair loss: 用learn2rank的loss</li>
</ul>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="二阶相似度计算的负采样"><a href="#二阶相似度计算的负采样" class="headerlink" title="二阶相似度计算的负采样"></a>二阶相似度计算的负采样</h3><ul>
<li>对softmax的优化：负采样，<script type="math/tex">\sigma</script>为sigmoid函数，<script type="math/tex">P_{n}(v) \propto d_{v}^{3 / 4}</script>利用出度构成的节点权重做采样，与w2v分布一致<script type="math/tex; mode=display">
\log \sigma\left(\vec{u}_{j}^{\prime T} \cdot \vec{u}_{i}\right)+\sum_{i=1}^{K} E_{v_{n} \sim P_{n}(v)}\left[\log \sigma\left(-\vec{u}_{n}^{\prime T} \cdot \vec{u}_{i}\right)\right]</script></li>
</ul>
<h3 id="边采样"><a href="#边采样" class="headerlink" title="边采样"></a>边采样</h3><ul>
<li>O2的梯度与边权重wij有关，若学习率的设定由小权重决定，则遇到大权重时会梯度爆炸；反之则会梯度消失<script type="math/tex; mode=display">
\frac{\partial O_{2}}{\partial \vec{u}_{i}}=w_{i j} \cdot \frac{\partial \log p_{2}\left(v_{j} \mid v_{i}\right)}{\partial \vec{u}_{i}}</script></li>
<li>简单的优化方式：将权重为w的edge变为w条二进制edge —&gt; oom —&gt; edge sampling</li>
<li>采样算法：alias算法，可以达到O(1)复杂度</li>
</ul>
<h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><ul>
<li>新节点的GE（新节点）：<ul>
<li>新节点和已有节点相连：优化如下目标函数 <script type="math/tex">-\sum_{j \in N(i)} w_{j i} \log p_{1}\left(v_{j}, v_{i}\right), \text { or }-\sum_{j \in N(i)} w_{j i} \log p_{2}\left(v_{j} \mid v_{i}\right)</script></li>
<li>新节点不和已有节点相连：文中没给 —》利用side info</li>
</ul>
</li>
<li>低度数顶点（孤岛节点）：对于一些顶点由于其邻接点非常少会导致embedding向量的学习不充分，论文提到可以利用邻居的邻居构造样本进行学习，这里也暴露出LINE方法仅考虑一阶和二阶相似性，对高阶信息的利用不足。</li>
</ul>
<h1 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h1><ul>
<li>边权重矩阵和节点权重矩阵（pagerank/出度/centrality/clustering coefficient/…）的设计</li>
<li>不同的loss: KL/cross-entropy/rankloss/…</li>
<li>采样算法</li>
</ul>
<h1 id="Pros-amp-Cons"><a href="#Pros-amp-Cons" class="headerlink" title="Pros &amp; Cons"></a>Pros &amp; Cons</h1><ul>
<li>效率高</li>
<li>只用了一跳信息</li>
<li>对孤岛节点和新节点没有好的处理</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>LINE</tag>
      </tags>
  </entry>
  <entry>
    <title>Java学习资源</title>
    <url>/2021/12/10/Java%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90/</url>
    <content><![CDATA[<h2 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h2><ul>
<li><a href="https://github.com/apachecn/thinking-in-java-zh">Java编程思想</a></li>
<li><a href="https://github.com/Chenzk1/PrivateNotes/blob/master/books/Head%20First%20Java%28%E7%AC%AC2%E7%89%88%29.pdf">Head First Java</a></li>
<li><a href="https://github.com/Chenzk1/PrivateNotes/blob/master/books/Java%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%20%E5%8D%B71%20%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%20%E5%8E%9F%E4%B9%A6%E7%AC%AC9%E7%89%88%20%E5%AE%8C%E6%95%B4%E4%B8%AD%E6%96%87%E7%89%88%20.pdf">Java核心技术卷1</a></li>
</ul>
<h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><ul>
<li><a href="https://www.liaoxuefeng.com/wiki/1252599548343744">廖雪峰的Java教程</a></li>
</ul>
<h2 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h2><ul>
<li><a href="https://segmentfault.com/a/1190000014933213">Java学习的四个阶段</a></li>
<li><a href="https://juejin.cn/post/6844903985170612238">可能是最适合你的 Java 学习路线和方法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/115890802">2020年最新Java学习路线图</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Resource</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-LightGBM</title>
    <url>/2021/12/10/LightGBM/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>解决GBDT遇到海量数据时的问题：GBDT 在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。</p>
<a id="more"></a>
<h3 id="对xgboost的优化"><a href="#对xgboost的优化" class="headerlink" title="对xgboost的优化"></a>对xgboost的优化</h3><ul>
<li>基于 Histogram 的决策树算法</li>
<li>带深度限制的 Leaf-wise 的叶子生长策略</li>
<li>直方图做差加速</li>
<li>直接支持类别特征(Categorical Feature)</li>
<li>Cache 命中率优化</li>
<li>基于直方图的稀疏特征优化</li>
<li>多线程优化</li>
</ul>
<h4 id="基于-Histogram-的决策树算法"><a href="#基于-Histogram-的决策树算法" class="headerlink" title="基于 Histogram 的决策树算法"></a>基于 Histogram 的决策树算法</h4><ul>
<li>做法：把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</li>
<li>优点<ul>
<li>内存消耗小：不需要保存既保存数据的特征值，又保存预排序结果，只保存k个key-values即可</li>
<li>时间消耗小：只需要计算k次增益</li>
<li>泛化性能增强：决策树是弱模型，分割点精确与否不是很重要，更粗的分割点还可以引入噪音，增强泛化性能，防止过拟合</li>
</ul>
</li>
</ul>
<h4 id="带深度限制的-Leaf-wise-的叶子生长策略"><a href="#带深度限制的-Leaf-wise-的叶子生长策略" class="headerlink" title="带深度限制的 Leaf-wise 的叶子生长策略"></a>带深度限制的 Leaf-wise 的叶子生长策略</h4><ul>
<li>Level-wise 过一次数据可以<strong>同时分裂同一层的叶子</strong>，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</li>
<li>Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到<strong>分裂增益最大的一个叶子</strong>，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</li>
</ul>
<h4 id="直方图加速"><a href="#直方图加速" class="headerlink" title="直方图加速"></a>直方图加速</h4><ul>
<li>一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图。加速一倍。</li>
</ul>
<h4 id="接受categorical-features"><a href="#接受categorical-features" class="headerlink" title="接受categorical features"></a>接受categorical features</h4><p>LightGBM 可以直接使用 categorical features（分类特征）作为 input（输入）. 它不需要被转换成 one-hot coding（独热编码）, 并且它比 one-hot coding（独热编码）更快（约快上 8 倍）。</p>
<h4 id="并行学习"><a href="#并行学习" class="headerlink" title="并行学习"></a>并行学习</h4><p>目前支持特征并行和数据并行的两种。</p>
<ul>
<li>特征并行：主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。</li>
<li>数据并行：让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。</li>
</ul>
<h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><pre><code>params = &#123;  
    &#39;boosting_type&#39;: &#39;gbdt&#39;,  
    &#39;objective&#39;: &#39;binary&#39;,  
    &#39;metric&#39;: &#123;&#39;binary_logloss&#39;, &#39;auc&#39;&#125;,  #二进制对数损失
    &#39;num_leaves&#39;: 5,  
    &#39;max_depth&#39;: 6,  
    &#39;min_data_in_leaf&#39;: 450,  
    &#39;learning_rate&#39;: 0.1,  
    &#39;feature_fraction&#39;: 0.9,  
    &#39;bagging_fraction&#39;: 0.95,  
    &#39;bagging_freq&#39;: 5,  
    &#39;lambda_l1&#39;: 1,    
    &#39;lambda_l2&#39;: 0.001,  # 越小l2正则程度越高  
    &#39;min_gain_to_split&#39;: 0.2,  
    &#39;verbose&#39;: 5,  
    &#39;is_unbalance&#39;: True  
&#125;  
</code></pre>]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>LightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Decision Tree</title>
    <url>/2021/12/10/ML-Decision%20Tree/</url>
    <content><![CDATA[<p>[TOC]</p>
<ol>
<li>决策树</li>
</ol>
<p><strong>问题：如何挑选用于分裂节点的特征—&gt;ID3 C4.5 …(一个标准：使分裂出来的节点尽可能纯，即一个分支尽可能属于同类)</strong></p>
<a id="more"></a>
<ol>
<li><p>ID3<br><strong>信息增益</strong></p>
<p> 信息增益 = 信息熵 - 条件熵</p>
<ul>
<li>信息增益：针对每个 <strong>属性</strong></li>
<li>信息熵：整个样本空间的不确定度。其中Pk一定是label取值的概率。</li>
<li><p>条件熵：给定某个属性，求其信息熵</p>
<p>—&gt; 问题：某属性所包括的类别越多，信息增益越大。极限：每个类别仅有1个实例（label数量为1），log p = log1 = 0， 所以最终条件熵=0。或：属性类别越多，条件熵越小，其纯度越高。</p>
<p>—&gt; 信息增益准则其实是对可取值数目较多的属性有所偏好！</p>
<p>—&gt; 泛化能力不强</p>
</li>
</ul>
</li>
<li><p>C4.5 <strong>信息增益率+信息增益</strong></p>
<p> 属性a的信息增益率 = 属性a的信息增益 / a的某个固有统计量IV(a)</p>
<p> <img src="https://pic4.zhimg.com/80/v2-812104c0291d20935e910919a9fa5c27_hd.png" alt="IV(a)公式"></p>
<p> V为a的取值数目。<br> （实际上是属性a的信息熵）</p>
<ul>
<li>直接使用信息增益率：偏好取值数目小的属性。</li>
<li>先选择高于平均水平信息增益的属性，再选择最高信息增益率的属性。</li>
</ul>
</li>
<li><p>CART <strong>基尼系数+MAE/MSE</strong></p>
<p>与ID3、C4.5的不同：形成二叉树，因此 —&gt; 既要确定要分割的属性，也要确定要分割的值</p>
<ul>
<li><p>回归树：MAE/MSE</p>
<ul>
<li>example(MSE)：<blockquote>
<ol>
<li>考虑数据集 D 上的所有特征 j，遍历每一个特征下所有可能的取值或者切分点 s，将数据集 D 划分成两部分 D1 和 D2</li>
<li>分别计算上述两个子集的平方误差和，选择最小的平方误差对应的特征与分割点，生成两个子节点。</li>
<li>对上述两个子节点递归调用步骤1 2,直到满足停止条件。</li>
</ol>
</blockquote>
</li>
</ul>
</li>
<li><p>分类树：(Gini不纯度)</p>
<p><img src="https://img-blog.csdn.net/20150109184544578?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQW5kcm9pZGx1c2hhbmdkZXJlbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="某属性A的基尼不纯度"><br>基尼不纯度越小，纯度越高</p>
</li>
</ul>
<blockquote>
<ol>
<li>对每个特征 A，对它的所有可能取值 a，将数据集分为 A＝a，和 A!＝a 两个子集，计算集合 D 的基尼指数：<br>Gini(A) = D1/D <em> Gini(D1) + D2/D </em> Gini(D2)</li>
<li>遍历所有的特征 A，计算其所有可能取值 a 的基尼指数，选择 D 的基尼指数最小值对应的特征及切分点作为最优的划分，将数据分为两个子集。</li>
<li>对上述两个子节点递归调用步骤1 2, 直到满足停止条件。</li>
<li>生成 CART 决策树。</li>
</ol>
</blockquote>
<pre><code>停止条件有：
1. 节点中的样本个数小于预定阈值;
2. 样本集的Gini系数小于预定阈值（此时样本基本属于同一类）;
3. 没有更多特征。
</code></pre><ul>
<li>剪枝</li>
<li>例子：<a href="https://www.jianshu.com/p/b90a9ce05b28">example</a></li>
</ul>
</li>
<li><p>控制决策树过拟合的方法</p>
<ul>
<li>剪枝</li>
<li>控制终止条件，避免树形结构过细</li>
<li>构建随机森林</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DecisionTree</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Metrics</title>
    <url>/2021/12/10/ML-metrics/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]</p>
<h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><h2 id="ACC-Accuracy"><a href="#ACC-Accuracy" class="headerlink" title="ACC(Accuracy)"></a>ACC(Accuracy)</h2><ul>
<li>Accuracy = 预测正确的样本数量 / 总样本数量 = (TP+TN) / (TP+FP+TN+FN)</li>
</ul>
<a id="more"></a>
<h2 id="Precision查准"><a href="#Precision查准" class="headerlink" title="Precision查准"></a>Precision查准</h2><ul>
<li>Precision = TP / (TP+FP)</li>
</ul>
<h2 id="Recall-TPR查全"><a href="#Recall-TPR查全" class="headerlink" title="Recall/TPR查全"></a>Recall/TPR查全</h2><ul>
<li>Recall = TPR = TP / (TP+FN)</li>
</ul>
<h2 id="F1-amp-Fn"><a href="#F1-amp-Fn" class="headerlink" title="F1&amp;Fn"></a>F1&amp;Fn</h2><h3 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h3><ul>
<li>precision和recall的调和平均</li>
</ul>
<h3 id="Fn"><a href="#Fn" class="headerlink" title="Fn"></a>Fn</h3><ul>
<li>可用来解决分类不均衡问题/对precision和recall进行强调<ul>
<li>如F1认为precision和recall同等重要，F2认为recall的重要程度是precision的2倍<script type="math/tex; mode=display">
F_{\beta}=\left(1+\beta^{2}\right) \cdot \frac{\text { precision } \cdot \text { recall }}{\left(\beta^{2} \cdot \text { precision }\right)+\text { recall }}</script></li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
F_{\beta}=\frac{\left(1+\beta^{2}\right) \cdot \text { true positive }}{\left(1+\beta^{2}\right) \cdot \text { true positive }+\beta^{2} \cdot \text { false negative }+\text { false positive }}</script><h2 id="P-R曲线-Precision-Recall"><a href="#P-R曲线-Precision-Recall" class="headerlink" title="P-R曲线(Precision-Recall)"></a>P-R曲线(Precision-Recall)</h2><ul>
<li>P为纵轴，R为横轴</li>
<li>主要关心正例</li>
<li>公式：<script type="math/tex; mode=display">
\sum_{n}\left(R_{n}-R_{n-1}\right) P_{n}</script></li>
</ul>
<h2 id="FPR"><a href="#FPR" class="headerlink" title="FPR"></a>FPR</h2><ul>
<li>FPR = FP/(FP+TN) 真实label为0的样本中，被预测为1的样本占的比例</li>
</ul>
<h2 id="ROC曲线-amp-AUC"><a href="#ROC曲线-amp-AUC" class="headerlink" title="ROC曲线&amp;AUC"></a>ROC曲线&amp;AUC</h2><ul>
<li>衡量分类准确性，同时考虑了模型对正样本(TPR)和负样本(FPR)的分类能力，因此在样本非均衡的情况下也能做出合理的评价。侧重于<strong>排序</strong>。</li>
</ul>
<h3 id="ROC-Receiver-operating-characteristic-curve"><a href="#ROC-Receiver-operating-characteristic-curve" class="headerlink" title="ROC(Receiver operating characteristic curve)"></a>ROC(Receiver operating characteristic curve)</h3><ul>
<li>横轴FPR，纵轴TPR</li>
<li>ROC曲线在绘制时，需要先对所有样本的预测概率做排序，并不断取不同的阈值计算TPR和FPR，因此AUC在计算时会侧重于排序。</li>
</ul>
<h3 id="AUC-Area-under-Curve"><a href="#AUC-Area-under-Curve" class="headerlink" title="AUC(Area under Curve)"></a>AUC(Area under Curve)</h3><ul>
<li>ROC曲线与x轴的面积</li>
<li>一般认为：AUC最小值=0.5（其实存在AUC小于0.5的情况，例如每次都预测与真实值相反的情况，但是这种情况，只要把预测值取反就可以得到大于0.5的值，因此还是认为AUC最小值=0.5）</li>
<li>AUC的物理意义为<strong>任取一对例和负例，正例得分大于负例得分的概率</strong>，AUC越大，表明方法效果越好。—&gt; 排序<script type="math/tex; mode=display">A U C=\frac{\sum p r e d_{p o s}>p r e d_{n e g}}{p o s i t i v e N u m * n e g a ti v e N u m}</script>分母是正负样本总的组合数，分子是正样本大于负样本的组合数<script type="math/tex; mode=display">
A U C=\frac{\sum_{\text {ins}_{i} \in \text {positiveclass}} \operatorname{rank}_{\text {ins}_{i}}-\frac{M \times(M+1)}{2}}{M \times N}</script></li>
</ul>
<h1 id="区分度"><a href="#区分度" class="headerlink" title="区分度"></a>区分度</h1><h2 id="KS-Kolmogorov-Smirnov"><a href="#KS-Kolmogorov-Smirnov" class="headerlink" title="KS(Kolmogorov-Smirnov)"></a>KS(Kolmogorov-Smirnov)</h2><ul>
<li>KS用于模型风险区分能力进行评估，指标衡量的是好坏样本累计分布之间的差值。好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。</li>
<li>KS的计算步骤如下： <ul>
<li>1)计算每个评分区间的好坏账户数。 </li>
<li>2)计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 </li>
<li>3)计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。</li>
</ul>
</li>
<li>低分段累计坏百分比应高于累计好百分比，之后会经历两者差距先扩大再缩小的变化<br><img src="https://img-blog.csdnimg.cn/2019013111150139.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NzY2NfbGVhcm5pbmc=,size_16,color_FFFFFF,t_70" alt="KS-曲线"></li>
</ul>
<h2 id="gini"><a href="#gini" class="headerlink" title="gini"></a>gini</h2><ul>
<li>计算每个评分区间的好坏账户数。 </li>
<li>计算每个评分区间的累计好账户数占总好账户数比率（累计good%）和累计坏账户数占总坏账户数比率(累计bad%)。 </li>
<li>按照累计好账户占比和累计坏账户占比得出下图所示曲线ADC。 </li>
<li>计算出图中阴影部分面积，阴影面积占直角三角形ABC面积的百分比，即为GINI系数。</li>
</ul>
<h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><ul>
<li><a href="http://note.youdao.com/s/D64Y5VaG">http://note.youdao.com/s/D64Y5VaG</a></li>
<li>map和ndcg都是<strong>属于per item的评估</strong>，即逐条对搜索结果进行分等级的打分，并计算其指标。</li>
<li><strong>基于precision的指标，例如MAP，每个条目的值是的评价用0或1表示；而DCG可以使用多值指标来评价</strong>。</li>
<li><strong>基于precision的指标，天然考虑了排序信息</strong>。</li>
</ul>
<h2 id="map-k"><a href="#map-k" class="headerlink" title="map@k"></a>map@k</h2><h3 id="prec-k"><a href="#prec-k" class="headerlink" title="prec@k"></a>prec@k</h3><ul>
<li>k指到第k个正确的召回结果，precision的值<script type="math/tex; mode=display">P @ k(\pi, l)=\frac{\left.\sum_{t \leq k} I_{\left\{l_{\pi}-1_{(t)}\right.}=1\right\}}{k}</script><ul>
<li>这里$\pi$代表documents list，即推送结果列。$I$是指示函数，$\pi^{(-1)}(t)$代表排在位置$t$处的document的标签（相关为1，否则为0）。这一项可以理解为前k个documents中，标签为1的documents个数与k的比值。</li>
</ul>
</li>
<li>只能表示单点的策略效果</li>
</ul>
<h3 id="ap-k"><a href="#ap-k" class="headerlink" title="ap@k"></a>ap@k</h3><script type="math/tex; mode=display">
\mathrm{AP}(\pi, l)=\frac{\left.\sum_{k=1}^{m} P @ k \cdot I_{\left\{l_{\pi}-1(k)\right.}=1\right\}}{m_{1}}</script><ul>
<li>其中$m_1$代表与该query相关的document的数量（即真实标签为1），$m$则代表模型找出的前$m$个documents，本例中 [公式] ，并假设 [公式] ，即真正和query相关的document有6个。（但是被模型找出来的7个doc中仅仅有3个标签为1，说明这个模型召回并不怎么样）</li>
<li>@1到@k的precision的平均</li>
</ul>
<h3 id="map-k-1"><a href="#map-k-1" class="headerlink" title="map@k"></a>map@k</h3><ul>
<li>多个query的ap@k平均</li>
</ul>
<h2 id="ndcg"><a href="#ndcg" class="headerlink" title="ndcg"></a>ndcg</h2><ul>
<li>基于CG的评价指标允许我们使用多值评价一个item：例如对一个结果可评价为Good（好）、Fair（一般）、Bad（差），然后可以赋予为3、2、1.</li>
</ul>
<h3 id="cg-Cumulative-Gain"><a href="#cg-Cumulative-Gain" class="headerlink" title="cg(Cumulative Gain)"></a>cg(Cumulative Gain)</h3><script type="math/tex; mode=display">
\mathrm{CG}_{\mathrm{p}}=\sum_{i=1}^{p} r e l_{i}</script><ul>
<li>CG是在这个查询输出结果里面所有的结果的等级对应的得分的总和。如一个输出结果页面有P个结果.</li>
</ul>
<h3 id="dcg-Discounted-Cumulative-Gain"><a href="#dcg-Discounted-Cumulative-Gain" class="headerlink" title="dcg(Discounted Cumulative Gain)"></a>dcg(Discounted Cumulative Gain)</h3><script type="math/tex; mode=display">
\mathrm{DCG}_{\mathrm{p}}=\sum_{i=1}^{p} \frac{r e l_{i}}{\log _{2}(i+1)}</script><ul>
<li>取对数是因为：根据大量的用户点击与其所点内容的位置信息，模拟出一条衰减的曲线。</li>
</ul>
<h3 id="ndcg-normalize-DCG"><a href="#ndcg-normalize-DCG" class="headerlink" title="ndcg(normalize DCG)"></a>ndcg(normalize DCG)</h3><script type="math/tex; mode=display">nDCG = \frac{DCG}{IDCG}</script><ul>
<li>IDCG（Ideal DCG）就是理想的DCG。</li>
<li>IDCG如何计算？首先要拿到搜索的结果，然后对这些结果进行排序，排到最好的状态后，算出这个排列下的DCG，就是iDCG。因此nDCG是一个0-1的值，nDCG越靠近1，说明策略效果越好，或者说只要nDCG&lt;1，策略就存在优化调整空间。因为nDCG是一个相对比值，那么不同的搜索结果之间就可以通过比较nDCG来决定哪个排序比较好。</li>
</ul>
<h2 id="MRR-Mean-Reciprocal-Rank"><a href="#MRR-Mean-Reciprocal-Rank" class="headerlink" title="MRR(Mean Reciprocal Rank)"></a>MRR(Mean Reciprocal Rank)</h2><script type="math/tex; mode=display">
\operatorname{MRR}=\frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\operatorname{rank}_{i}}</script><ul>
<li>其中|Q|是查询个数，$rank_i$是第i个查询，第一个相关的结果所在的排列位置</li>
</ul>
<h1 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h1><ul>
<li>评价一个检测算法时，主要看两个指标<ul>
<li>是否正确的预测了框内物体的类别</li>
<li>预测的框和人工标注框的重合程度</li>
</ul>
</li>
</ul>
<h2 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h2><ul>
<li>求取每个<strong>类别</strong>的ap的平均值<ul>
<li>即，对每个类别进行预测，得到其预测值的排序，求ap</li>
<li>求map</li>
</ul>
</li>
</ul>
<h2 id="IOU-Interest-Over-Union"><a href="#IOU-Interest-Over-Union" class="headerlink" title="IOU(Interest Over Union)"></a>IOU(Interest Over Union)</h2><h3 id="检测里的IOU"><a href="#检测里的IOU" class="headerlink" title="检测里的IOU"></a>检测里的IOU</h3><ul>
<li>框的IOU<script type="math/tex; mode=display">IOU = \frac{ {DetectionResult}\cap{GroundTruth}} { {DetectionResult}\cup{GroundTruth} }</script><img src="https://pic3.zhimg.com/80/v2-99faeb1f9876f11a32f90263ff1cafba_1440w.jpg" alt="示意"></li>
</ul>
<h3 id="语义分割里的IOU"><a href="#语义分割里的IOU" class="headerlink" title="语义分割里的IOU"></a>语义分割里的IOU</h3><ul>
<li>像素集合的IOU<script type="math/tex; mode=display">
\begin{array}{c}
I O U=\frac{p_{i i}}{\sum_{j=0}^{k} p_{i j}+\sum_{j=0}^{k} p_{j i}-p_{i i}} \\
I o U=\frac{T P}{F N+F P+T P}
\end{array}</script></li>
<li>$p<em>{ij}$表示真实值为i，被预测为j的数量， K+1是类别个数（包含空类）。$p</em>{ii}$是真正的数量。$p<em>{ij}$、$p</em>{ji}$则分别表示假正和假负。</li>
</ul>
<h2 id="mIOU-mean-IOU"><a href="#mIOU-mean-IOU" class="headerlink" title="mIOU(mean IOU)"></a>mIOU(mean IOU)</h2><ul>
<li><a href="https://blog.csdn.net/baidu_27643275/article/details/90445422">blog</a></li>
<li>用于语义分割<script type="math/tex; mode=display">
\begin{array}{c}
M I O U=\frac{1}{k+1} \sum_{i=0}^{k} \frac{p_{i i}}{\sum_{j=0}^{k} p_{i j}+\sum_{j=0}^{k} p_{j i}-p_{i i}} \\
M I o U=\frac{1}{k+1} \sum_{i=0}^{k} \frac{T P}{F N+F P+T P}
\end{array}</script></li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>ML</tag>
        <tag>Metrics</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-信息熵&amp;交叉熵&amp;条件熵&amp;相对熵(KL散度)&amp;互信息</title>
    <url>/2021/12/10/ML-%E4%BF%A1%E6%81%AF%E7%86%B5&amp;%E4%BA%A4%E5%8F%89%E7%86%B5&amp;%E6%9D%A1%E4%BB%B6%E7%86%B5&amp;%E7%9B%B8%E5%AF%B9%E7%86%B5(KL%E6%95%A3%E5%BA%A6)&amp;%E4%BA%92%E4%BF%A1%E6%81%AF/</url>
    <content><![CDATA[<ul>
<li>设随机变量X，有n个事件$x_i$ –&gt; $x_n$，概率分布为$p(x)$</li>
</ul>
<a id="more"></a>
<h2 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h2><ul>
<li>某随机变量X取值为xi的信息为 $I(X=xi)=log_2( 1 / p(x_i) ) = - log_2p(x_i)$</li>
<li>某事件xi的信息代表这个事件能提供的信息，一个发生概率越小的事件能够提供的信息量越大。</li>
</ul>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><ul>
<li>信息代表一个事件的不确定性，信息熵是整个随机变量X不确定性的度量：信息的期望。<script type="math/tex; mode=display">H(X) = \sum_0^n p(x_i)*I(x_i) = - \sum_0^n p(x_i)\log_2(p(x_i))</script></li>
<li>信息熵只与变量X的分布有关，与其取值无关。例如二分类中，两取值的概率均为0.5时，其熵最大，也最难预测某时刻哪一类别会发生。</li>
<li><a href="https://www.zhihu.com/question/41252833/answer/195901726">如何通俗的解释交叉熵与相对熵?</a></li>
<li>对于一个系统而言，若获知其真实分布，则我们能够找到一个最优策略，以最小的代价来消除系统的不确定性，而这个最小的代价（猜题次数、编码长度等）就是信息熵。</li>
</ul>
<h2 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h2><ul>
<li>给定条件X下，Y的分布（Y|X）的熵对X的数学期望：$H(Y|X)=\sum_x p(x)H(Y|X=x)$, 即给定条件分布下的信息熵对某个条件的数学期望</li>
<li>在ML中，即选定某个特征X(X有n类)后，label(Y)的条件概率熵求期望：给定X特征的条件下Y的信息熵。</li>
<li>条件熵越小，代表在这个特征下，label的信息熵越小，也就是说要解决问题的代价越小。</li>
</ul>
<h2 id="信息增益-ID3"><a href="#信息增益-ID3" class="headerlink" title="信息增益 - ID3"></a>信息增益 - ID3</h2><ul>
<li>$IG(Y|X)=H(Y)-H(Y|X)$ 信息熵-条件熵</li>
<li>在决策树中作为选择特征的指标，IG越大，这个特征的选择性越好，也可以理解为：待分类的集合的熵和选定某个特征的条件熵之差越大，这个特征对整个集合的影响越大。</li>
<li>对于条件熵来说，条件熵越小，分类后的纯度越高，但是问题是：X的取值越多，每个取值下Y的纯度越高，H(Y|X)越小，但此时并不有利于Y的区分。信息增益也是如此。–&gt; 信息增益率。</li>
</ul>
<h2 id="信息增益率-信息增益比-—-C4-5"><a href="#信息增益率-信息增益比-—-C4-5" class="headerlink" title="信息增益率/信息增益比 — C4.5"></a>信息增益率/信息增益比 — C4.5</h2><ul>
<li>信息增益 / 条件的信息熵</li>
<li>偏好取值少的特征。C4.5：先选择高于平均水平信息增益的特征，再在其中选择最高信息增益率的特征。见Decision Tree</li>
</ul>
<h2 id="基尼不纯度-—-CART"><a href="#基尼不纯度-—-CART" class="headerlink" title="基尼不纯度 — CART"></a>基尼不纯度 — CART</h2><ul>
<li>GINI：先对特征分区，对每个区中的label求gini系数，pk(1-pk)；然后对每个区的基尼系数求期望</li>
<li>表示数据的不纯度。既有分类也有回归，既要确定特征，也要确定特征的分叉值。</li>
<li>见<a href="https://chenzk1.github.io/2020/10/25/ML-Decision%20Tree/">Decision Tree</a></li>
</ul>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><ul>
<li>前面提到：信息熵是最优策略下，消除系统不确定性的最小代价。这里的前提是：我们得到了系统的真实分布。</li>
<li>实际中，一般难以获知系统真实分布，所以要以假设分布去近似。交叉熵：用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。$CEH(p,q)=\sum_{k=1}^np_k\log_2\frac{1}{q_k}$，注意这里log中是q，是基于非真实分布q的信息量对真实分布的期望。<br>当假设分布$q_k$与真实分布$p_k$相同时，交叉熵最低，等于信息熵，所以得到的策略为最优策略。</li>
<li>在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。<ul>
<li>例如：在逻辑斯蒂回归或者神经网络中都有用到交叉熵作为评价指标，其中p即为真实分布的概率，而q为预测的分布，以此衡量两不同分布的相似性。</li>
</ul>
</li>
<li>如何衡量不同策略的差异：相对熵</li>
</ul>
<h2 id="相对熵-K-L散度"><a href="#相对熵-K-L散度" class="headerlink" title="相对熵/K-L散度"></a>相对熵/K-L散度</h2><ul>
<li>用来衡量两个概率分布之间的差异。两者相同相对熵为0</li>
<li>使用非真实分布q的交叉熵，与使用真实分布p的的信息熵的差值：相对熵，又称K-L散度。</li>
<li>KL(p, q) = 交叉熵(p, q) - 信息熵(p)<br>$KL(p,q)=CEH(p,q)-H(p)=\sum_{i=1}^np(x_i)\log\frac{p(x_i)}{q(x_i)}$</li>
</ul>
<h2 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h2><ul>
<li>H(X,Y) 随机变量X,Y联合表示的信息熵</li>
</ul>
<h2 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h2><ul>
<li>H（X；Y）俩变量交集，也记作I(X;Y)</li>
<li>H（X；Y) = H(X,Y)-H(Y|X)-H(X|Y)</li>
<li>I(X;Y)=KL(P(X,Y), P(X)P(Y))</li>
<li>互信息越小，两变量独立性越强，P(X,Y)与P(X)P(Y)差异越小，P(X,Y)与P(X)P(Y)的相对熵越小</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>信息论</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Naive Bayes及其sklearn实现</title>
    <url>/2021/12/10/Naive%20Bayes/</url>
    <content><![CDATA[<p>[TOC]</p>
<p>P(B|A) = P(A|B)*P(B)/P(A)</p>
<p>朴素：特征之间相互独立</p>
<h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><ol>
<li>x = {a1, a2, …, am}为待分类项，a是特征。</li>
<li>类别集合C = {y1, …, yn}.</li>
<li>计算P(y1|x), P(y2|x) …</li>
<li>P(yk|x) = max{P(yi|x)}，则x属于yk类</li>
</ol>
<a id="more"></a>
<p><strong>总结：</strong>某类在待分类项出现的条件下的概率是所有类中最大的，这个分类项就属于这一类。</p>
<p>e.g.判断一个黑人来自哪个洲，求取每个洲黑人的比率，非洲最高，选非洲。</p>
<p>其中x = {a1, a2, …, am}，即P(C|a1,a2…) = P(C)*P(a1,a2,…|C)/P(a1,a2…)。posterior = prior * likelihood / evidence, 这里evidence是常数，不影响。</p>
<p>——-&gt;求解P(C) * P(a1,a2,a3…|C)</p>
<p>——-&gt;链式法则：P(C) * P(a2,a3…|C, a1) * P(a1|C)</p>
<p>—-&gt; …</p>
<p>—-&gt; P(C) * P(a1|C) * P(a2|C, a1) * P(a3|C, a1, a2)…<br>由于特征之间的相互独立性，a2发生于a1无关，转化为</p>
<p>—-&gt; P(C) * P(a1|C) * P(a2|C) …  * P(am|C)</p>
<p>——-&gt;问题转化为求取条件概率：</p>
<ol>
<li>找到一个已知分类的待分类项集合，这个集合叫做训练样本集。</li>
<li>统计得到在各类别下各个特征属性的条件概率估计。</li>
</ol>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>NaiveBayes</tag>
      </tags>
  </entry>
  <entry>
    <title>P、NP、NP-hard问题</title>
    <url>/2021/12/15/P&amp;NP&amp;NP-hard/</url>
    <content><![CDATA[<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><ul>
<li>P Problem: <strong>任意性</strong>，对于<strong>任意的</strong>输入规模n，问题都可以在n的多项式时间内得到解决；</li>
<li>NP(Non-deterministic Polynomial) Problem: <strong>存在性</strong>，<strong>可以</strong>在多项式的时间里验证一个解的问题；</li>
<li>NPC(Non-deterministic Polynomial Complete) Problem: 满足两个条件 (1)是一个NP问题 (2)所有的NP问题都可以约化到它。可以理解为<strong>NP的泛化问题</strong>。</li>
<li>NP-Hard Problem: 满足NPC问题的第二条，但不一定要满足第一条 —&gt; <strong>不一定可以在多项式时间内解决的问题</strong></li>
</ul>
<h1 id="搞笑版P-NP证明"><a href="#搞笑版P-NP证明" class="headerlink" title="搞笑版P=NP证明"></a>搞笑版P=NP证明</h1><blockquote>
<p>反证法。设P = NP。令y为一个P = NP的证明。证明y可以用一个合格的计算机科学家在多项式时间内验证，我们认定这样的科学家的存在性为真。但是，因为P = NP，该证明y可以在多项式时间内由这样的科学家发现。但是这样的发现还没有发生（虽然这样的科学家试图发现这样的一个证明），我们得到了矛盾。</p>
</blockquote>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Python-sort&amp;sorted</title>
    <url>/2021/12/10/Python-sort&amp;sorted%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="不同"><a href="#不同" class="headerlink" title="不同"></a>不同</h2><ul>
<li>sorted<ul>
<li>返回已排序<strong>列表</strong></li>
<li>built-in函数，接受任何可迭代对象</li>
</ul>
</li>
<li>sort<ul>
<li>原位排序，返回None</li>
<li>是list的成员函数，因此只接受list</li>
</ul>
</li>
</ul>
<h2 id="相同"><a href="#相同" class="headerlink" title="相同"></a>相同</h2><ul>
<li>稳定排序</li>
</ul>
<a id="more"></a>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li>key：接受一个参数，返回用于排序的<strong>键</strong><ul>
<li>operator模块函数：一些常用key函数的封装<ul>
<li>itemgetter()：适用tuple/list的list</li>
<li>attrgetter()：适用dict/object的list</li>
<li>methodcaller()</li>
</ul>
</li>
</ul>
</li>
<li>reverse</li>
</ul>
<h3 id="自定义排序规则"><a href="#自定义排序规则" class="headerlink" title="自定义排序规则"></a>自定义排序规则</h3><h4 id="Python2-cmp函数"><a href="#Python2-cmp函数" class="headerlink" title="Python2: cmp函数"></a>Python2: cmp函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cmp</span>(<span class="params">a, b</span>):</span></span><br><span class="line">  <span class="comment"># 如果逻辑上认为 a &lt; b ，返回 -1</span></span><br><span class="line">  <span class="comment"># 如果逻辑上认为 a &gt; b , 返回 1</span></span><br><span class="line">  <span class="comment"># 如果逻辑上认为 a == b, 返回 0 </span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">a = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">a = <span class="built_in">sorted</span>(a, cmp=cmp)</span><br></pre></td></tr></table></figure>
<h4 id="python3：无cmp，只存key。两种自定义排序规则的做法。"><a href="#python3：无cmp，只存key。两种自定义排序规则的做法。" class="headerlink" title="python3：无cmp，只存key。两种自定义排序规则的做法。"></a>python3：无cmp，只存key。两种自定义排序规则的做法。</h4><ul>
<li>做法1：利用functools module里封装的cmp_to_key</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cmp_to_key</span>(<span class="params">mycmp</span>):</span></span><br><span class="line">    <span class="string">&#x27;Convert a cmp= function into a key= function&#x27;</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">K</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, obj, *args</span>):</span></span><br><span class="line">            self.obj = obj</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) &lt; <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__gt__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) &gt; <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) == <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__le__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) &lt;= <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__ge__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) &gt;= <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__ne__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">            <span class="keyword">return</span> mycmp(self.obj, other.obj) != <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> K</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cmp</span>(<span class="params">a, b</span>):</span></span><br><span class="line">    <span class="keyword">if</span> b &lt; a:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">if</span> a &lt; b:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>]</span><br><span class="line">print(<span class="built_in">sorted</span>(a, key=functools.cmp_to_key(cmp)))</span><br></pre></td></tr></table></figure>
<ul>
<li>做法2：自己实现cmp类</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LargerNumKey</span>(<span class="params"><span class="built_in">str</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span>(<span class="params">x, y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x+y &gt; y+x</span><br><span class="line"></span><br><span class="line">largest_num = <span class="string">&#x27;&#x27;</span>.join(<span class="built_in">sorted</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, nums), key=LargerNumKey))</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python-面向对象</title>
    <url>/2021/12/10/Python-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
    <content><![CDATA[<h3 id="属性命名"><a href="#属性命名" class="headerlink" title="属性命名"></a>属性命名</h3><ul>
<li>属性以双下划线开头，类内变量，实例无法访问。但可以通过某些方式访问，例如Student例中定义了__name变量，可以用_Student_name来实现访问，但不建议，因为不同的解释器的转化方式不一样。</li>
<li>单下划线可以打开，但需要注意不能随意更改。</li>
<li>双下划线结尾与开头，特殊变量，类内可以访问，实例不知。</li>
</ul>
<a id="more"></a>
<h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3><p>开闭原则：定义一个类Animal及其多个之类Dog/Cat/…，当定义一个函数或操作时：</p>
<ul>
<li>对扩展开放：允许新增Animal的子类；</li>
<li>对修改封闭：不需要修改依赖Animal类型的run_twice()等函数，仍然可以传入Dog/Cat等类。<br>事实上，不需要继承也可以实现多态————鸭子类型。</li>
</ul>
<h3 id="若干方法"><a href="#若干方法" class="headerlink" title="若干方法"></a>若干方法</h3><ul>
<li>isinstance(object,class) 判断是否属于某个类</li>
<li>dir() 列举出一个对象的属性和方法</li>
<li>getattr()、setattr()、hasattr()可以获得、添加、查询是否需要某个属性<ul>
<li>__slots__ 限制可以添加的属性，__slots__ = (‘name’, ‘age’) # 用tuple定义允许绑定的属性名称</li>
</ul>
</li>
<li>装饰器</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-XGBoost</title>
    <url>/2021/12/10/XGBoost/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[toc]<br><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Doc</a><br><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">Slides</a></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p><img src="http://i.imgur.com/L7PhJwO.png" alt="Summary"><br><img src="https://img-blog.csdnimg.cn/2019031119461897.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI0ODUyNDM5,size_16,color_FFFFFF,t_70" alt="Summary"></p>
<a id="more"></a>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul>
<li>每轮生成新的树，该树的生成标准为：损失函数最小化。损失函数代表的意义：保证模型复杂度越低的同时，使预测误差尽可能小。其中模型复杂度包括树的个数以及叶子数值尽可能不极端。<em>（这个怎么看，如果某个样本label数值为4，那么第一个回归树预测3，第二个预测为1；另外一组回归树，一个预测2，一个预测2，那么倾向后一种，为什么呢？前一种情况，第一棵树学的太多，太接近4，也就意味着有较大的过拟合的风险）</em></li>
<li>每一轮产生新的树的时候，其loss是本轮加之前模型与y的差，然后通过泰勒展开做成<strong>t-1轮损失函数对于t-1轮模型即上一轮残差</strong>的一阶导和二阶导与当前树及其平方的乘积</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><script type="math/tex; mode=display">\text{obj} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i)</script><p>其中l为loss，l为二次则利用二次优化，不是二次则用泰勒展开；Omega为正则项。</p>
<ul>
<li><p>loss：采用加法策略，第t颗树时：</p>
<script type="math/tex; mode=display">\hat{y}_i^{(0)} = 0</script><script type="math/tex; mode=display">\hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)</script><script type="math/tex; mode=display">\dots</script><script type="math/tex; mode=display">\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)</script><p>所以在添加第t颗树时，需要优化的目标函数为：</p>
<script type="math/tex; mode=display">l(y_i, \hat{y}_i^{(t)}) = l(y_i, \hat{y}_i^{(t-1)} + f_{t-1}(x_i)) = \sum_{i=1}^n [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)</script><p>其中g和h分别为t-1轮损失函数对于t-1轮模型的一阶导和二阶导：</p>
<script type="math/tex; mode=display">g_i = \partial_{\hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)})</script><script type="math/tex; mode=display">h_i = \partial_{\hat{y}_i^{(t-1)}}^2 l(y_i, \hat{y}_i^{(t-1)})</script><p>note: 是对谁的导</p>
<p>此处为了简化目标函数，用到了泰勒二阶展开：<br><img src="https://img-blog.csdnimg.cn/20190311165107976.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI0ODUyNDM5,size_16,color_FFFFFF,t_70" alt="Summary"><br>loss第一项是真实值与t-1轮预测结果的loss，对t轮不明显，因此删去。所以对于一个模型来说，在每一次优化时，先定义好loss函数，然后计算每一轮loss的一阶导和二阶导即可写出loss，优化即可（每个特征求最佳分裂点，使用最小的特征）。</p>
</li>
<li>正则项：复杂度：<script type="math/tex; mode=display">\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2</script>其中w是叶子上的score vector，T是叶子数量</li>
</ul>
<h3 id="损失函数求解"><a href="#损失函数求解" class="headerlink" title="损失函数求解"></a>损失函数求解</h3><ul>
<li>最佳树结构（特征和分裂值）以及叶子节点的预测分数都是通过优化目标函数来得出的，其中叶子节点的预测分数是正则项的一部分；但其实分裂时是通过增益最大化来寻找最佳分裂特征和分裂值的，只有w是通过计算目标函数得出的<br><img src="https://upload-images.jianshu.io/upload_images/1371984-364c3b6e258cc671.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/782/format/webp" alt="Summary"><br>求出各个叶子节点的最佳值以及此时目标函数的值:<br><img src="https://upload-images.jianshu.io/upload_images/1371984-3b24159e2a85b4a3.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/365/format/webp" alt="Summary"></li>
</ul>
<h3 id="分裂"><a href="#分裂" class="headerlink" title="分裂"></a>分裂</h3><ul>
<li>分裂方式<ul>
<li>枚举所有树结构的贪心法（先特征，再分裂点）：<ul>
<li>首先，对所有特征都按照特征的数值进行预排序。</li>
<li>其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。</li>
<li>最后，找到一个特征的分割点后，将数据分裂成左右子节点。</li>
</ul>
</li>
<li>优缺点：<ul>
<li>优点：精确找到分裂点</li>
<li>缺点：<strong>空间消耗大</strong>（保存数据的特征值，还保存了特征排序的结果，即2<em>数据大小）、<strong>时间开销大</strong>（遍历每一个分割点的时候，都需要进行分裂增益的计算）、<em>*cache优化不友好</em></em>（预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对 cache 进行优化）</li>
</ul>
</li>
</ul>
</li>
<li>分裂的标准：<strong>增益最大化</strong>，每次节点分裂，loss function被影响的只有这个节点的样本，因而每次分裂，计算分裂的<strong>增益</strong>（loss function的降低量）只需要关注打算分裂的那个节点的样本<br><img src="https://upload-images.jianshu.io/upload_images/1371984-d0a9c89dbbc34f7c.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/544/format/webp" alt="Summary"></li>
<li>终止条件<ul>
<li>树的最大数量</li>
<li>max_depth</li>
<li>最小增益：当引入的分裂带来的增益小于一个阀值的时候，我们可以剪掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思</li>
<li>当样本权重和小于设定阈值时则停止建树，这个解释一下，涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样，大意就是一个叶子节点样本太少了终止</li>
</ul>
</li>
</ul>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ul>
<li>每个树的每个节点上的预测值相加<br><img src="https://upload-images.jianshu.io/upload_images/1371984-bbe17b3b253a6d1a.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/901/format/webp" alt="Summary"></li>
</ul>
<h2 id="DART-Booster"><a href="#DART-Booster" class="headerlink" title="DART Booster"></a>DART Booster</h2><p>为了解决过拟合，会随机drop trees:</p>
<ul>
<li>训练速度可能慢于gbtree</li>
<li>由于随机性，早停可能不稳定</li>
</ul>
<h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul>
<li>算法：二阶导、正则化、自定义loss、缺失数据处理、支持类别特征、早停</li>
<li>工程：行采样、列采样、</li>
</ul>
<h3 id="正则化-amp-行采样-amp-列采样-amp-早停"><a href="#正则化-amp-行采样-amp-列采样-amp-早停" class="headerlink" title="正则化&amp;行采样&amp;列采样&amp;早停"></a>正则化&amp;行采样&amp;列采样&amp;早停</h3><ul>
<li>通过最优化求出w，而不是平均值或者多数表决</li>
<li>防止过拟合</li>
</ul>
<h3 id="支持自定义loss"><a href="#支持自定义loss" class="headerlink" title="支持自定义loss"></a>支持自定义loss</h3><h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><h3 id="并行"><a href="#并行" class="headerlink" title="并行"></a>并行</h3><ul>
<li><strong>特征间并行</strong>：由于将数据按列存储，可以同时访问所有列，那么可以对所有属性同时执行split finding算法，从而并行化split finding（切分点寻找）</li>
<li><strong>特征内并行</strong>：可以用多个block(Multiple blocks)分别存储不同的样本集，多个block可以并行计算-特征内并行</li>
</ul>
<h3 id="Monotonic-Constraints单调性限制"><a href="#Monotonic-Constraints单调性限制" class="headerlink" title="Monotonic Constraints单调性限制"></a>Monotonic Constraints单调性限制</h3><ul>
<li><p>一个可选特性:<br>会限制模型的结果按照某个特征 单调的进行增减</p>
<p>也就是说可以降低模型对数据的敏感度，如果明确已知某个特征与预测结果呈单调关系时，那在生成模型的时候就会跟特征数据的单调性有关。</p>
</li>
</ul>
<h3 id="Feature-Interaction-Constraints单调性限制"><a href="#Feature-Interaction-Constraints单调性限制" class="headerlink" title="Feature Interaction Constraints单调性限制"></a>Feature Interaction Constraints单调性限制</h3><ul>
<li><p>一个可选特性：<br>不用时，在tree生成的时候，一棵树上的节点会无限制地选用多个特征</p>
<p>设置此特性时，可以规定，哪些特征可以有interaction（一般独立变量之间可以interaction，非独立变量的话可能会引入噪声）</p>
</li>
<li>好处：<ul>
<li>预测时更小的噪声</li>
<li>对模型更好地控制</li>
</ul>
</li>
</ul>
<h3 id="Instance-Weight-File"><a href="#Instance-Weight-File" class="headerlink" title="Instance Weight File"></a>Instance Weight File</h3><ul>
<li>规定了模型训练时data中每一条instance的权重</li>
<li>有些instance质量较差，或与前一示例相比变化不大，所以可以调节其所占权重</li>
</ul>
<h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><ul>
<li>通用参数：宏观函数控制。booster/slient/nthread</li>
<li>Booster参数：控制每一步的booster(tree/regression)。</li>
<li>学习目标参数：控制训练目标的表现。</li>
</ul>
<h3 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h3><p>与overfitting有关的参数：</p>
<ul>
<li>学习率</li>
<li>直接控制模型复杂度：max_depth, min_child_weight, gamma, lambda, alpha, max_leaf_nodes</li>
<li>增加模型随机性以使得模型对噪声有更强的鲁棒性：<ul>
<li>subsample and colsample_bytree. </li>
</ul>
</li>
</ul>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ul>
<li>选择较高的学习速率(learning rate)。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。</li>
<li>对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数。</li>
<li>xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。</li>
<li>降低学习速率，确定理想参数。</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>健身Basics</title>
    <url>/2022/01/16/bodyfitting/</url>
    <content><![CDATA[<h1 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h1><ul>
<li>水平对照：<a href="https://strengthlevel.com/strength-standards">https://strengthlevel.com/strength-standards</a></li>
</ul>
<h1 id="Guides"><a href="#Guides" class="headerlink" title="Guides"></a>Guides</h1><ul>
<li><a href="https://stronglifts.com/#guides">https://stronglifts.com/#guides</a></li>
</ul>
<a id="more"></a>
]]></content>
      <categories>
        <category>BodyFitness</category>
      </categories>
      <tags>
        <tag>Resource</tag>
        <tag>健身</tag>
      </tags>
  </entry>
  <entry>
    <title>CUDA介绍以及耗时分析</title>
    <url>/2021/12/28/cuda-intros/</url>
    <content><![CDATA[<ul>
<li><a href="https://www.cnblogs.com/1024incn/p/4537177.html">https://www.cnblogs.com/1024incn/p/4537177.html</a>)</li>
<li><a href="https://developer.download.nvidia.cn/CUDA/training/StreamsAndConcurrencyWebinar.pdf">https://developer.download.nvidia.cn/CUDA/training/StreamsAndConcurrencyWebinar.pdf</a><a id="more"></a>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>git操作整理</title>
    <url>/2021/12/10/git-git%E6%93%8D%E4%BD%9C%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<p>git操作整理<br><a id="more"></a></p>
<ul>
<li>git log: 记录版本历史</li>
<li>git reflog：记录操作历史</li>
</ul>
<h2 id="远程库"><a href="#远程库" class="headerlink" title="远程库"></a>远程库</h2><ul>
<li>git remote add <remote name> xxx 其中<remote name>是git对远程库的命名，origin是默认叫法。这个命名是本地对远程库的一个命名。   </li>
<li>git remote -v 查看远程库</li>
<li>git remote rm <remote name></li>
</ul>
<h2 id="工作区、缓存区"><a href="#工作区、缓存区" class="headerlink" title="工作区、缓存区"></a>工作区、缓存区</h2><ul>
<li>stage：缓存，add后缓存区和工作区文件一致，commit后缓存区清空，进入下一个版本</li>
</ul>
<h3 id="diff"><a href="#diff" class="headerlink" title="diff"></a>diff</h3><ul>
<li>工作区 ↔ 缓存区：git diff，当前修改对比上次add都有啥，git add后当前工作区的文件就不能通过git diff查到了</li>
<li>缓存区 ↔ 版本库(HEAD)：git diff —cached 当前add的文件和上个版本的diff</li>
<li>工作区 ↔ 库(HEAD)：git diff HEAD — filename</li>
<li>库 ↔ 库：git diff 243550a 24bc01b filename     #较旧的id 较新的id</li>
</ul>
<h2 id="文件修改撤销"><a href="#文件修改撤销" class="headerlink" title="文件修改撤销"></a>文件修改撤销</h2><h3 id="文件修改撤销（老版本git）"><a href="#文件修改撤销（老版本git）" class="headerlink" title="文件修改撤销（老版本git）"></a>文件修改撤销（老版本git）</h3><h4 id="checkout-—（撤销工作区修改）"><a href="#checkout-—（撤销工作区修改）" class="headerlink" title="checkout —（撤销工作区修改）"></a>checkout —（撤销工作区修改）</h4><ul>
<li>不加—可能变成切换分支</li>
<li>文件回到最近一次git commit或git add时的状态。<ul>
<li>修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态（git commit）；</li>
<li>已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态（git add）。</li>
</ul>
</li>
</ul>
<h4 id="reset-HEAD-（撤销暂存区修改）"><a href="#reset-HEAD-（撤销暂存区修改）" class="headerlink" title="reset HEAD （撤销暂存区修改）"></a>reset HEAD <fileName>（撤销暂存区修改）</h4><ul>
<li><strong>把暂存区的文件回退到工作区</strong></li>
<li>再利用git checkout —就可以删除修改了</li>
<li>或者直接用git checkout HEAD <fileName> 直接从HEAD恢复文件</li>
</ul>
<h3 id="文件修改撤销（新版本git）"><a href="#文件修改撤销（新版本git）" class="headerlink" title="文件修改撤销（新版本git）"></a>文件修改撤销（新版本git）</h3><ul>
<li>撤销工作区的修改：git restore <fileName></li>
<li>撤销工作区和暂存区的修改，恢复到HEAD：git resotre —worktree <fileName></li>
<li>撤销暂存区的修改，前提是上次add后该文件未做其他修改：git restore —staged <fileName></li>
<li>从master同时恢复工作区和暂存区：git restore —source=HEAD —staged —worktree <fileName></li>
</ul>
<h2 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h2><h3 id="回退"><a href="#回退" class="headerlink" title="回退"></a>回退</h3><ul>
<li>HEAD始终指向当前分支的最新commit，HEAD^ 上一个commit，HEAD~100 上一百个commit</li>
<li>若想reset更新的commit，需要知道版本号 git reset [版本号]</li>
</ul>
<h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><ul>
<li>分支的切换：HEAD一直指向当前分支最新commit，切换分支时，HEAD指向另一个分支</li>
<li>git branch: list all branchs</li>
<li>git branch &lt;&gt;: create branch</li>
<li>git switch &lt;&gt; / git checkout &lt;&gt;：切换</li>
<li>git switch -c &lt;&gt; / git checkout -b &lt;&gt;: 创建并切换</li>
<li>git branch -d &lt;&gt;: 删除</li>
<li>git merge <new branch>: 合并new branch到当前分支<ul>
<li>无冲突：fast forward</li>
<li>有冲突：解决冲突，commit。git log —graph可以看分支合并图</li>
</ul>
</li>
<li>git rebase:把本地未push的分叉提交历史整理成直线<br><a href="https://blog.csdn.net/liuxiaoheng1992/article/details/79108233">rebase 和 merge</a></li>
</ul>
<h3 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h3><ul>
<li>fast forward：不会留下commit信息</li>
<li>—no-ff：会留下commit信息，用法：git merge —no-ff -m “merge with no-ff” dev</li>
</ul>
<h3 id="工作现场保护-amp-Bug分支"><a href="#工作现场保护-amp-Bug分支" class="headerlink" title="工作现场保护&amp;Bug分支"></a>工作现场保护&amp;Bug分支</h3><p><a href="https://www.liaoxuefeng.com/wiki/896043488029600/900388704535136">https://www.liaoxuefeng.com/wiki/896043488029600/900388704535136</a></p>
<ul>
<li>修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除；</li>
<li>当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场；</li>
<li>在master分支上修复的bug，想要合并到当前dev分支，可以用git cherry-pick <commit>命令，把bug提交的修改“复制”到当前分支，避免重复劳动。</li>
</ul>
<h3 id="多人协作"><a href="#多人协作" class="headerlink" title="多人协作"></a>多人协作</h3><ul>
<li>试图用git push origin <your-branch>推送自己的修改；</li>
<li>如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并；<ul>
<li>如果合并有冲突，则解决冲突，并在本地提交；</li>
<li>如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch —set-upstream-to=origin/<origin-branch> <your-branch></li>
</ul>
</li>
<li>git push origin <your-branch></li>
</ul>
<p>或者：</p>
<ul>
<li>git push -u <remote> <branch>，其中u为upstream。git push -u origin branch：把本地branch分支push到origin/branch</li>
<li>git push —set-upstream <remote> <branch></li>
</ul>
<h2 id="ignore"><a href="#ignore" class="headerlink" title="ignore"></a>ignore</h2><ul>
<li><a href="https://github.com/github/gitignore">部分规则</a></li>
</ul>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Kaggle-Kaggle相关</title>
    <url>/2021/12/10/kaggle%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="如何在-Kaggle-首战中进入前-10"><a href="#如何在-Kaggle-首战中进入前-10" class="headerlink" title="如何在 Kaggle 首战中进入前 10%"></a>如何在 Kaggle 首战中进入前 10%</h1><p><a href="https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/">原文</a></p>
<a id="more"></a>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><h3 id="Exploration-Data-Analysis-EDA"><a href="#Exploration-Data-Analysis-EDA" class="headerlink" title="Exploration Data Analysis(EDA)"></a>Exploration Data Analysis(EDA)</h3><h4 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h4><p>matplotlib + seaborn</p>
<ul>
<li>查看目标变量的分布。当分布不平衡时，根据评分标准和具体模型的使用不同，可能会严重影响性能。</li>
<li>对 Numerical Variable，可以用 Box Plot 来直观地查看它的分布。</li>
<li>对于坐标类数据，可以用 Scatter Plot 来查看它们的分布趋势和是否有离群点的存在。</li>
<li>对于分类问题，将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助。</li>
<li>绘制变量之间两两的分布和相关度图表。</li>
</ul>
<p><a href="https://www.kaggle.com/benhamner/python-data-visualizations">example_visualization</a></p>
<h4 id="Statistical-Tests"><a href="#Statistical-Tests" class="headerlink" title="Statistical Tests"></a>Statistical Tests</h4><p>可视化为定性，这里专注于定量，例如对于新创造的特征，可以将其加入原模型当中，看结果的变化。</p>
<p>在某些比赛中，由于数据分布比较奇葩或是噪声过强，Public LB(Leader board)的分数可能会跟 Local CV(Cross Validation)的结果相去甚远。可以根据一些统计测试的结果来粗略地建立一个阈值，用来衡量一次分数的提高究竟是实质的提高还是由于数据的随机性导致的。</p>
<h3 id="Data-Preprossing"><a href="#Data-Preprossing" class="headerlink" title="Data Preprossing"></a>Data Preprossing</h3><p>处理策略主要依赖于EDA中得到的结论。</p>
<ul>
<li>有时数据会分散在几个不同的文件中，需要 Join 起来。</li>
<li>处理 Missing Data。</li>
<li>处理 Outlier。</li>
<li>必要时转换某些 Categorical Variable 的表示方式。例如应用one-hot encoding(pd.get_dummies)将categorical variable转化为数字变量。</li>
<li>有些 Float 变量可能是从未知的 Int 变量转换得到的，这个过程中发生精度损失会在数据中产生不必要的 Noise，即两个数值原本是相同的却在小数点后某一位开始有不同。这对 Model 可能会产生很负面的影响，需要设法去除或者减弱 Noise。</li>
</ul>
<h3 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h3><h4 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h4><p>总的来说，应该<strong>生成尽量多的 Feature，相信 Model 能够挑出最有用的 Feature</strong>。但有时先做一遍 Feature Selection 也能带来一些好处：</p>
<ul>
<li>Feature 越少，训练越快。</li>
<li>有些 Feature 之间可能存在线性关系，影响 Model 的性能。</li>
<li>通过挑选出最重要的 Feature，可以将它们之间进行各种运算和操作的结果作为新的 Feature，可能带来意外的提高。</li>
<li>Feature Selection 最实用的方法也就是看 Random Forest 训练完以后得到的 Feature Importance 了。其他有一些更复杂的算法在理论上更加 Robust，但是缺乏实用高效的实现。从原理上来讲，增加 Random Forest 中树的数量可以在一定程度上加强其对于 Noisy Data 的 Robustness。</li>
</ul>
<p>看 Feature Importance 对于某些数据经过脱敏处理的比赛尤其重要。这可以免得你浪费大把时间在琢磨一个不重要的变量的意义上。(脱敏：数据脱敏(Data Masking),又称数据漂白、数据去隐私化或数据变形。百度百科对数据脱敏的定义为：指对某些敏感信息通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。在涉及客户安全数据或者一些商业性敏感数据的情况下，在不违反系统规则条件下，对真实数据进行改造并提供测试使用，如身份证号、手机号、卡号、客户号等个人信息都需要进行数据脱敏。)</p>
<h4 id="Feature-Encoding"><a href="#Feature-Encoding" class="headerlink" title="Feature Encoding"></a>Feature Encoding</h4><p>假设有一个 Categorical Variable 一共有几万个取值可能，那么创建 Dummy Variables 的方法就不可行了。这时一个比较好的方法是根据 Feature Importance 或是这些取值本身在数据中的出现频率，为最重要（比如说前 95% 的 Importance）那些取值（有很大可能只有几个或是十几个）创建 Dummy Variables，而所有其他取值都归到一个“其他”类里面。</p>
<h3 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h3><p>Base Model:</p>
<ul>
<li>SVM</li>
<li>Linear Regression</li>
<li>Logistic Regression</li>
<li>Neural Networks</li>
</ul>
<p>Most Used Models:</p>
<ul>
<li>Gradient Boosting</li>
<li>Random Forest</li>
<li><p>Extra Randomized Trees</p>
<p><strong>XGBoost</strong></p>
</li>
</ul>
<h4 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h4><p>通过Grid Search来确定模型的最佳参数。<br>e.g.</p>
<ul>
<li>sklearn 的 RandomForestClassifier 来说，比较重要的就是随机森林中树的数量 n_estimators 以及在训练每棵树时最多选择的特征数量 max_features。</li>
<li><p>Xgboost 的调参。通常认为对它性能影响较大的参数有：</p>
<ul>
<li>eta：每次迭代完成后更新权重时的步长。越小训练越慢。</li>
<li>num_round：总共迭代的次数。</li>
<li>subsample：训练每棵树时用来训练的数据占全部的比例。用于防止 Overfitting。</li>
<li>colsample_bytree：训练每棵树时用来训练的特征的比例，类似 RandomForestClassifier 的 max_features。</li>
<li>max_depth：每棵树的最大深度限制。与 Random Forest 不同，Gradient Boosting 如果不对深度加以限制，最终是会 Overfit 的。</li>
<li>early_stopping_rounds：用于控制在 Out Of Sample 的验证集上连续多少个迭代的分数都没有提高后就提前终止训练。用于防止 Overfitting。</li>
</ul>
<p>一般的调参步骤是：</p>
<ol>
<li>将训练数据的一部分划出来作为验证集。</li>
<li>先将 eta 设得比较高（比如 0.1），num_round 设为 300 ~ 500。</li>
<li>用 Grid Search 对其他参数进行搜索。</li>
<li>逐步将 eta 降低，找到最佳值。</li>
<li>以验证集为 watchlist，用找到的最佳参数组合重新在训练集上训练。注意观察算法的输出，看每次迭代后在验证集上分数的变化情况，从而得到最佳的 early_stopping_rounds。</li>
</ol>
<p><em>所有具有随机性的 Model 一般都会有一个 seed 或是 random_state 参数用于控制随机种子。得到一个好的 Model 后，在记录参数时务必也记录下这个值，从而能够在之后重现 Model。</em></p>
</li>
</ul>
<h4 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h4><p>一般5-fold。</p>
<p>fold越多训练越慢。</p>
<h4 id="Ensemble-Generation"><a href="#Ensemble-Generation" class="headerlink" title="Ensemble Generation"></a>Ensemble Generation</h4><p>常见的 Ensemble 方法有这么几种：</p>
<ul>
<li>Bagging：使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。</li>
<li>Boosting：迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting 的原理。比 Bagging 效果好，但更容易 Overfit。</li>
<li>Blending：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。</li>
<li>Stacking：接下来会详细介绍。</li>
</ul>
<p>从理论上讲，Ensemble 要成功，有两个要素：</p>
<ul>
<li>Base Model 之间的相关性要尽可能的小。这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。</li>
<li>Base Model 之间的性能表现不能差距太大。这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。</li>
</ul>
<h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>workflow比较复杂，因此一个高自动化的pipeline比较重要。</p>
<p>这里是以一个例子：<a href="https://github.com/ChenglongChen/Kaggle_CrowdFlower">example</a></p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>MTL多任务学习</title>
    <url>/2022/01/18/mtl/</url>
    <content><![CDATA[<blockquote>
<p>MTL更多的是一种思想</p>
</blockquote>
<h1 id="两种模式"><a href="#两种模式" class="headerlink" title="两种模式"></a>两种模式</h1><ul>
<li>hard share: 不同任务直接共享部分模型参数</li>
<li>soft share: 不共享参数，添加正则来保证参数的相似</li>
</ul>
<a id="more"></a>
<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><ul>
<li>隐式数据增加（implicit data augmentation）：为了训练任务A，通过其他任务的数据来扩充任务A训练过程的数据量。这些数据可以看做是引入额外的噪声，在理想情况下起到提高模型泛化效果的作用</li>
<li>注意力机制（Attention focusing）：当训练数据量有限且高维时，模型很难区分出相关的特征和不相关的特征。多任务学习可以使模型更关注于有用的特征</li>
<li>监听机制（Eavesdropping）：从任务B中有可能容易学习到特征G，但是从任务A中很难学习得到。通过多任务学习，可以通过任务B学习到特征G，再利用特征G预测任务A<br>特征偏置（Representation bias）：多任务学习使得模型更容易去选择其他任务容易选择到的特征，这样有助于模型在假设空间下获得更好的模型泛化能力。</li>
</ul>
<h1 id="常见模型结构"><a href="#常见模型结构" class="headerlink" title="常见模型结构"></a>常见模型结构</h1><ul>
<li>常见模型：share bottom/MMoE/SNR/ESMM；损失函数按照指定权重融合，或者使用GradNorm做动态融合</li>
</ul>
<h2 id="Share-bottom"><a href="#Share-bottom" class="headerlink" title="Share bottom"></a>Share bottom</h2><ul>
<li>最常见的mtl结构，不同任务共享底层</li>
</ul>
<h2 id="MMoE-Multi-gate-Mixture-of-Experts"><a href="#MMoE-Multi-gate-Mixture-of-Experts" class="headerlink" title="MMoE(Multi-gate Mixture-of-Experts)"></a>MMoE(Multi-gate Mixture-of-Experts)</h2><ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007">paper</a></li>
<li>是一种soft share</li>
<li>添加了expert机制获取不同信息</li>
<li>添加gate机制对expert进行加权<img src="/2022/01/18/mtl/mmoe.png" class="" title="mmoe">
</li>
</ul>
<h2 id="CGC-Customized-Gate-Control-PLE-Progressive-Layered-Extraction"><a href="#CGC-Customized-Gate-Control-PLE-Progressive-Layered-Extraction" class="headerlink" title="CGC(Customized Gate Control)/PLE(Progressive Layered Extraction)"></a>CGC(Customized Gate Control)/PLE(Progressive Layered Extraction)</h2><ul>
<li>MMoE的基础上，把experts分为share experts和domain experts</li>
<li>单层多任务网络结构(CGC)，多层多任务网络结构（PLE）<img src="/2022/01/18/mtl/cgc.png" class="" title="cgc">
<img src="/2022/01/18/mtl/ple.png" class="" title="ple">
</li>
</ul>
<h2 id="SNR-Sub-Network-Routing"><a href="#SNR-Sub-Network-Routing" class="headerlink" title="SNR(Sub-Network Routing)"></a>SNR(Sub-Network Routing)</h2><ul>
<li><a href="https://ojs.aaai.org//index.php/AAAI/article/view/3788">paper</a><img src="/2022/01/18/mtl/snr.png" class="" title="snr"></li>
<li>SNR-Trans: 下层输出通过权重矩阵变换，再做加权后输出至下一层。会引入更多参数。其中z为二进制。<script type="math/tex; mode=display">
\left[\begin{array}{l}
\boldsymbol{v}_{1} \\
\boldsymbol{v}_{2}
\end{array}\right]=\left[\begin{array}{lll}
z_{11} W_{11} & z_{12} W_{12} & z_{13} W_{13} \\
z_{21} W_{21} & z_{22} W_{22} & z_{23} W_{23}
\end{array}\right]\left[\begin{array}{l}
u_{1} \\
u_{2} \\
u_{3}
\end{array}\right]</script></li>
<li>SNR-Avg: 下层输出加权后输出至下一层<script type="math/tex; mode=display">
\left[\begin{array}{l}
\boldsymbol{v}_{1} \\
\boldsymbol{v}_{2}
\end{array}\right]=\left[\begin{array}{lll}
z_{11} \boldsymbol{I}_{11} & z_{12} \boldsymbol{I}_{12} & z_{13} \boldsymbol{I}_{13} \\
z_{21} \boldsymbol{I}_{21} & z_{22} \boldsymbol{I}_{22} & z_{23} \boldsymbol{I}_{23}
\end{array}\right]\left[\begin{array}{l}
\boldsymbol{u}_{1} \\
\boldsymbol{u}_{2} \\
\boldsymbol{u}_{3}
\end{array}\right]</script></li>
<li>损失函数：<script type="math/tex; mode=display">
\min _{\boldsymbol{W}, \boldsymbol{\pi}} \boldsymbol{E}_{\boldsymbol{z} \sim p(\boldsymbol{z} ; \boldsymbol{\pi})} \frac{1}{N} \sum_{i=1}^{N} L\left(f\left(\boldsymbol{x}_{i} ; \boldsymbol{W}, \boldsymbol{z}\right), \boldsymbol{y}_{i}\right)</script></li>
<li>z为二进制，无法优化，需要把z松弛为平滑变量，将z变为hardSigmoid，其中s是一个服从q分布的连续的随机变量<script type="math/tex; mode=display">
z=g(s)=\min (1, \max (0, s))</script></li>
<li>可以继续转换，其中epsilon是一个噪声变量，r(epsilon)是一个无参数的噪声分布，h是一个确定且可微的分布<script type="math/tex; mode=display">
\min _{\boldsymbol{W}, \boldsymbol{\pi}} \boldsymbol{E}_{\boldsymbol{\epsilon} \sim r(\boldsymbol{\epsilon})} \frac{1}{N} \sum_{i=1}^{N} L\left(f\left(\boldsymbol{x}_{i} ; \boldsymbol{W}, g(h(\boldsymbol{\phi}, \boldsymbol{\epsilon}))\right), \boldsymbol{y}_{i}\right)</script></li>
<li>在实际应用中，结合重采样技术和hard concrete distribution，可以继续做转换，其中u为均匀分布，log(α)为需要学习的参数，其他为超参<script type="math/tex; mode=display">
\begin{aligned}
u \sim U(0,1), s &=\operatorname{sigmoid}((\log (u)-\log (1-u)+\log (\alpha) / \beta)\\
\bar{s} &=s(\zeta-\gamma)+\gamma, z=\min (1, \max (\bar{s}, 0))
\end{aligned}</script></li>
<li>training的时候，加入z的L0正则，则最终loss为：<script type="math/tex; mode=display">
\begin{array}{r}
\boldsymbol{E}_{\boldsymbol{\epsilon} \sim r(\boldsymbol{\epsilon})} \frac{1}{N} \sum_{i=1}^{N} L\left(f\left(\boldsymbol{x}_{i} ; \boldsymbol{W}, g(h(\boldsymbol{\phi}, \boldsymbol{\epsilon}))\right), \boldsymbol{y}_{i}\right) \\
+\lambda \sum_{j=1}^{|\boldsymbol{z}|} 1-Q\left(s_{j}<0 ; \phi_{j}\right)
\end{array}</script></li>
<li>training: 总结来说，模型需要学习的参数是W和隐变量分布变量log(alpha)。训练流程如下所示：<ul>
<li>首先，采样一组均匀分布的随机变量u；</li>
<li>其次，计算z来获得网络结构；</li>
<li>最后，将训练数据喂给模型来计算损失函数。W和log(alpha)的梯度通过反馈计算得到。</li>
</ul>
</li>
<li>serving: 使用如下的estimator来得到z的值<script type="math/tex; mode=display">
\hat{\boldsymbol{z}}=\min (1, \max (0, \operatorname{sigmoid}(\log (\boldsymbol{\alpha}))(\zeta-\gamma)+\gamma)) \text {. }</script></li>
</ul>
<p>When sigmoid <script type="math/tex">\left(\log \left(\alpha_{i j}\right)\right)(\zeta-\gamma)+\gamma<0</script>, we will have <script type="math/tex">\hat{z}_{i j}=0</script> and the resulted model will be sparsely connected.</p>
<h2 id="Star-One-Model-to-Serve-All-Star-Topology-Adaptive-Recommender-for-Multi-Domain-CTR-Prediction"><a href="#Star-One-Model-to-Serve-All-Star-Topology-Adaptive-Recommender-for-Multi-Domain-CTR-Prediction" class="headerlink" title="Star(One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction)"></a>Star(One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction)</h2><ul>
<li><a href="https://arxiv.org/abs/2101.11427">paper</a><img src="/2022/01/18/mtl/star.png" class="" title="star"></li>
<li>center tower和domain tower的结合，论文中是两塔权重element-wise相乘，也可以改为两塔logits相加</li>
<li>PN: partitioned normalization, 在BN的基础上，对每个domain引入domain相关的两个scale参数<script type="math/tex; mode=display">
z^{\prime}=\left(\gamma * \gamma_{p}\right) \frac{z-\mu}{\sqrt{\sigma^{2}+\epsilon}}+\left(\beta+\beta_{p}\right)</script></li>
</ul>
<h2 id="十字绣-Cross-Stitch-Network"><a href="#十字绣-Cross-Stitch-Network" class="headerlink" title="十字绣(Cross-Stitch Network)"></a>十字绣(Cross-Stitch Network)</h2><ul>
<li><a href="https://arxiv.org/pdf/1604.03539.pdf">paper</a></li>
</ul>
<h2 id="ESMM-Entire-Space-Multi-Task-Model"><a href="#ESMM-Entire-Space-Multi-Task-Model" class="headerlink" title="ESMM(Entire Space Multi-Task Model)"></a>ESMM(Entire Space Multi-Task Model)</h2><ul>
<li>混合ctr、cvr数据流</li>
<li>传统CVR预估模型的本质，不是预测“item被点击，然后被转化”的概率（CTCVR），而是“假设item被点击，那么它被转化”的概率（CVR）。即CVR模型的样本空间，是click空间。</li>
<li>ESMM<ul>
<li>使用了全样本空间，通过预测CTR和CTCVR间接求CVR</li>
<li>解决training样本有偏的问题: training的时候在点击空间，serving的时候在展示空间。</li>
<li>直接在show空间求ctcvr，label太稀疏，建模ctcvr=cvr*ctr，解决稀疏问题</li>
</ul>
</li>
<li>利用全概率公式，<strong>隐式学习pCVR</strong></li>
<li><a href="https://arxiv.org/pdf/1804.07931.pdf">paper</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/57481330">blog</a><script type="math/tex; mode=display">
\underbrace{p(y=1, z=1 \mid x)}_{p C T C V R}=\underbrace{p(y=1 \mid x)}_{p C T R} \times \underbrace{p(z=1 \mid y=1, x)}_{p C V R}</script></li>
<li>pCVR只是一个variable，无显式监督信号<img src="/2022/01/18/mtl/esmm.png" class="" title="esmm">
</li>
</ul>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="GradNorm"><a href="#GradNorm" class="headerlink" title="GradNorm"></a>GradNorm</h2><ul>
<li><a href="https://openreview.net/pdf?id=H1bM1fZCW">paper</a></li>
<li>分两个loss，加权多任务loss以及每个任务权重的loss，后者只过share bottom</li>
<li>两个作用<ul>
<li>动态调整不同任务损失函数的权重：不同目标重要性不同/收敛的程度/loss的大小 diff较大，可以考虑使用</li>
<li>share bottom的更新受到每个任务权重的影响</li>
</ul>
</li>
<li><a href="https://blog.csdn.net/Leon_winter/article/details/105014677">https://blog.csdn.net/Leon_winter/article/details/105014677</a></li>
</ul>
<h2 id="MAMDR-MAMDR-A-Model-Agnostic-Learning-Method-for-Multi-Domain-Recommendation"><a href="#MAMDR-MAMDR-A-Model-Agnostic-Learning-Method-for-Multi-Domain-Recommendation" class="headerlink" title="MAMDR(MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation)"></a>MAMDR(MAMDR: A Model Agnostic Learning Method for Multi-Domain Recommendation)</h2><ul>
<li><a href="https://arxiv.org/pdf/2202.12524.pdf">paper</a></li>
<li>提出了一种通用的，模型结构无关的，多域推荐的学习方法<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3></li>
<li>不同域数据分布差异很大：<ul>
<li>(1) Shared parameters suffer from the domain conflict problem: 共享参数停在compromised position</li>
<li>(2) Specific parameters are inclined to overfitting: domain参数overfitting</li>
<li>(3) Existing Multi-domain recommendation models cannot generalize to all circumstances: 与应用场景相关</li>
</ul>
</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ul>
<li>包括Domain Negotiatio和Domain Regularization<img src="/2022/01/18/mtl/mamdr.png" class="" title="mamdr"></li>
<li>Domain Negotiatio: 针对共享参数。所有domain更新完之后，再兑回一点。<img src="/2022/01/18/mtl/mamdr1.png" class="" title="mamdr1"></li>
<li>Domain Regularization: 针对domain参数。针对domain i，先sample若干其他domain，利用其他domain更新domain i的参数，再利用domain i更新domain i的参数。<img src="/2022/01/18/mtl/mamdr2.png" class="" title="mamdr2">
</li>
</ul>
<h2 id="share-vs-not-share"><a href="#share-vs-not-share" class="headerlink" title="share vs not share"></a>share vs not share</h2><ul>
<li>not share bias emb: tend to useful</li>
<li>share sparse: tend to useful</li>
<li>share bottom use small learning rate, tower use adam or bigger learning rate</li>
</ul>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="https://blog.nowcoder.net/n/8a9d69d063c546b291a3c9a5091cfbbe">一篇综述</a></li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>MTL</tag>
      </tags>
  </entry>
  <entry>
    <title>序列模型</title>
    <url>/2022/03/09/seq/</url>
    <content><![CDATA[<p>广告/推荐中的序列模型<br><a id="more"></a></p>
<h1 id="DIN-Deep-Interest-Network"><a href="#DIN-Deep-Interest-Network" class="headerlink" title="DIN(Deep Interest Network)"></a>DIN(Deep Interest Network)</h1><p><a href="https://dl.acm.org/doi/abs/10.1145/3219819.3219823">paper</a></p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><ul>
<li>思想：针对不同的候选广告，用户历史行为与该广告的权重是不同的。</li>
<li>方法：以当前候选广告为query，用户历史行为为key/value，利用attention机制，求query与key的attention score，并利用attention score加权value<img src="/2022/03/09/seq/din.png" class="" title="din">
</li>
</ul>
<script type="math/tex; mode=display">V_{u}=f\left(V_{a}\right)=\sum_{i=1}^{N} w_{i} * V_{i}=\sum_{i=1}^{N} g\left(V_{i}, V_{a}\right) * V_{i}</script><ul>
<li>Vi表示behavior id i的嵌入向量，比如good_id,shop_id等。Vu是所有behavior ids的加权和，表示的是用户兴趣；Va是候选广告的嵌入向量；wi是候选广告影响着每个behavior id的权重，也就是Local Activation。wi通过Activation Unit计算得出，这一块用函数去拟合，表示为g(Vi,Va)。</li>
</ul>
<h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><h3 id="GAUC"><a href="#GAUC" class="headerlink" title="GAUC"></a>GAUC</h3><ul>
<li>group auc</li>
<li>auc的评估是对所有样本排序后，评估正样本排在负样本的概率。但推荐、广告系统中，关注的是单个用户的排序。</li>
<li>gauc是在单个用户auc基础上，按照点击次数、show次数等进行加权平均，消除用户偏差对模型的影响。n为用户数量。<script type="math/tex; mode=display">\mathrm{GAUC}=\frac{\sum_{i=1}^{n} w_{i} * \mathrm{AUC}_{i}}{\sum_{i=1}^{n} w_{i}}=\frac{\sum_{i=1}^{n} \text { impression }_{i} * \mathrm{AUC}_{i}}{\sum_{i=1}^{n} \text { impression }_{i}}</script></li>
</ul>
<h3 id="DICE"><a href="#DICE" class="headerlink" title="DICE"></a>DICE</h3><ul>
<li>Data Dependent Activation Function<script type="math/tex; mode=display">f(s)=p(s) . s+(1-p(s)) \cdot \alpha s, p(s)=\frac{1}{1+e^{-\frac{s-E(s)}{\sqrt{V} a r(s)+\epsilon}}}</script></li>
<li>可以视为Batch Normalization的变化。使得<strong>激活函数随着一个batch的数据分布做自适应调整</strong>——data dependent。<script type="math/tex; mode=display">f(s)=\operatorname{sigmoid}(B N(s)) \cdot s+(1-\operatorname{sigmoid}(B N(s))) \cdot \alpha s</script></li>
<li>pi是一个概率值，这个概率值决定着输出是取 s 或者是 alpha_i * s，起到了一个整流器的作用。pi的计算分为两步：<ul>
<li>首先，对 x 进行均值归一化处理，这使得整流点是在数据的均值处，实现了 data dependent 的想法；</li>
<li>其次，经过一个 sigmoid 函数的计算，得到了一个 0 到 1 的概率值。</li>
</ul>
</li>
<li>另外，期望和方差使用每次训练的 mini batch data 直接计算，并类似于 Momentum 使用了 指数加权平均：</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
E\left[s_{i}\right]_{t+1}^{\prime} &=E\left[s_{i}\right]_{t}^{\prime}+\alpha E\left[s_{i}\right]_{t+1} \\
\operatorname{Var}\left[s_{i}\right]_{t+1}^{\prime} &=\operatorname{Var}\left[s_{i}\right]_{t}^{\prime}+\alpha \operatorname{Var}\left[s_{i}\right]_{t+1}
\end{aligned}</script><h3 id="Adaptive-L2-Regularization"><a href="#Adaptive-L2-Regularization" class="headerlink" title="Adaptive L2 Regularization"></a>Adaptive L2 Regularization</h3><ul>
<li>两点改进<ul>
<li>针对 feature id 出现的频率，来自适应的调整他们正则化的强度:<ul>
<li>对于出现频率高的，给与较小的正则化强度；</li>
<li>对于出现频率低的，给予较大的正则化强度。</li>
</ul>
</li>
<li>正则化涉及的参数限制在了仅在Mini-batch出现过的特征所影响的权重, 有效地缓解了过拟合的问题</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">L_{2}(W) \approx \sum_{j=1}^{K} \sum_{m=1}^{B} \frac{\alpha_{m j}}{n_{j}}\left\|w_{j}\right\|_{2}^{2}</script><ul>
<li>amj表示是否至少有一个样本的id为j的特征出现在mini-batch中；nj表示feature id为j出现的次数，惩罚了出现频率低的item</li>
</ul>
<h1 id="DIEN-Deep-Interest-Evolution-Network-for-Click-Through-Rate-Prediction"><a href="#DIEN-Deep-Interest-Evolution-Network-for-Click-Through-Rate-Prediction" class="headerlink" title="DIEN(Deep Interest Evolution Network for Click-Through Rate Prediction)"></a>DIEN(Deep Interest Evolution Network for Click-Through Rate Prediction)</h1><p><a href="https://arxiv.org/abs/1809.03672v1">paper</a>, <a href="https://github.com/mouna99/dien">git</a></p>
<ul>
<li>DIN: <strong>将用户历史行为视作用户兴趣</strong>，并用attention机制来捕捉target和历史行为的相对兴趣</li>
<li>DIEN: <strong>行为不等于兴趣，需要从行为中挖掘兴趣，并考虑兴趣的动态变化</strong><ul>
<li>兴趣抽取层：计算一个辅助loss，来提升兴趣表达(每个历史行为embedding学习)的准确性&lt; GRU + Loss&gt;</li>
<li>兴趣进化层：更加准确的表达用户兴趣的动态变化性(加权历史行为embedding的学习)&lt; AUGRU(GRU + attention) &gt;</li>
</ul>
</li>
</ul>
<h2 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h2><img src="/2022/03/09/seq/dien.jpeg" class="" title="dien">
<h3 id="Interest-Extractor层：兴趣的挖掘。利用GRU-辅助loss来挖掘用户兴趣。"><a href="#Interest-Extractor层：兴趣的挖掘。利用GRU-辅助loss来挖掘用户兴趣。" class="headerlink" title="Interest Extractor层：兴趣的挖掘。利用GRU+辅助loss来挖掘用户兴趣。"></a>Interest Extractor层：兴趣的挖掘。利用GRU+辅助loss来挖掘用户兴趣。</h3><ul>
<li>辅助loss: 第t个时间步输入e(t)，GRU输出隐单元h(t)，第t步loss: 令下一个时间步的输入向量e(t+1)作为正样本，随机采样负样本e(t+1)。<script type="math/tex; mode=display">\begin{aligned} L_{a u x}=-& \frac{1}{N}\left(\sum_{i=1}^{N} \sum_{t} \log \sigma\left(\mathbf{h}_{t}^{i}, \mathbf{e}_{b}^{i}[t+1]\right)\right.\\ &\left.+\log \left(1-\sigma\left(\mathbf{h}_{t}^{i}, \hat{\mathbf{e}}_{b}^{i}[t+1]\right)\right)\right) \end{aligned}</script></li>
<li>GRU, u表示update gate, r 表示reset gate, h’表示候选的隐藏状态,通过tanh缩放到-1～1之间，对Ht-1进行reset同时+Item, ht 通过last hidden states <em> (1-update) 表示遗忘，update </em> h’表示记忆当前状态<script type="math/tex; mode=display">\begin{aligned}
\mathbf{u}_{t} &=\sigma\left(W^{u} \mathbf{i}_{t}+U^{u} \mathbf{h}_{t-1}+\mathbf{b}^{u}\right) \\
\mathbf{r}_{t} &=\sigma\left(W^{r} \mathbf{i}_{t}+U^{r} \mathbf{h}_{t-1}+\mathbf{b}^{r}\right) \\
\tilde{\mathbf{h}}_{t} &=\tanh \left(W^{h} \mathbf{i}_{t}+\mathbf{r}_{t} \circ U^{h} \mathbf{h}_{t-1}+\mathbf{b}^{h}\right) \\
\mathbf{h}_{t} &=\left(\mathbf{1}-\mathbf{u}_{t}\right) \circ \mathbf{h}_{t-1}+\mathbf{u}_{t} \circ \tilde{\mathbf{h}}_{t}
\end{aligned}</script></li>
</ul>
<h3 id="Interest-Evolving层"><a href="#Interest-Evolving层" class="headerlink" title="Interest Evolving层"></a>Interest Evolving层</h3><ul>
<li>Interest Evolving层对与target item相关的兴趣演化轨迹进行建模：利用注意力机制+GRU</li>
<li>AUGRU: attention update-gate gru。利用attention与update gate相乘，替换原始的update gate</li>
<li>变体<ul>
<li>GRU with attentional input（AIGRU）: 较为简单，直接将attention系数和输入相乘</li>
<li>Attention based GRU（AGRU）：采用问答领域文章提到的一种方法，直接将attention系数来替换GRU的update gate，直接对隐状态进行更新</li>
</ul>
</li>
</ul>
<h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><ul>
<li>GRU的耗时</li>
<li>可以借鉴的：aug loss?</li>
</ul>
<h1 id="DSIN-Deep-Session-Interest-Network-for-Click-Through-Rate-Prediction"><a href="#DSIN-Deep-Session-Interest-Network-for-Click-Through-Rate-Prediction" class="headerlink" title="DSIN(Deep Session Interest Network for Click-Through Rate Prediction)"></a>DSIN(Deep Session Interest Network for Click-Through Rate Prediction)</h1><p><a href="https://arxiv.org/abs/1905.06482">paper</a></p>
<ul>
<li><strong>session内行为同构，不同session行为异构</strong><ul>
<li>Bias encoding + Transformer：获取session内的兴趣表达</li>
<li>Bi-LSTM: session间的序列关系</li>
<li>activation unit: 类似于din，获得行为序列表达与target item的关系</li>
</ul>
</li>
</ul>
<h2 id="Model-2"><a href="#Model-2" class="headerlink" title="Model"></a>Model</h2><img src="/2022/03/09/seq/dsin.png" class="" title="dsin">
<h3 id="Session-Division-Layer"><a href="#Session-Division-Layer" class="headerlink" title="Session Division Layer"></a>Session Division Layer</h3><ul>
<li>长度为n的seq，切分为K个session，session内有T个item，一个item D维，一个session 30min</li>
<li>得到Q ∈ R^(K*T*D)</li>
</ul>
<h3 id="Session-Interest-Extractor-Layer"><a href="#Session-Interest-Extractor-Layer" class="headerlink" title="Session Interest Extractor Layer"></a>Session Interest Extractor Layer</h3><ul>
<li><strong>session内的序列建模</strong></li>
<li>bias encoding: BE(k, t, c)指session k，item t下，位置c的偏置值<script type="math/tex; mode=display">\mathbf{BE}_{(k, t, c)}=\mathbf{w}_{k}^{K}+\mathbf{w}_{t}^{T}+\mathbf{w}_{c}^{C}</script></li>
</ul>
<script type="math/tex; mode=display">\mathbf{Q}=\mathbf{Q}+\mathbf{B} \mathbf{E}</script><ul>
<li>分别对每个session k：<ul>
<li>过一个Transformer得到<script type="math/tex">{I}_{k}^{Q}</script></li>
<li>做avg，得到Ik，shape为K*D：<script type="math/tex; mode=display">\mathbf{I}_{k}=\operatorname{Avg}\left(\mathbf{I}_{k}^{Q}\right)</script></li>
</ul>
</li>
</ul>
<h3 id="Session-Interest-Interacting-Layer"><a href="#Session-Interest-Interacting-Layer" class="headerlink" title="Session Interest Interacting Layer"></a>Session Interest Interacting Layer</h3><ul>
<li><strong>session间的序列关系</strong></li>
<li>Bi-LSTM</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{l}
\mathbf{i}_{t}=\sigma\left(\mathbf{W}_{x i} \mathbf{I}_{t}+\mathbf{W}_{h i} \mathbf{h}_{t-1}+\mathbf{W}_{c i} \mathbf{c}_{t-1}+\mathbf{b}_{i}\right)\\
\mathbf{f}_{t}=\sigma\left(\mathbf{W}_{x f} \mathbf{I}_{t}+\mathbf{W}_{h f} \mathbf{h}_{t-1}+\mathbf{W}_{c f} \mathbf{c}_{t-1}+\mathbf{b}_{f}\right)\\
\mathbf{c}_{t}=\mathbf{f}_{t} \mathbf{c}_{t-1}+\mathbf{i}_{t} \tanh \left(\mathbf{W}_{x c} \mathbf{I}_{t}+\mathbf{W}_{h c} \mathbf{h}_{t-1}+\mathbf{b}_{c}\right)\\
\mathbf{o}_{t}=\sigma\left(\mathbf{W}_{x o} \mathbf{I}_{t}+\mathbf{W}_{h o} \mathbf{h}_{t-1}+\mathbf{W}_{c o} \mathbf{c}_{t}+\mathbf{b}_{o}\right)\\
\mathbf{h}_{t}=\mathbf{o}_{t} \tanh \left(\mathbf{c}_{t}\right)\\
\mathbf{H}_{t}=\overrightarrow{\mathbf{h}_{f t}} \oplus \overleftarrow{\mathbf{h}_{b t}}
\end{array}</script><h3 id="Session-Interest-Activating-Layer"><a href="#Session-Interest-Activating-Layer" class="headerlink" title="Session Interest Activating Layer"></a>Session Interest Activating Layer</h3><ul>
<li><strong>与target交互</strong></li>
<li>包括两部分：<ul>
<li>Ik与target的attention：未建模session间关系</li>
<li>Hk与target的attention：建模session间关系之后</li>
</ul>
</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><img src="/2022/03/09/seq/dsin_result.jpeg" class="" title="dsin_result">
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><ul>
<li>DIN-RNN的效果差与DIN，而DSIN-BE的效果好于DSIN-BE-No-SIIL：说明切分session后，序列建模有效，而切分前，序列建模效果有损。原因猜测为，用户行为在长期来看是跳跃的，序列建模可能在某些时间点出有很大的噪声</li>
</ul>
<h1 id="DMIN-Deep-Multi-Interest-Network-for-Click-through-Rate-Prediction"><a href="#DMIN-Deep-Multi-Interest-Network-for-Click-through-Rate-Prediction" class="headerlink" title="DMIN(Deep Multi-Interest Network for Click-through Rate Prediction)"></a>DMIN(Deep Multi-Interest Network for Click-through Rate Prediction)</h1><p><a href="https://www.researchgate.net/publication/345125472_Deep_Multi-Interest_Network_for_Click-through_Rate_Prediction">paper</a>、<a href="https://www.jianshu.com/p/69929b24bb37">推荐系统遇上深度学习(一零零)-[阿里]深度多兴趣网络DMIN</a></p>
<ul>
<li>利用multi-head attention机制，建模用户多兴趣</li>
<li>引入DIEN中的aux loss</li>
</ul>
<h2 id="Model-3"><a href="#Model-3" class="headerlink" title="Model"></a>Model</h2><img src="/2022/03/09/seq/dmin.jpg" class="" title="dmin">
<h3 id="Behavior-Refiner-Layer"><a href="#Behavior-Refiner-Layer" class="headerlink" title="Behavior Refiner Layer"></a>Behavior Refiner Layer</h3><ul>
<li>multi-head attention，提炼用户行为序列，为每个历史行为生成一个新的embedding。HR为head数（R表示Refine），xb的shape: T*D，W: T*Dh，得到T*Dh，concat后得到T*(Dh*HR)<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{head}_{h} &=\operatorname{Attention}\left(\mathbf{x}_{b} \mathbf{W}_{h}^{Q}, \mathbf{x}_{b} \mathbf{W}_{h}^{K}, \mathbf{x}_{b} \mathbf{W}_{h}^{V}\right) \\
&=\operatorname{Softmax}\left(\frac{\mathbf{x}_{b} \mathbf{W}_{h}^{Q} \cdot\left(\mathbf{x}_{b} \mathbf{W}_{h}^{K}\right)^{\top}}{\sqrt{d_{h}}} \cdot \mathbf{x}_{b} \mathbf{W}_{h}^{V}\right)
\end{aligned}</script></li>
</ul>
<script type="math/tex; mode=display">
\mathrm{Z}=\text { MultiHead }\left(\mathbf{x}_{b}\right)=\text { Concat }\left(\text { head }_{1}, \text { head }_{2}, \ldots, \text { head }_{H_{R}}\right) \mathbf{W}^{O}</script><ul>
<li>aux loss: 同DIEN</li>
</ul>
<h3 id="Multi-Interest-Extractor-Layer"><a href="#Multi-Interest-Extractor-Layer" class="headerlink" title="Multi-Interest Extractor Layer"></a>Multi-Interest Extractor Layer</h3><ul>
<li>multi-head attention，HE个head<script type="math/tex; mode=display">
\begin{aligned}
\text { head }_{h}^{\prime} &=\operatorname{Attention}\left(\mathbf{Z W}_{h}^{\prime Q}, \mathbf{Z W}_{h}^{\prime K}, \mathbf{Z W}_{h}^{\prime V}\right) \\
&=\operatorname{Softmax}\left(\frac{\mathbf{Z W}_{h}^{\prime} \begin{array}{l}
Q \\
h
\end{array} \cdot\left(\mathbf{Z W}_{h}^{\prime K}\right)^{\top}}{\sqrt{d_{\text {model }}}} \cdot \mathbf{Z W}_{h}^{\prime V}\right)
\end{aligned}</script></li>
<li>类似din，得到第h个兴趣，Ijh为第h个head的第j个item，xt为target item,pj为position encoding：<script type="math/tex; mode=display">
\text { interest }_{h}=\sum_{j=1}^{T} a\left(\mathbf{I}_{j h}, \mathbf{x}_{t}, \mathbf{p}_{j}\right) \mathbf{I}_{j h}=\sum_{j=1}^{T} w_{j} \mathbf{I}_{j h}</script></li>
</ul>
<h1 id="MIND-Multi-Interest-Network-with-Dynamic-Routing-for-Recommendation-at-Tmall"><a href="#MIND-Multi-Interest-Network-with-Dynamic-Routing-for-Recommendation-at-Tmall" class="headerlink" title="MIND(Multi-Interest Network with Dynamic Routing for Recommendation at Tmall)"></a>MIND(Multi-Interest Network with Dynamic Routing for Recommendation at Tmall)</h1><p><a href="https://arxiv.org/pdf/1904.08030.pdf">paper</a>、<a href="https://www.jianshu.com/p/5e339afbf2e7">推荐系统遇上深度学习(七十四)-[天猫]MIND：多兴趣向量召回</a></p>
<ul>
<li>DIN：通过attention机制建模用户多样的兴趣，并得到一个embedding。MIND采用了另一种表达用户多样兴趣的思路：用多个embedding表示用户多样兴趣，具体来说，可以对用户历史行为的embedding进行聚类，聚类后的每个簇代表用户的一组兴趣。</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><img src="/2022/03/09/seq/mind.jpg" class="" title="mind">
<h3 id="behavior2interest-dynamic-routing"><a href="#behavior2interest-dynamic-routing" class="headerlink" title="behavior2interest dynamic routing"></a>behavior2interest dynamic routing</h3><h4 id="胶囊网络"><a href="#胶囊网络" class="headerlink" title="胶囊网络"></a>胶囊网络</h4><ul>
<li>背景：传统的神经网络输入一组标量，对这组标量求加权和，之后输入非线性激活函数得到一个标量的输出。而<strong>Capsule输入是一组向量，对这组向量进行仿射变换之后求加权和，把加权和输入非线性激活函数，如此经过j次迭代得到一个向量的输出</strong>。Hinton提出Capsule Network是为了解决传统的CNN中只能编码某个特征是否存在而<strong>无法编码特征的orientation</strong>。</li>
<li>来自<a href="https://zhuanlan.zhihu.com/p/68897114">zhihu</a><img src="/2022/03/09/seq/capsule.jpeg" class="" title="capsule"></li>
<li>一个两层胶囊网络（从low-level到high-level）：<ul>
<li>low-level: m个Nl维度的vector; high-level: n个Nh维度的vector</li>
<li>对每一个low-level vector做映射，然后做加权融合（softmax），再做非线性激活</li>
<li>其中softmax需要参数，参数需要迭代求解：初始化为0，即权重为1/m；后续迭代更新<img src="/2022/03/09/seq/capsule2.png" class="" title="capsule2">
</li>
</ul>
</li>
</ul>
<h4 id="B2I-Dynamic-Routing"><a href="#B2I-Dynamic-Routing" class="headerlink" title="B2I Dynamic Routing"></a>B2I Dynamic Routing</h4><p> MIND中的capsule:<br> （在MIND的中我们只要记住Capsule可以接受一组向量输入，输出一个向量；如果我们K个capsule，就会有K个输出向量）</p>
]]></content>
      <categories>
        <category>MachineLearning</category>
        <category>广告推荐</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Seq Model</tag>
      </tags>
  </entry>
  <entry>
    <title>tools-vim&amp;tmux</title>
    <url>/2021/12/10/tools/</url>
    <content><![CDATA[<h1 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h1><p>Some useful tools.</p>
<h2 id="Vim"><a href="#Vim" class="headerlink" title="Vim"></a>Vim</h2><p><a href="https://github.com/Chenzk1/vimrc">https://github.com/Chenzk1/vimrc</a></p>
<h2 id="Tmux"><a href="#Tmux" class="headerlink" title="Tmux"></a>Tmux</h2><p><a href="https://github.com/gpakosz/.tmux">https://github.com/gpakosz/.tmux</a></p>
]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-Apriori</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94Apriori/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]</p>
<h2 id="协同过滤推荐有哪些类型"><a href="#协同过滤推荐有哪些类型" class="headerlink" title="协同过滤推荐有哪些类型"></a>协同过滤推荐有哪些类型</h2><ul>
<li><p>基于用户(user-based)的协同过滤</p>
<p>基于用户(user-based)的协同过滤主要考虑的是用户和用户之间的相似度，只要找出相似用户喜欢的物品，并预测目标用户对对应物品的评分，就可以找到评分最高的若干个物品推荐给用户。 </p>
</li>
<li><p>基于项目(item-based)的协同过滤</p>
<p>基于项目(item-based)的协同过滤和基于用户的协同过滤类似，只不过这时我们转向找到物品和物品之间的相似度，只有找到了目标用户对某些物品的评分，那么我们就可以对相似度高的类似物品进行预测，将评分最高的若干个相似物品推荐给用户 </p>
</li>
<li><p>基于模型(model based)的协同过滤 </p>
<p>用机器学习的思想来建模解决，主流的方法可以分为：用关联算法，聚类算法，分类算法，回归算法，矩阵分解，神经网络,图模型以及隐语义模型来解决。</p>
</li>
</ul>
<a id="more"></a>
<h2 id="基于模型的协同过滤"><a href="#基于模型的协同过滤" class="headerlink" title="基于模型的协同过滤"></a>基于模型的协同过滤</h2><ul>
<li><p>用关联算法做协同过滤</p>
<p>做频繁集挖掘，找到满足支持度阈值的关联物品的频繁N项集或者序列。将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括<strong>支持度</strong>，<strong>置信度</strong>和<strong>提升度</strong>等。 常用的关联推荐算法有<strong>Apriori</strong>，<strong>FP Tree</strong>和<strong>PrefixSpan</strong> </p>
</li>
<li><p>用聚类算法做协同过滤</p>
<ul>
<li><p>基于用户聚类，则可以将用户按照一定距离度量方式分成不同的目标人群，将<strong>同样目标人群评分高的物品推荐给目标用户</strong>。</p>
</li>
<li><p>基于物品聚类，则是<strong>将用户评分高物品的相似同类物品推荐给用户</strong>。常用的聚类推荐算法有<strong>K-Means</strong>, <strong>BIRCH</strong>, <strong>DBSCAN</strong>和<strong>谱聚类</strong></p>
</li>
</ul>
</li>
<li><p>用分类算法做协同过滤</p>
<p>设置一份评分阈值，评分高于阈值的就是推荐，评分低于阈值就是不推荐，我们<strong>将问题变成了一个二分类问题</strong>。虽然分类问题的算法多如牛毛，但是目前使用最广泛的是逻辑回归。因为<strong>逻辑回归的解释性比较强</strong>，每个物品是否推荐我们都有一个明确的概率放在这，同时可以对数据的特征做工程化，得到调优的目的。常见的分类推荐算法有逻辑回归和朴素贝叶斯，两者的特点是解释性很强。</p>
</li>
<li><p>用回归算法做协同过滤</p>
<p>评分可以是一个连续的值而不是离散的值，<strong>通过回归模型</strong>我们可以得到目标用户对某商品的<strong>预测打分</strong>。常用的回归推荐算法有Ridge回归，回归树和支持向量回归。</p>
</li>
<li><p>用矩阵分解做协同过滤</p>
<p>用矩阵分解做协同过滤是目前使用也很广泛的一种方法。由于传统的奇异值分解SVD要求矩阵不能有缺失数据，必须是稠密的，而我们的用户物品评分矩阵是一个很典型的稀疏矩阵，直接使用传统的SVD到协同过滤是比较复杂的。 </p>
</li>
<li><p>用神经网络做协同过滤</p>
<p>用神经网络乃至深度学习做协同过滤应该是以后的一个趋势。目前比较主流的用两层神经网络来做推荐算法的是限制玻尔兹曼机(RBM) </p>
</li>
<li><p>用隐语义模型做协同过滤</p>
<p>隐语义模型主要是基于NLP的，涉及到<strong>对用户行为的语义分析来做评分推荐</strong>，主要方法有隐性语义分析LSA和隐含狄利克雷分布LDA，</p>
</li>
<li><p>用图模型做协同过滤</p>
<p>用图模型做协同过滤，则将用户之间的相似度放到了一个图模型里面去考虑，常用的算法是SimRank系列算法和马尔科夫模型算法。</p>
<h2 id="频繁项集的评估标准"><a href="#频繁项集的评估标准" class="headerlink" title="频繁项集的评估标准"></a>频繁项集的评估标准</h2></li>
<li><p>支持度: </p>
<ul>
<li>支持度就是几个关联的数据在数据集中出现的次数占总数据集的比重。或者说几个数据关联出现的概率。 <script type="math/tex; mode=display">
\text {Support} (X, Y)=P(X Y)=\frac{\text { number }(X Y)}{\text { num (AllSamples) }}</script></li>
</ul>
</li>
</ul>
<ul>
<li><p>置信度:</p>
<ul>
<li>一个数据出现后，另一个数据出现的概率，或者说数据的条件概率。 <script type="math/tex; mode=display">
\text {Confidence }(X \Leftarrow Y)=P(X | Y)=\frac{P(X Y)}{ P(Y)}</script></li>
</ul>
</li>
<li><p>提升度 ：</p>
<ul>
<li>提升度表示含有Y的条件下，同时含有X的概率，与X总体发生的概率之比 <script type="math/tex; mode=display">
\text {Lift }(X \Leftarrow Y)=\frac{P(X | Y)}{ P(X)} = \frac{\text { Confidence }(X \Leftarrow Y) }{ P(X)}</script></li>
</ul>
</li>
<li><p>注意：</p>
<ul>
<li>支持度高的数据不一定构成频繁项集，但是支持度太低的数据肯定不构成频繁项集。 </li>
<li>提升度体先了$X$和$Y$之间的关联关系, 提升度大于1则$X\Leftarrow Y$是有效的强关联规则， 提升度小于等于1则$X\Leftarrow Y$是无效的强关联规则 。一个特殊的情况，如果$X$和$Y$独立,则$\operatorname{Lift}(X \Leftarrow Y)=1$，因此$P(X | Y)=P(X)$</li>
</ul>
</li>
</ul>
<h2 id="使用Aprior算法找出频繁k项集"><a href="#使用Aprior算法找出频繁k项集" class="headerlink" title="使用Aprior算法找出频繁k项集"></a>使用Aprior算法找出频繁k项集</h2><p>输入：数据集合$D$，支持度阈值$\alpha$</p>
<p>输出：最大的频繁$k$项集</p>
<ul>
<li><p>扫描整个数据集，得到所有出现过的数据，作为候选频繁1项集。$k=1$，频繁0项集为空集。</p>
</li>
<li><p>挖掘频繁$k$项集</p>
<ul>
<li>扫描数据计算候选频繁$k$项集的支持度</li>
<li>去除候选频繁$k$项集中支持度低于阈值的数据集,得到频繁$k$项集。如果得到的频繁$k$项集为空，则直接返回频繁$k-1$项集的集合作为算法结果，算法结束。如果得到的频繁$k$项集只有一项，则直接返回频繁$k$项集的集合作为算法结果，算法结束。</li>
<li>基于频繁$k$项集，连接生成候选频繁$k+1$项集。</li>
</ul>
</li>
<li><p>令$k=k+1$，转入步骤挖掘频繁$k$项集。</p>
</li>
</ul>
<p>从算法的步骤可以看出，Aprior算法每轮迭代都要扫描数据集，因此在数据集很大，数据种类很多的时候，算法效率很低。</p>
<p>具体实现:</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170117161036255-1753157633.png" alt=""></p>
<h2 id="使用Aprior算法找出强关联规则"><a href="#使用Aprior算法找出强关联规则" class="headerlink" title="使用Aprior算法找出强关联规则"></a>使用Aprior算法找出强关联规则</h2><ul>
<li><p>强关联规则:</p>
<ul>
<li>如果规则$R$:$\Rightarrow $满足 :</li>
</ul>
<script type="math/tex; mode=display">
\tag{1} { support }(X \Rightarrow Y) \geq \min {sup}</script><script type="math/tex; mode=display">
\tag{2} confidence (X \Rightarrow Y) \geq \min conf</script><p>称关联规则$X\Rightarrow Y$为强关联规则,否则称关联规则$X\Rightarrow Y$为弱关联规则。在挖掘关联规则时,产生的关联规则要经过$\min sup$和$\min conf$的衡量筛选出来的强关联规则才能用商家的决策 </p>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Apriori</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-RNN&amp;LSTM</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94RNN+LSTM/</url>
    <content><![CDATA[<p><meta name="referrer" content="no-referrer"/><br>[TOC]</p>
<h2 id="LSTM产生的原因"><a href="#LSTM产生的原因" class="headerlink" title="LSTM产生的原因"></a>LSTM产生的原因</h2><ul>
<li><strong>RNN在处理长期依赖</strong>（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成<strong>梯度消失或者梯度膨胀</strong>的现象。RNN结构之所以出现梯度爆炸或者梯度消失，最本质的原因是因为梯度在传递过程中存在极大数量的连乘 。</li>
<li>相对于RNN，LSTM的神经元加入了<strong>输入门i、遗忘门f、输出门o 、内部记忆单元c</strong> </li>
</ul>
<a id="more"></a>
<h2 id="分别介绍一下输入门i、遗忘门f、输出门o-、内部记忆单元c"><a href="#分别介绍一下输入门i、遗忘门f、输出门o-、内部记忆单元c" class="headerlink" title="分别介绍一下输入门i、遗忘门f、输出门o 、内部记忆单元c"></a>分别介绍一下输入门i、遗忘门f、输出门o 、内部记忆单元c</h2><ul>
<li><p>内部记忆单元$c$</p>
<ul>
<li>类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。  </li>
<li>操作步骤：</li>
<li>示意图<br><img src="https://img-blog.csdn.net/20170919124608594?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbHJlYWRlcmw=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></li>
</ul>
</li>
</ul>
<ul>
<li><p>遗忘门$f$</p>
<ul>
<li><p>将<strong>内部记忆单元</strong>中的信息选择性的遗忘</p>
</li>
<li><p>操作步骤：</p>
<ul>
<li>读取：$h_{t-1}$、$x_t$，</li>
<li>输出：$f<em>{t}=\sigma\left(W</em>{f} \cdot\left[h<em>{t-1}, x</em>{t}\right]+b_{f}\right)$</li>
<li>$\sigma$表示一个在 0 到 1 之间的数值。1 表示“完全保留”，0 表示“完全舍弃”</li>
</ul>
</li>
<li><p>示意图<br><img src="https://pic4.zhimg.com/80/v2-11ca9e4a19504874202ac9880da9840f_1440w.jpg" alt="遗忘门"></p>
</li>
</ul>
<script type="math/tex; mode=display">
f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)</script></li>
<li><p>输入门$i$</p>
<ul>
<li><p>将新的信息记录到<strong>内部记忆单元</strong>中</p>
</li>
<li><p>操作步骤：</p>
<ul>
<li><p>步骤一：$sigmoid$ 层称 <strong>输入门层</strong>决定什么值我们将要更新。</p>
</li>
<li><p>步骤二：$tanh$ 层创建一个新的候选值向量$\tilde{C}_t$加入到状态中。其示意图如下：</p>
<p><img src="https://img-blog.csdn.net/20170301115512234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="1578624753318"></p>
<script type="math/tex; mode=display">
i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right)</script><script type="math/tex; mode=display">
\tilde{C}_{t}=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)</script></li>
<li><p>步骤三：将$C<em>{t-1}$更新为$C</em>{t}$。将旧状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息。接着加上$i_t * \tilde{C}_t$得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：</p>
<p><img src="https://img-blog.csdn.net/20170301120227745?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSmVycl9feQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="1578624724604"></p>
<script type="math/tex; mode=display">
C_{t}=f_{t} * C_{t-1}+i_{t} * \tilde{C}_{t}</script></li>
</ul>
</li>
</ul>
</li>
<li><p>输出门$o$</p>
<ul>
<li><p>确定隐层$h_t$输出什么值</p>
</li>
<li><p>操作步骤：</p>
<ul>
<li><p>步骤一：通过sigmoid 层来确定细胞状态的哪个部分将输出。</p>
</li>
<li><p>步骤二：把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p><img src="https://pic4.zhimg.com/80/v2-f928df2c02e17fb5da95bf8354880613_1440w.jpg" alt="输出门"></p>
<script type="math/tex; mode=display">
\begin{array}{l}
{o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right)} \\
{h_{t}=o_{t} * \tanh \left(C_{t}\right)}
\end{array}</script></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>NLP</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-决策树</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]</p>
<h2 id="1-简单介绍决策树算法"><a href="#1-简单介绍决策树算法" class="headerlink" title="1. 简单介绍决策树算法"></a>1. 简单介绍决策树算法</h2><ul>
<li>决策树将算法组织成一颗树的形式。其实这就是将平时所说的<strong>if-then语句</strong>构建成了树的形式。决策树主要包括<strong>三个部分：内部节点、叶节点、边。内部节点是划分的特征，边代表划分的条件，叶节点表示类别。</strong></li>
<li>构建决策树 就是一个递归的选择内部节点，计算划分条件的边，最后到达叶子节点的过程。 决策树在本质上是一组嵌套的if-else判定规则，从数学上看是分段常数函数，对应于用平行于坐标轴的平面对空间的划分。判定规则是人类处理很多问题时的常用方法，这些规则是我们通过经验总结出来的，而决策树的这些规则是通过训练样本自动学习得到的。</li>
<li>训练时，通过最大化Gini或者其他指标来寻找最佳分裂。决策树可以输特征向量每个分量的重要性。</li>
<li><strong>决策树是一种判别模型，既支持分类问题，也支持回归问题，是一种非线性模型（分段线性函数不是线性的）。它天然的支持多分类问题。</strong></li>
</ul>
<a id="more"></a>
<h2 id="2-决策树和条件概率分布的关系？"><a href="#2-决策树和条件概率分布的关系？" class="headerlink" title="2. 决策树和条件概率分布的关系？"></a>2. 决策树和条件概率分布的关系？</h2><p><strong>决策树可以表示成给定条件下类的条件概率分布。</strong> </p>
<p>决策树中的每一条路径都对应是划分的一个条件概率分布. 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大。</p>
<h2 id="3-信息增益比相对信息增益有什么好处？"><a href="#3-信息增益比相对信息增益有什么好处？" class="headerlink" title="3. 信息增益比相对信息增益有什么好处？"></a>3. 信息增益比相对信息增益有什么好处？</h2><ul>
<li><p>使用信息增益时：模型<strong>偏向于选择取值较多</strong>的特征</p>
</li>
<li><p>使用信息增益比时：<strong>对取值多的特征加上的惩罚</strong>，对这个问题进行了校正。</p>
</li>
</ul>
<h2 id="4-ID3算法—-gt-C4-5算法—-gt-CART算法"><a href="#4-ID3算法—-gt-C4-5算法—-gt-CART算法" class="headerlink" title="4. ID3算法—&gt;C4.5算法—&gt; CART算法"></a>4. ID3算法—&gt;C4.5算法—&gt; CART算法</h2><ul>
<li><p>$ID3$</p>
<ul>
<li>$ID3$算法没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。</li>
<li>$ID3$算法采用信息增益大的特征优先建立决策树的节点，偏向于取值比较多的特征</li>
<li>$ID3$算法对于缺失值的情况没有做考虑</li>
<li>$ID3$算法没有考虑过拟合的问题</li>
</ul>
</li>
<li><p>$C4.5$在$ID3$算法上面的改进</p>
<ul>
<li>连续的特征离散化 </li>
<li>使用信息增益比 </li>
<li>通过剪枝算法解决过拟合</li>
</ul>
</li>
<li><p>$C4.5$的不足：</p>
<ul>
<li>$C4.5$生成的是多叉树</li>
<li>$C4.5$只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</li>
<li>$C4.5$由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算</li>
</ul>
</li>
<li><p>$CART$算法 </p>
<ul>
<li>可以做回归，也可以做分类， </li>
<li>使用基尼系数来代替信息增益比 </li>
<li>$CART$分类树离散值的处理问题，采用的思路是不停的二分离散特征。 </li>
</ul>
</li>
</ul>
<h2 id="5-决策树的缺失值是怎么处理的"><a href="#5-决策树的缺失值是怎么处理的" class="headerlink" title="5. 决策树的缺失值是怎么处理的"></a>5. 决策树的缺失值是怎么处理的</h2><ul>
<li><p>如何在特征值缺失的情况下进行划分特征的选择？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">（比如“色泽”这个特征有的样本在该特征上的值是缺失的，那么该如何计算“色泽”的信息增益？）</span><br></pre></td></tr></table></figure>
<ul>
<li><p>每个样本设置一个权重（初始可以都为1） </p>
</li>
<li><p>划分数据，一部分是有特征值$a$的数据，另一部分是没有特征值$a$的数据,记为$\tilde{D}$，</p>
</li>
<li><p><strong>对</strong>没有缺失特征值$a$的<strong>数据集$\tilde{D}$，</strong>来和对应的特征$A$的各个特征值一起<strong>计算加权重后的信息增益比</strong>，最后乘上一个系数$\rho$ 。</p>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
\rho=\frac{\sum_{x \in \tilde{D}} w_{x}}{\sum_{x \in {D}} w_{x}}</script><script type="math/tex; mode=display">
\tilde{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}} \quad(1 \leq \mathrm{k} \leq|y|)</script><script type="math/tex; mode=display">
\tilde{r}_{v}=\frac{\sum_{x \in \tilde D^{v}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}} \quad(1 \leq v \leq V)</script><p>​        假设特征$A$有$v$个取值${a_1,a_2 \dots a_v}$</p>
<p>​        $\tilde D$：该特征上没有缺失值的样本</p>
<p>​        $\tilde D_k$：$\tilde D$中属于第$k$类的样本子集</p>
<p>​        $\tilde D^v$：$\tilde D$中在特征$a$上取值为$a_v$的样本子集</p>
<p>​        $\rho$：无特征$A$缺失的样本加权后所占加权总样本的比例。</p>
<p>​        $\tilde{p}_{k}$：无缺失值样本第$k$类所占无缺失值样本的比例</p>
<p>​        $\tilde{r}_{v}$：无缺失值样本在特征$a$上取值$a^v$的样本所占无缺失值样本的比例</p>
<p>​        新的信息增益公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\operatorname{Gain}(D, a)=\rho \times \operatorname{Gain}(\tilde{D}, a)=\rho \times\left(\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right)\right)\\
&\operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k} \log _{2} \tilde{p}_{k}
\end{aligned}</script><ul>
<li><p>给定划分特征，若样本在该特征上的值是缺失的，那么该如何对这个样本进行划分？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">（即到底把这个样本划分到哪个结点里？）</span><br></pre></td></tr></table></figure>
<ul>
<li>让包含缺失值的样本以不同的概率划分到不同的子节点中去。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2&#x2F;9,3&#x2F;9, 4&#x2F;9。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="6-决策树的目标函数是什么？"><a href="#6-决策树的目标函数是什么？" class="headerlink" title="6. 决策树的目标函数是什么？"></a><strong>6. 决策树的目标函数是什么？</strong></h2><script type="math/tex; mode=display">
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+a|T|</script><script type="math/tex; mode=display">
H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}</script><p>其中$|T|$代表叶节点个数</p>
<p>$N_t$表示具体某个叶节点的样例数</p>
<p> $H_t(T)$ 表示叶节点$t$上的经验熵</p>
<p>$\alpha|T|$为正则项，$\alpha \geqslant 0 $ 为参数。</p>
<h2 id="7-决策树怎么处理连续性特征？"><a href="#7-决策树怎么处理连续性特征？" class="headerlink" title="7. 决策树怎么处理连续性特征？"></a>7. 决策树怎么处理连续性特征？</h2><p>因为连续特征的可取值数目不再有限，因此不能像前面处理离散特征枚举离散特征取值来对结点进行划分。因此需要连续特征离散化，常用的离散化策略是二分法，这个技术也是$C4.5$中采用的策略。下面来具体介绍下，如何采用二分法对连续特征离散化： </p>
<ul>
<li><p>训练集D，连续特征$A$，其中A有n个取值</p>
</li>
<li><p>对$A$的取值进行从小到大排序得到：${a_1,a_2\dots a_n}$</p>
</li>
<li><p>寻找划分点$t$，$t$将D分为子集$D<em>{t}^{-}$与$D</em>{t}^{+}$</p>
<ul>
<li>$D_{t}^{-}$：特征$A$上取值不大于$t$的样本</li>
<li>$D_{t}^{+}$：特征$A$上取值大于$t$的样本</li>
</ul>
</li>
<li><p>对相邻的特征取值$a<em>i$与$a</em>{i+1}$，t再区间$[a<em>i,a</em>{i+1})$中取值所产生的划分结果相同，因此对于连续特征$A$,包含有$n-1$个元素的后选划分点集合</p>
</li>
</ul>
<script type="math/tex; mode=display">
T_a = \{\frac{a_i + a_{i+1}}{2}|1\leq{i}\leq{n-1} \}</script><ul>
<li><p>把区间$[a<em>i,a</em>{i+1})$的中位点$\frac{a<em>i + a</em>{i+1}}{2}$作为候选划分点</p>
</li>
<li><p>按照处理离散值那样来选择最优的划分点,使用公式：</p>
<script type="math/tex; mode=display">
Gain(D,a) =\underbrace{max}_{t\in T_a}Gain(D,a,t) = \underbrace{max}_{t\in T_a}\ (Ent(D) - \sum_{\lambda \in \{-,+ \}}\frac{|D_t^{\lambda}|}{|D|}Ent(D_t^{\lambda}))</script><p>其中$Gain(D,a,t)$是样本集$D$基于划分点$t$二分之后的信息增益。划分点时候选择使用$Gain(D,a,t)$最大的划分点。</p>
</li>
</ul>
<h2 id="8-决策树怎么防止过拟合？"><a href="#8-决策树怎么防止过拟合？" class="headerlink" title="8. 决策树怎么防止过拟合？"></a><strong>8. 决策树怎么防止过拟合？</strong></h2><h3 id="构建随机森林"><a href="#构建随机森林" class="headerlink" title="构建随机森林"></a>构建随机森林</h3><ul>
<li>构建随机森林</li>
</ul>
<h3 id="控制树的结构复杂程度"><a href="#控制树的结构复杂程度" class="headerlink" title="控制树的结构复杂程度"></a>控制树的结构复杂程度</h3><ul>
<li><p>预剪枝(提前停止)：控制<strong>深度、当前的节点数、分裂对测试集的准确度提升大小</strong></p>
<ul>
<li>限制树的高度，可以利用交叉验证选择</li>
<li>利用分类指标，如果下一次切分没有降低误差，则停止切分</li>
<li>限制树的节点个数，比如某个节点小于100个样本，停止对该节点切分</li>
</ul>
</li>
<li><p>后剪枝(自底而上)：<strong>生成决策树、交叉验证剪枝：子树删除，节点代替子树、测试集准确率判断决定剪枝</strong></p>
<ul>
<li>在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作. 剪枝的目的就是防止过拟合，是模型在测试数据上变现良好，更加鲁棒。</li>
</ul>
</li>
</ul>
<h2 id="9-如果特征很多，决策树中最后没有用到的特征一定是无用吗？"><a href="#9-如果特征很多，决策树中最后没有用到的特征一定是无用吗？" class="headerlink" title="9. 如果特征很多，决策树中最后没有用到的特征一定是无用吗？"></a>9. 如果特征很多，决策树中最后没有用到的特征一定是无用吗？</h2><p>不是无用的，从两个角度考虑：</p>
<ul>
<li><p><strong>特征替代性</strong>，如果可以已经使用的特征$A$和特征$B$可以提点特征$C$，特征$C$可能就没有被使用，但是如果把特征$C$单独拿出来进行训练，依然有效</p>
</li>
<li><p>决策树的每一条路径就是<strong>计算条件概率的条件</strong>，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据.</p>
</li>
</ul>
<h2 id="10-决策树的优缺点？"><a href="#10-决策树的优缺点？" class="headerlink" title="10.决策树的优缺点？"></a>10.决策树的优缺点？</h2><ul>
<li><p>优点: </p>
<ul>
<li>简单直观，生成的决策树很直观。</li>
<li>基本不需要预处理，不需要提前归一化，处理缺失值。</li>
<li>既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</li>
<li>可以处理多维度输出的分类问题。</li>
<li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li>
<li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li>
<li>对于异常点的容错能力好，健壮性高。</li>
</ul>
</li>
<li><p>缺点:</p>
<ul>
<li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li>
<li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li>
<li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li>
<li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li>
<li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li>
</ul>
</li>
</ul>
<h2 id="11-树形结构为什么不需要归一化"><a href="#11-树形结构为什么不需要归一化" class="headerlink" title="11. 树形结构为什么不需要归一化?"></a>11. 树形结构为什么不需要归一化?</h2><ul>
<li><strong>数值缩放不影响分裂点位置，对树模型的结构不造成影响</strong>。</li>
<li><p>按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。</p>
</li>
<li><p>树模型是<strong>不能进行梯度下降</strong>的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此<strong>树模型是阶跃的，阶跃点是不可导</strong>的，并且求导没意义，也就不需要归一化。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DecisionTree</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-梯度下降</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    <content><![CDATA[<h2 id="1-机器学习中为什么需要梯度下降"><a href="#1-机器学习中为什么需要梯度下降" class="headerlink" title="1. 机器学习中为什么需要梯度下降"></a>1. 机器学习中为什么需要梯度下降</h2><ul>
<li>梯度下降的作用：<ul>
<li>梯度下降是迭代法的一种，可以用于<strong>求解最小二乘问题</strong>。</li>
<li>在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，<strong>得到最小化的损失函数和模型参数值。</strong></li>
<li>如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。<strong>梯度下降法和梯度上升法可相互转换</strong>。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="2-梯度下降法缺点"><a href="#2-梯度下降法缺点" class="headerlink" title="2. 梯度下降法缺点"></a>2. 梯度下降法缺点</h2><p><strong>缺点</strong>：</p>
<ul>
<li>靠近极小值时收敛速度减慢。</li>
<li>直线搜索时可能会产生一些问题。</li>
<li>可能会“之字形”地下降。</li>
</ul>
<p><strong>注意</strong>：</p>
<ul>
<li>梯度是一个向量，即<strong>有方向有大小</strong>。 </li>
<li>梯度的方向是<strong>最大方向导数的方向</strong>。 </li>
<li>梯度的值是<strong>最大方向导数的值</strong>。</li>
</ul>
<h2 id="3-梯度下降法直观理解"><a href="#3-梯度下降法直观理解" class="headerlink" title="3. 梯度下降法直观理解"></a>3. 梯度下降法直观理解</h2><p>​    形象化举例，由上图所示，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。<br>​    由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<h2 id="4-梯度下降核心思想归纳"><a href="#4-梯度下降核心思想归纳" class="headerlink" title="4. 梯度下降核心思想归纳"></a>4. 梯度下降核心思想归纳</h2><ul>
<li>确定优化模型的假设函数及损失函数。</li>
<li>初始化参数，随机选取取值范围内的任意数；</li>
</ul>
<ul>
<li>迭代操作：<ul>
<li>计算当前梯度</li>
<li>修改新的变量</li>
<li>计算朝最陡的下坡方向走一步</li>
<li>判断是否需要终止，如否，<strong>梯度更新</strong></li>
</ul>
</li>
<li>得到全局最优解或者接近全局最优解。</li>
</ul>
<h2 id="5-如何对梯度下降法进行调优"><a href="#5-如何对梯度下降法进行调优" class="headerlink" title="5. 如何对梯度下降法进行调优"></a>5. 如何对梯度下降法进行调优</h2><p>实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：</p>
<p>(1)<strong>算法迭代步长$\alpha$选择。</strong><br>    在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。</p>
<p>(2)<strong>参数的初始值选择。</strong><br>    初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
<p>(3)<strong>标准化处理。</strong><br>    由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。</p>
<h2 id="6-随机梯度和批量梯度区别"><a href="#6-随机梯度和批量梯度区别" class="headerlink" title="6. 随机梯度和批量梯度区别"></a>6. 随机梯度和批量梯度区别</h2><p>​    随机梯度下降(SDG)和批量梯度下降(BDG)是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。<br>下面通过介绍两种梯度下降法的求解思路，对其进行比较。<br>假设函数为：</p>
<script type="math/tex; mode=display">
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n</script><p>损失函数为：</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1, ... , \theta_n) = 
            \frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)^2</script><p>其中，$m$为样本个数，$j$为参数个数。</p>
<p>1、 <strong>批量梯度下降的求解思路如下：</strong><br>a) 得到每个$\theta$对应的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i</script><p>b) 由于是求最小化风险函数，所以按每个参数 $\theta$ 的梯度负方向更新 $ \theta_i $ ：</p>
<script type="math/tex; mode=display">
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i</script><p>c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，如果样本数据很大，这种方法迭代速度就很慢。<br>相比而言，随机梯度下降可避免这种问题。</p>
<p>2、<strong>随机梯度下降的求解思路如下：</strong><br>a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。<br>损失函数可以写成如下这种形式，</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1, ... , \theta_n) = 
            \frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
            ,x^{j}_1,...,x^{j}_n))^2 = 
            \frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))</script><p>b)对每个参数 $ \theta$ 按梯度方向更新 $ \theta$：</p>
<script type="math/tex; mode=display">
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))</script><p>c) 随机梯度下降是通过每个样本来迭代更新一次。<br>随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。</p>
<p><strong>小结：</strong><br>随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">批量梯度下降</td>
<td style="text-align:left">a)采用所有数据来梯度下降。<br/>b)批量梯度下降法在样本量很大的时候，训练速度慢。</td>
</tr>
<tr>
<td style="text-align:center">随机梯度下降</td>
<td style="text-align:left">a)随机梯度下降用一个样本来梯度下降。<br/>b)训练速度很快。<br />c)随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br />d)收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</td>
</tr>
</tbody>
</table>
</div>
<p>下面介绍能结合两种方法优点的小批量梯度下降法。</p>
<p>3、 <strong>小批量(Mini-Batch)梯度下降的求解思路如下</strong><br>对于总数为$m$个样本的数据，根据样本的数据，选取其中的$n(1&lt; n&lt; m)$个子样本来迭代。其参数$\theta$按梯度方向更新$\theta_i$公式如下：</p>
<script type="math/tex; mode=display">
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
        ( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}</script><h2 id="7-各种梯度下降法性能比较"><a href="#7-各种梯度下降法性能比较" class="headerlink" title="7.  各种梯度下降法性能比较"></a>7.  各种梯度下降法性能比较</h2><p>​    下表简单对比随机梯度下降(SGD)、批量梯度下降(BGD)、小批量梯度下降(Mini-batch GD)、和Online GD的区别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">BGD</th>
<th style="text-align:center">SGD</th>
<th style="text-align:center">Mini-batch GD</th>
<th style="text-align:center">Online GD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">训练集</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">实时更新</td>
</tr>
<tr>
<td style="text-align:center">单次迭代样本数</td>
<td style="text-align:center">整个训练集</td>
<td style="text-align:center">单个样本</td>
<td style="text-align:center">训练集的子集</td>
<td style="text-align:center">根据具体算法定</td>
</tr>
<tr>
<td style="text-align:center">算法复杂度</td>
<td style="text-align:center">高</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">低</td>
</tr>
<tr>
<td style="text-align:center">时效性</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">高</td>
</tr>
<tr>
<td style="text-align:center">收敛性</td>
<td style="text-align:center">稳定</td>
<td style="text-align:center">不稳定</td>
<td style="text-align:center">较稳定</td>
<td style="text-align:center">不稳定</td>
</tr>
</tbody>
</table>
</div>
<p>​    Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p>
<p>​    Online GD在互联网领域用的较多，比如搜索广告的点击率(CTR)预估模型，网民的点击行为会随着时间改变。用普通的BGD算法(每天更新一次)一方面耗时较长(需要对所有历史数据重新训练)；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p>
<h2 id="8-推导多元函数梯度下降法的迭代公式。"><a href="#8-推导多元函数梯度下降法的迭代公式。" class="headerlink" title="8. 推导多元函数梯度下降法的迭代公式。"></a>8. 推导多元函数梯度下降法的迭代公式。</h2><p>根据多元函数泰勒公式，如果忽略一次以上的项，函数在$\mathbf{x}$点处可以展开为</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\Delta \mathbf{x})=f(\mathbf{x})+(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}+o(\|\mathbf{\Delta} \mathbf{x}\|)</script><p>对上式变形，函数的增量与自变量增量、函数梯度的关系为</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\Delta \mathbf{x})-f(\mathbf{x})=(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}+o(\|\Delta \mathbf{x}\|)</script><p>如果令$\Delta \mathbf{x}=-\nabla f(\mathbf{x})$则有</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\Delta \mathbf{x})-f(\mathbf{x}) \approx-(\nabla f(\mathbf{x}))^{\mathrm{T}} \nabla f(\mathbf{x}) \leq 0</script><p>即函数值减小。即有</p>
<script type="math/tex; mode=display">
f(\mathbf{x}+\Delta \mathbf{x}) \leq f(\mathbf{x})</script><p>梯度下降法每次的迭代增量为</p>
<script type="math/tex; mode=display">
\Delta \mathbf{x}=-\alpha \nabla f(\mathbf{x})</script><p>其中$\alpha$为人工设定的接近于的正数，称为步长或学习率。其作用是保证$\mathbf{x}+\Delta \mathbf{x}$在$\mathbf{x}$的<br>邻域内，从而可以忽略泰勒公式中的$o(|\Delta \mathbf{x}|)$项。</p>
<p>使用该增量则有</p>
<script type="math/tex; mode=display">
(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}=-\alpha(\nabla f(\mathbf{x}))^{\mathrm{T}}(\nabla f(\mathbf{x})) \leq 0</script><p>函数值下降。从初始点$\mathbf{x}_{0}$开始，反复使用如下<strong>迭代公式</strong></p>
<script type="math/tex; mode=display">
\mathbf{x}_{k+1}=\mathbf{x}_{k}-\alpha \nabla f\left(\mathbf{x}_{k}\right)</script><p>只要没有到达梯度为0的点，函数值会沿序列$\mathbf{x}<em>{k}$递减，最终收敛到梯度为0 的点。从$\mathbf{x}</em>{0}$<br>出发，用<strong>迭代公式</strong>进行迭代，会形成一个函数值递减的序列$\left{\mathbf{x}_{i}\right}$</p>
<script type="math/tex; mode=display">
f\left(\mathbf{x}_{0}\right) \geq f\left(\mathbf{x}_{1}\right) \geq f\left(\mathbf{x}_{2}\right) \geq \ldots \geq f\left(\mathbf{x}_{k}\right)</script><h2 id="9-梯度下降法如何判断是否收敛？"><a href="#9-梯度下降法如何判断是否收敛？" class="headerlink" title="9.  梯度下降法如何判断是否收敛？"></a>9.  梯度下降法如何判断是否收敛？</h2><p>迭代终止的条件是函数的梯度值为0(实际实现时是接近于0 即可)，此时认为已经达<br>到极值点。可以通过判定梯度的二范数是否充分接近于0 而实现。</p>
<h2 id="10-梯度下降法为什么要在迭代公式中使用步长系数？"><a href="#10-梯度下降法为什么要在迭代公式中使用步长系数？" class="headerlink" title="10. 梯度下降法为什么要在迭代公式中使用步长系数？"></a>10. 梯度下降法为什么要在迭代公式中使用步长系数？</h2><p>其作用是保证$\mathbf{x}+\Delta \mathbf{x}$在$\mathbf{x}$的邻域内，即控制增量的步长，从而可以忽略泰勒公式中的<br>$o(|\Delta \mathbf{x}|)$项。否则不能保证每次迭代时函数值下降。</p>
<h2 id="11-梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？"><a href="#11-梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？" class="headerlink" title="11. 梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？"></a>11. 梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？</h2><p>不能，可能收敛到鞍点，不是极值点。</p>
<h2 id="12-解释一元函数极值判别法则。"><a href="#12-解释一元函数极值判别法则。" class="headerlink" title="12. 解释一元函数极值判别法则。"></a>12. 解释一元函数极值判别法则。</h2><p>假设$x_0$为函数的驻点，可分为以下三种情况。<br>case1：在该点处的二阶导数大于0，则为函数的极小值点；<br>case2：在该点处的二阶导数小于0，则为极大值点；<br>case3：在该点处的二阶导数等于0，则情况不定，可能是极值点，也可能不是极值点。</p>
<h2 id="13-解释多元函数极值判别法则。"><a href="#13-解释多元函数极值判别法则。" class="headerlink" title="13. 解释多元函数极值判别法则。"></a>13. 解释多元函数极值判别法则。</h2><p>假设多元函数在点M的梯度为0 ，即M 是函数的驻点。其Hessian 矩阵有如下几种情<br>况。<br>case1：Hessian 矩阵正定，函数在该点有极小值。<br>case2：Hessian 矩阵负定，函数在该点有极大值。<br>case3：Hessian 矩阵不定，则不是极值点，称为鞍点。<br>Hessian 矩阵正定类似于一元函数的二阶导数大于0，负定则类似于一元函数的二阶导<br>数小于0。</p>
<h2 id="14-什么是鞍点？"><a href="#14-什么是鞍点？" class="headerlink" title="14. 什么是鞍点？"></a>14. 什么是鞍点？</h2><p><strong>Hessian 矩阵不定的点称为鞍点</strong>，它不是函数的极值点。</p>
<h2 id="15-解释什么是局部极小值，什么是全局极小值。"><a href="#15-解释什么是局部极小值，什么是全局极小值。" class="headerlink" title="15. 解释什么是局部极小值，什么是全局极小值。"></a>15. 解释什么是局部极小值，什么是全局极小值。</h2><ul>
<li>全局极小值<ul>
<li>假设$\mathbf{x}^{*}$是一个可行解，如果对可行域内所有点$\mathbf{x}$都有$f\left(\mathbf{x}^{*}\right) \leq f(\mathbf{x})$，则<br>称$\mathbf{x}^{*}$为全局极小值。</li>
</ul>
</li>
<li>局部极小值<ul>
<li>对于可行解$\mathbf{x}^{*}$，如果存在其$\delta$邻域，使得该邻域内的所有点即所有满足<br>$\left|\mathbf{x}-\mathbf{x}^{*}\right| \leq \delta$的点$\mathbf{x}$，都有$f\left(x^{*}\right) \leq f(x)$，则称$\mathbf{x}^{*}$为局部极小值。</li>
</ul>
</li>
</ul>
<h2 id="16-推导多元函数牛顿法的迭代公式。"><a href="#16-推导多元函数牛顿法的迭代公式。" class="headerlink" title="16. 推导多元函数牛顿法的迭代公式。"></a>16. 推导多元函数牛顿法的迭代公式。</h2><p>根据费马定理，函数在点$\mathbf{x}$ 处取得极值的必要条件是梯度为0</p>
<script type="math/tex; mode=display">
\nabla f(\mathbf{x})=\mathbf{0}</script><p>对于一般的函数，直接求解此方程组存在困难。对目标函数在$\mathbf{x}_{0}$ 处作二阶泰勒展开</p>
<script type="math/tex; mode=display">
f(\mathbf{x})=f\left(\mathbf{x}_{0}\right)+\nabla f\left(\mathbf{x}_{0}\right)^{\mathrm{T}}\left(\mathbf{x}-\mathbf{x}_{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{0}\right)^{\mathrm{T}} \nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)+o\left(\left\|\mathbf{k}-\mathbf{x}_{0}\right\|^{2}\right)</script><p>忽略二次以上的项，将目标函数近似成二次函数，等式两边同时对$\mathbf{x}$求梯度，可得</p>
<script type="math/tex; mode=display">
\nabla f(\mathbf{x}) \approx \nabla f\left(\mathbf{x}_{0}\right)+\nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)</script><p>其中 $\nabla^{2} f\left(\mathbf{x}<em>{0}\right)$为在$\mathbf{x}</em>{0}$ 处的Hessian 矩阵。令函数的梯度为0 ，有 </p>
<script type="math/tex; mode=display">
\nabla f\left(\mathbf{x}_{0}\right)+\nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)=\mathbf{0}</script><p>解这个线性方程组可以得到</p>
<script type="math/tex; mode=display">
\tag{1}\mathbf{x}=\mathbf{x}_{0}-\left(\nabla^{2} f\left(\mathbf{x}_{0}\right)\right)^{-1} \nabla f\left(\mathbf{x}_{0}\right)</script><p>如果将梯度向量简写为$\mathbf{g}$ ，Hessian 矩阵简记为$\mathbf{H}$ ，式(1)可以简写为</p>
<script type="math/tex; mode=display">
\tag{2}\mathbf{x}=\mathbf{x}_{0}-\mathbf{H}^{-1} \mathbf{g}</script><p>在泰勒公式中忽略了高阶项将函数做了近似，因此这个解不一定是目标函数的驻点，需要反复用式(2) 进行迭代。从初始点$\mathbf{x}_{0}$处开始，计算函数在当前点处的Hessian 矩阵和梯度向量，然后用下面的公式进行迭代</p>
<script type="math/tex; mode=display">
\tag{3} \mathbf{x}_{k+1}=\mathbf{x}_{k}-\alpha \mathbf{H}_{k}^{-1} \mathbf{g}_{k}</script><p>直至收敛到驻点处。迭代终止的条件是梯度的模接近于0 ，或达到指定的迭代次数。其中$\alpha$是人工设置的学习率。需要学习率的原因与梯度下降法相同，是为了保证能够忽略泰勒公式中的高阶无穷小项。</p>
<h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><p>深度学习500问： <a href="https://github.com/scutan90/DeepLearning-500-questions">https://github.com/scutan90/DeepLearning-500-questions</a> </p>
<p>机器学习与深度学习习题集答案-1：<a href="https://mp.weixin.qq.com/s/4kWUE8ml_o6iF0F1TREyiA">https://mp.weixin.qq.com/s/4kWUE8ml_o6iF0F1TREyiA</a></p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>ML</tag>
        <tag>GradientDescent</tag>
      </tags>
  </entry>
  <entry>
    <title>线性回归&amp;逻辑回归</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92+%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="1-简单介绍一下线性回归。"><a href="#1-简单介绍一下线性回归。" class="headerlink" title="1. 简单介绍一下线性回归。"></a>1. 简单介绍一下线性回归。</h2><ul>
<li>线性：两个变量之间的关系<strong>是</strong>一次函数关系的——图象<strong>是直线</strong>，叫做线性。</li>
<li>非线性：两个变量之间的关系<strong>不是</strong>一次函数关系的——图象<strong>不是直线</strong>，叫做非线性。</li>
<li>回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算<strong>回归到真实值</strong>，这就是回归的由来。</li>
<li>线性回归就是利用的样本$D=(\mathrm{x}_i, \mathrm{y}_i)<br>$，$ \mathrm{i}=1,2,3 \ldots \mathrm{N}, \mathrm{x}_i$是特征数据，可能是一个，也可能是多个，通过有监督的学习，学习到由$x$到$y$的映射$h$，利用该映射关系对未知的数据进行预估，因为$y$为连续值，所以是回归问题。</li>
</ul>
<a id="more"></a>
<h2 id="2-线性回归的假设函数是什么形式？"><a href="#2-线性回归的假设函数是什么形式？" class="headerlink" title="2. 线性回归的假设函数是什么形式？"></a>2. 线性回归的假设函数是什么形式？</h2><p>线性回归的假设函数（$\theta<em>{0}$表示截距项，$ x</em>{0} = 1$，方便矩阵表达）：</p>
<script type="math/tex; mode=display">
f(x)=\theta_{0} x_{0}+\theta_{1} x_{1}+\theta_{2} x_{2} \ldots+\theta_{n} x_{n}  = \theta ^TX</script><p>其中$\theta,x$都是列向量</p>
<h2 id="3-线性回归的代价-损失-函数是什么形式？"><a href="#3-线性回归的代价-损失-函数是什么形式？" class="headerlink" title="3. 线性回归的代价(损失)函数是什么形式？"></a>3. 线性回归的代价(损失)函数是什么形式？</h2><script type="math/tex; mode=display">
MSE: \qquad J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(y_{i}-h_{\theta}\left(x_{i}\right)\right)^{2}</script><h2 id="4-简述岭回归与Lasso回归以及使用场景。"><a href="#4-简述岭回归与Lasso回归以及使用场景。" class="headerlink" title="4. 简述岭回归与Lasso回归以及使用场景。"></a>4. 简述岭回归与Lasso回归以及使用场景。</h2><ul>
<li><p>目的：</p>
<ul>
<li><p>解决线性回归出现的过拟合的请况。</p>
</li>
<li><p>解决在通过正规方程方法求解$\theta$的过程中出现的$X^TX$不可逆的请况。</p>
</li>
</ul>
</li>
<li><p>本质：</p>
<ul>
<li>约束(限制)要优化的参数 </li>
</ul>
</li>
</ul>
<p>这两种回归均通过在损失函数中引入<strong>正则化项</strong>来达到目的：</p>
<p><strong>线性回归的损失函数：</strong></p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><ul>
<li><strong>岭回归</strong><ul>
<li>损失函数：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}</script><ul>
<li><strong>Lasso回归</strong><ul>
<li>损失函数</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} |\theta_{j}|</script><p>本来Lasso回归与岭回归的解空间是全部区域，但通过正则化添加了一些约束，使得解空间变小了，甚至在个别正则化方式下，解变得稀疏了。<br><img src="https://img-blog.csdnimg.cn/20200101212656597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlaXRhbzUyMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" /><br>如图所示，这里的$w_1$，$w_2$都是模型的参数，要优化的目标参数，那个红色边框包含的区域，其实就是解空间，正如上面所说，这个时候，解空间“缩小了”，你只能在这个缩小了的空间中，寻找使得目标函数最小的$w_1$，$w_2$。左边图的解空间是圆的，是由于采用了$L2$范数正则化项的缘故，右边的是个四边形，是由于采用了$L1$范数作为正则化项的缘故，大家可以在纸上画画，$L2$构成的区域一定是个圆，$L1$构成的区域一定是个四边形。</p>
<p>再看看那蓝色的圆圈，再次提醒大家，这个<strong>坐标轴和特征（数据）没关系</strong>，它完全是参数的坐标系，每一个圆圈上，可以取无数个$w_1$，$w_2$，这些$w_1$，$w_2$有个共同的特点，用它们计算的目标函数值是相等的！那个蓝色的圆心，就是实际最优参数，但是由于我们对解空间做了限制，所以最优解只能在“缩小的”解空间中产生。</p>
<p>蓝色的圈圈一圈又一圈，代表着参数$w_1$，$w_2$在不停的变化，并且是在解空间中进行变化（这点注意，图上面没有画出来，估计画出来就不好看了），直到脱离了解空间，也就得到了图上面的那个$w^*$，这便是目标函数的最优参数。</p>
<p>对比一下左右两幅图的$w^*$，我们明显可以发现，右图的$w^*$的$w_1$分量是0，有没有感受到一丝丝凉意？稀疏解诞生了！是的，这就是我们想要的稀疏解，我们想要的简单模型。$L1$比$L2$正则化更容易产生稀疏矩阵。</p>
<ul>
<li><p>补充</p>
<ul>
<li><p><strong>ElasticNet 回归</strong>： 线性回归 + L1正则化 + L2 正则化。</p>
<ul>
<li><p>ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。</p>
</li>
<li><p>损失函数</p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2} \sum_{i}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}+\lambda\left(\rho \sum_{j}^{n}\left|\theta_{j}\right|+(1-\rho) \sum_{j}^{n} \theta_{j}^{2}\right)</script></li>
</ul>
</li>
<li><p><strong>LWR(局部加权)回归</strong>：</p>
<ul>
<li><p>局部加权线性回归是在线性回归的基础上对每一个测试样本（训练的时候就是每一个训练样本）在其已有的样本进行一个加权拟合，<strong>权重的确定</strong>可以通过一个核来计算，常用的有<strong>高斯核</strong>（离测试样本越近，权重越大，反之越小），这样对每一个测试样本就得到了不一样的权重向量，所以最后得出的拟合曲线不再是线性的了，这样就增加的模型的复杂度来更好的拟合非线性数据。</p>
</li>
<li><p>损失函数</p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2} \sum_{i=1}^{m} w^{(i)}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="5-线性回归要求因变量服从正态分布吗？"><a href="#5-线性回归要求因变量服从正态分布吗？" class="headerlink" title="5. 线性回归要求因变量服从正态分布吗？"></a>5. 线性回归要求因变量服从正态分布吗？</h2><p><strong>线性回归的假设前提是噪声服从正态分布，即因变量服从正态分布。但实际上难以达到，因变量服从正态分布时模型拟合效果更好。</strong></p>
<p>参考资料： <a href="http://www.julyedu.com/question/big/kp_id/23/ques_id/2914">http://www.julyedu.com/question/big/kp_id/23/ques_id/2914</a> </p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="1-简单介绍一下逻辑回归"><a href="#1-简单介绍一下逻辑回归" class="headerlink" title="1. 简单介绍一下逻辑回归"></a>1. 简单介绍一下逻辑回归</h2><p>逻辑回归用来解决<strong>分类</strong>问题，线性回归的结果$Y$带入一个非线性变换的<strong>Sigmoid函数</strong>中，得到$[0,1]$之间取值范围的数$S$，$S$可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么$S$大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。</p>
<ul>
<li>逻辑回归的本质： 极大似然估计</li>
<li>逻辑回归的激活函数：Sigmoid</li>
<li>逻辑回归的代价函数：交叉熵</li>
</ul>
<h2 id="2-简单介绍一下Sigmoid函数"><a href="#2-简单介绍一下Sigmoid函数" class="headerlink" title="2. 简单介绍一下Sigmoid函数"></a>2. 简单介绍一下Sigmoid函数</h2><p>函数公式如下：</p>
<script type="math/tex; mode=display">
S(t)=\frac{1}{1+e^{-t}}</script><p>函数中$t$无论取什么值，其结果都在$[0,{1}]$的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，那又有人问了，你这不是$[0,1]$的区间吗，怎么会只有0和1呢？这个问题问得好，我们假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。</p>
<p>好了，接下来我们把$\theta^T X+b$带入$t$中就得到了我们的逻辑回归的一般模型方程：</p>
<p>逻辑回归的<strong>假设函数</strong>：</p>
<script type="math/tex; mode=display">
H(\theta, b)=\frac{1}{1+e^{(\theta^T X+b)}}</script><p>结果$P$也可以理解为概率，换句话说概率大于0.5的属于1分类，概率小于0.5的属于0分类，这就达到了分类的目的。</p>
<h2 id="3-逻辑回归的损失函数是什么"><a href="#3-逻辑回归的损失函数是什么" class="headerlink" title="3. 逻辑回归的损失函数是什么"></a>3. 逻辑回归的损失函数是什么</h2><p>逻辑回归的损失函数是<strong>对数似然函数</strong>，函数公式如下：</p>
<script type="math/tex; mode=display">
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}-\log \left(h_{\theta}(x)\right) & \qquad  y=1 \\-\log \left(1-h_{\theta}(x)\right) & \qquad  y=0 \end{aligned}\right.</script><p><strong>两式合并</strong>得到<strong>概率分布表达式</strong>：</p>
<script type="math/tex; mode=display">
(P(y|x,\theta ) = h_{\theta}(x)^y(1-h_{\theta}(x))^{1-y})</script><p> <strong>对数似然函数最大化</strong>得到<strong>似然函数的代数表达式</strong>为 ：</p>
<script type="math/tex; mode=display">
L(\theta) = \prod\limits_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}</script><p> <strong>对似然函数对数化取反</strong>得到<strong>损失函数表达式</strong> ：</p>
<script type="math/tex; mode=display">
J(\theta) = -lnL(\theta) = -\sum\limits_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+ (1-y^{(i)})log(1-h_{\theta}(x^{(i)})))</script><ul>
<li>为何不能用mse</li>
</ul>
<p><a href="http://sofasofa.io/forum_main_post.php?postid=1001792">解释</a></p>
<h2 id="4-可以进行多分类吗？"><a href="#4-可以进行多分类吗？" class="headerlink" title="4.可以进行多分类吗？"></a>4.可以进行多分类吗？</h2><p>多分类问题一般将二分类推广到多分类的方式有三种，一对一，一对多，多对多。</p>
<ul>
<li><p>一对一：</p>
<ul>
<li>将$N$个类别两两配对，产生$N(N-1)/2$个二分类任务，测试阶段新样本同时交给所有的分类器，最终结果通过投票产生。</li>
</ul>
</li>
<li><p>一对多：</p>
<ul>
<li>每一次将一个例作为正例，其他的作为反例，训练$N$个分类器，测试时如果只有一个分类器预测为正类，则对应类别为最终结果，如果有多个，则一般选择置信度最大的。从分类器角度一对一更多，但是每一次都只用了2个类别，因此当类别数很多的时候一对一开销通常更小(只要训练复杂度高于$O(N)$即可得到此结果)。</li>
</ul>
</li>
<li><p>多对多：</p>
<ul>
<li>若干各类作为正类，若干个类作为反类。注意正反类必须特殊的设计。</li>
</ul>
</li>
</ul>
<h2 id="5-逻辑回归的优缺点"><a href="#5-逻辑回归的优缺点" class="headerlink" title="5.逻辑回归的优缺点"></a>5.逻辑回归的优缺点</h2><ul>
<li><p>优点</p>
<ul>
<li>LR能以概率的形式输出结果，而非只是0,1判定。</li>
<li>LR的可解释性强、可控度高、训练速度快</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li><p>对模型中自变量多重共线性较为敏感</p>
<p>例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性；</p>
</li>
<li><p>预测结果呈$S$型，因此从$log(odds)$向概率转化的过程是非线性的，在两端随着$log(odds)$值的变化，概率变化很小，边际值太小，slope太小，而中间概率的变化很大，很敏感。 导致很多区间的变量变化对目标概率的影响没有区分度，无法确定阀值。</p>
</li>
</ul>
</li>
</ul>
<h2 id="6-逻辑斯特回归为什么要对特征进行离散化。"><a href="#6-逻辑斯特回归为什么要对特征进行离散化。" class="headerlink" title="6. 逻辑斯特回归为什么要对特征进行离散化。"></a>6. 逻辑斯特回归为什么要对特征进行离散化。</h2><ul>
<li><p>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；离散特征的增加和减少都很容易，易于模型的快速迭代； </p>
</li>
<li><p>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； </p>
</li>
<li><p>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</p>
</li>
<li><p>方便交叉与特征组合：离散化后可以进行特征交叉，由$M+N$个变量变为$M*N$个变量，进一步引入非线性，提升表达能力； </p>
</li>
</ul>
<h2 id="7-线性回归与逻辑回归的区别"><a href="#7-线性回归与逻辑回归的区别" class="headerlink" title="7.  线性回归与逻辑回归的区别"></a>7.  线性回归与逻辑回归的区别</h2><ul>
<li><p>线性回归的样本的输出，都是连续值，$ y\in (-\infty ,+\infty )$，而逻辑回归中$y\in (0,1)$，只能取0和1。</p>
</li>
<li><p>对于拟合函数也有本质上的差别： </p>
<ul>
<li>线性回归：$f(x)=\theta ^{T}x=\theta <em>{1}x </em>{1}+\theta <em>{2}x </em>{2}+…+\theta <em>{n}x </em>{n}$</li>
<li>逻辑回归：$f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)$，其中，$g(z)=\frac{1}{1+e^{-z}}$</li>
</ul>
</li>
</ul>
<p>  线性回归的拟合函数，是对$f(x)$的输出变量$y$的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合。</p>
<ul>
<li><p>为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？ </p>
<p>$\theta ^{T}x=0$就相当于是1类和0类的决策边界： </p>
<p>当$\theta ^{T}x&gt;0$，则$y&gt;0.5$；若$\theta ^{T}x\rightarrow +\infty $，则$y \rightarrow  1 $，即$y$为1类; </p>
<p>当$\theta ^{T}x&lt;0$，则$y&lt;0.5$；若$\theta ^{T}x\rightarrow -\infty $，则$y \rightarrow  0 $，即$y$为0类; </p>
</li>
</ul>
<ul>
<li><p>这个时候就能看出区别，在线性回归中$\theta ^{T}x$为预测值的拟合函数；而在逻辑回归中$\theta ^{T}x$为决策边界。下表为线性回归和逻辑回归的区别。</p>
<p><center>线性回归和逻辑回归的区别</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">线性回归</th>
<th style="text-align:center">逻辑回归</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">目的</td>
<td style="text-align:center">预测</td>
<td style="text-align:center">分类</td>
</tr>
<tr>
<td style="text-align:center">$y^{(i)}$</td>
<td style="text-align:center">未知</td>
<td style="text-align:center">（0,1）</td>
</tr>
<tr>
<td style="text-align:center">函数</td>
<td style="text-align:center">拟合函数</td>
<td style="text-align:center">预测函数</td>
</tr>
<tr>
<td style="text-align:center">参数计算方式</td>
<td style="text-align:center">最小二乘法</td>
<td style="text-align:center">极大似然估计</td>
</tr>
</tbody>
</table>
</div>
<p> 下面具体解释一下： </p>
<ol>
<li>拟合函数和预测函数什么关系呢？简单来说就是将拟合函数做了一个逻辑函数的转换，转换后使得$y^{(i)} \in (0,1)$;</li>
<li>最小二乘和最大似然估计可以相互替代吗？回答当然是不行了。我们来看看两者依仗的原理：最大似然估计是计算使得数据出现的可能性最大的参数，依仗的自然是Probability。而最小二乘是计算误差损失。</li>
</ol>
<h2 id="8-逻辑回归有哪些应用"><a href="#8-逻辑回归有哪些应用" class="headerlink" title="8. 逻辑回归有哪些应用"></a>8. 逻辑回归有哪些应用</h2><ul>
<li>CTR预估/推荐系统的learning to rank/各种分类场景。</li>
<li>某搜索引擎厂的广告CTR预估基线版是LR。</li>
<li>某电商搜索排序/广告CTR预估基线版是LR。</li>
<li>某电商的购物搭配推荐用了大量LR。</li>
<li>某现在一天广告赚1000w+的新闻app排序基线是LR。</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-贝叶斯</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<p>[TOC]</p>
<h2 id="1-简述朴素贝叶斯算法原理和工作流程"><a href="#1-简述朴素贝叶斯算法原理和工作流程" class="headerlink" title="1.简述朴素贝叶斯算法原理和工作流程"></a>1.简述朴素贝叶斯算法原理和工作流程</h2><p><strong>工作原理</strong>：</p>
<ul>
<li>假设现在有样本$x=(x_1, x_2, x_3, \dots x_n)$待分类项</li>
<li>假设样本有$m$个特征$(a_1,a_2,a_3,\dots a_m)$(特征独立)</li>
<li>再假设现在有分类目标$Y={ y_1，y_2，y_3，\dots ,y_n}$</li>
<li>那么就$\max ({ P }({ y }_1 | { x }), { P }({ y }_2 | {x}), {P}({y}_3 | {x}) ,{P}({y_n} | {x}))$是最终的分类类别。</li>
<li>而$ P(y_i | x)=\frac{P(x | y_i) * P(y_i)}{ P(x)} $，因为$x$对于每个分类目标来说都一样，所以就是求$\max({P}({x}|{y_i})*{P}({y_i}))$</li>
<li>$P(x | y_i) * P(y_i)=P(y_i) * \prod(P(a_j| y_i))$，而具体的$P(a_j|y_i)$和$P(y_i)$都是能从训练样本中统计出来</li>
<li>${P}({a_j} | {y_i})$表示该类别下该特征$a_j$出现的概率$P(y_i)$表示全部类别中这个这个类别出现的概率,这样就能找到应该属于的类别了</li>
</ul>
<a id="more"></a>
<h2 id="2-条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念"><a href="#2-条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念" class="headerlink" title="2. 条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念"></a>2. 条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念</h2><ul>
<li><p>条件概率：</p>
<ul>
<li>$P(X|Y)$含义： 表示$Y$发生的条件下$X$发生的概率。</li>
</ul>
</li>
<li><p>先验概率</p>
<ul>
<li><strong>表示事件发生前的预判概率。</strong>这个可以是基于历史数据统计，也可以由背景常识得出，也可以是主观观点得出。一般都是单独事件发生的概率，如 $P(X)$</li>
</ul>
</li>
<li><p>后验概率</p>
<ul>
<li>基于先验概率求得的<strong>反向条件概率</strong>，形式上与条件概率相同(若$P(X|Y)$ 为正向，则$P(Y|X)$ 为反向)</li>
</ul>
</li>
<li><p>联合概率：</p>
</li>
<li><p>事件$X$与事件$Y$同时发生的概率。</p>
</li>
<li><p>贝叶斯公式</p>
<ul>
<li><script type="math/tex; mode=display">
P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}  \\</script></li>
<li><p>$P(Y)$ 叫做<strong>先验概率</strong>：事件$X$发生之前，我们根据以往经验和分析对事件$Y$发生的一个概率的判断</p>
</li>
<li><p>$P(Y|X)$ 叫做<strong>后验概率</strong>：事件$X$发生之后，我们对事件$Y$发生的一个概率的重新评估</p>
</li>
<li><p>$P(Y,X)$叫做<strong>联合概率</strong>：事件$X$与事件$Y$同时发生的概率。</p>
</li>
<li><p>先验概率和后验概率是相对的。如果以后还有新的信息引入，更新了现在所谓的后验概率，得到了新的概率值，那么这个新的概率值被称为后验概率。</p>
</li>
</ul>
</li>
</ul>
<h2 id="3-为什么朴素贝叶斯如此“朴素”？"><a href="#3-为什么朴素贝叶斯如此“朴素”？" class="headerlink" title="3.为什么朴素贝叶斯如此“朴素”？"></a>3.为什么朴素贝叶斯如此“朴素”？</h2><p>因为它<strong>假定所有的特征在数据集中的作用是同样重要和独立的</strong>。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。用贝叶斯公式表达如下：</p>
<script type="math/tex; mode=display">
P(Y|X_1, X_2) = \frac{P(X_1|Y) P(X_2|Y) P(Y)}{P(X_1)P(X_2)}</script><p><strong>而在很多情况下，所有变量几乎不可能满足两两之间的条件。</strong></p>
<p>朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是<strong>“很简单很天真”</strong>地假设样本特征彼此独立.这个假设现实中基本上不存在，但特征相关性很小的实际情况还是很多的，所以这个模型仍然能够工作得很好。</p>
<h2 id="4-什么是贝叶斯决策理论？"><a href="#4-什么是贝叶斯决策理论？" class="headerlink" title="4.什么是贝叶斯决策理论？"></a>4.什么是贝叶斯决策理论？</h2><p>贝叶斯决策理论是主观贝叶斯派归纳理论的重要组成部分。贝叶斯决策就是在不完全情报下，对部分未知的状态用主观概率估计，然后用贝叶斯公式对发生概率进行修正，最后再利用期望值和修正概率做出最优决策(选择概率最大的类别)。<br>贝叶斯决策理论方法是统计模型决策中的一个基本方法，其<strong>基本思想</strong>是：</p>
<ul>
<li>已知类条件概率密度参数表达式和先验概率</li>
<li>利用贝叶斯公式转换成后验概率</li>
<li>根据后验概率大小进行决策分类</li>
</ul>
<h2 id="5-朴素贝叶斯算法的前提假设是什么？"><a href="#5-朴素贝叶斯算法的前提假设是什么？" class="headerlink" title="5.朴素贝叶斯算法的前提假设是什么？"></a>5.朴素贝叶斯算法的前提假设是什么？</h2><ul>
<li>特征之间相互独立</li>
<li>每个特征同等重要</li>
</ul>
<h2 id="6-为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果"><a href="#6-为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果" class="headerlink" title="6.为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?"></a>6.为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?</h2><ul>
<li>对于分类任务来说，只要各类别的条件概率排序正确、无需精准概率值即可导致正确分类；</li>
<li>如果属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。</li>
</ul>
<h2 id="7-什么是朴素贝叶斯中的零概率问题？如何解决？"><a href="#7-什么是朴素贝叶斯中的零概率问题？如何解决？" class="headerlink" title="7.什么是朴素贝叶斯中的零概率问题？如何解决？"></a><strong>7.什么是朴素贝叶斯中的零概率问题？如何解决？</strong></h2><p><strong>零概率问题</strong>：在计算实例的概率时，如果某个量$x$，在观察样本库(训练集)中没有出现过，会导致整个实例的概率结果是0。</p>
<p><strong>解决办法</strong>：若$P(x)$为零则无法计算。为了解决零概率的问题，法国数学家拉普拉斯最早提出用加1的方法估计没有出现过的现象的概率，所以加法平滑也叫做<strong>拉普拉斯平滑</strong>。</p>
<p><strong>举个栗子</strong>：假设在文本分类中，有3个类，$C1、C2、C3$，在指定的训练样本中，某个词语$K1$，在各个类中观测计数分别为0，990，10，$K1$的概率为0，0.99，0.01，对这三个量使用拉普拉斯平滑的计算方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1&#x2F;1003&#x3D;0.001，</span><br><span class="line">991&#x2F;1003&#x3D;0.988，</span><br><span class="line">11&#x2F;1003&#x3D;0.011</span><br><span class="line">在实际的使用中也经常使用加 lambda(1≥lambda≥0)来代替简单加1。如果对N个计数都加上lambda，这时分母也要记得加上N*lambda。</span><br></pre></td></tr></table></figure>
<p>将朴素贝叶斯中的所有概率计算<strong>应用拉普拉斯平滑即可以解决零概率问题</strong>。</p>
<h2 id="8-朴素贝叶斯中概率计算的下溢问题如何解决？"><a href="#8-朴素贝叶斯中概率计算的下溢问题如何解决？" class="headerlink" title="8.朴素贝叶斯中概率计算的下溢问题如何解决？"></a>8.朴素贝叶斯中概率计算的下溢问题如何解决？</h2><p><strong>下溢问题</strong>：在朴素贝叶斯的计算过程中，需要对特定分类中各个特征出现的<strong>概率进行连乘，小数相乘，越乘越小，这样就造成了下溢出</strong>。<br>为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误。</p>
<script type="math/tex; mode=display">
\prod_{i=x}^{n} p\left(x_{i} | y_{j}\right)</script><p><strong>解决办法</strong>：对其<strong>取对数</strong>：</p>
<script type="math/tex; mode=display">
\log \prod_{i=1}^{n} p\left(x_{i} | y_{j}\right)</script><script type="math/tex; mode=display">
=\sum_{i=1}^{n} \log p\left(x_{i} | y_{j}\right)</script><p>将小数的乘法操作转化为取对数后的加法操作，规避了变为零的风险同时并不影响分类结果。</p>
<h2 id="9-当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？"><a href="#9-当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？" class="headerlink" title="9.当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？"></a>9.当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？</h2><p>当朴素贝叶斯算法数据的属性为连续型变量时，有两种方法可以计算属性的类条件概率。</p>
<ul>
<li>第一种方法：把一个连续的属性离散化，然后用相应的离散区间替换连续属性值。但这种方法不好控制离散区间划分的粒度。如果粒度太细，就会因为每个区间内训练记录太少而不能对$P(X|Y)$<br>做出可靠的估计，如果粒度太粗，那么有些区间就会有来自不同类的记录，因此失去了正确的决策边界。</li>
<li>第二种方法：假设连续变量服从某种概率分布，然后使用训练数据估计分布的参数，例如可以使用高斯分布来表示连续属性的类条件概率分布。<ul>
<li>高斯分布有两个参数，均值$\mu$和方差$\sigma 2$，对于每个类$y_i$，属性$X_i$的类条件概率等于：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
P\left(X_{i}=x_{i} | Y=y_{j}\right)=\frac{1}{\sqrt{2 \Pi} \sigma_{i j}^{2}} e^{\frac{\left(x_{i}-\mu_{j}\right)^{2}}{2 \sigma_{i}^{2}}}</script><p>$\mu_{i j}$：类$y_j$的所有训练记录关于$X_i$的样本均值估计</p>
<p>$\sigma_{i j}^{2}$：类$y_j$的所有训练记录关于$X$的样本方差</p>
<p>通过高斯分布估计出类条件概率。</p>
<h2 id="10-朴素贝叶斯有哪几种常用的分类模型？"><a href="#10-朴素贝叶斯有哪几种常用的分类模型？" class="headerlink" title="10.朴素贝叶斯有哪几种常用的分类模型？"></a><strong>10.朴素贝叶斯有哪几种常用的分类模型？</strong></h2><p>三个常用模型：高斯、多项式、伯努利。三个模型都是为了解决连续性变量引起的问题的。</p>
<ul>
<li><p>多项式模型：</p>
<ul>
<li>思想：将一个连续值视为特征的一个类别，这样会引起很多概率为0的问题。所以用拉普拉斯平滑解决。</li>
<li>其中$\alpha$为拉普拉斯平滑，加和的是属性出现的总次数，比如文本分类问题里面，不光看词语是否在文本中出现，也得看出现的次数。如果总词数为$n$，出现词数为$m$的话，说起来有点像掷骰子$n$次出现$m$次这个词的场景。<script type="math/tex; mode=display">
P\left(x_{i} | y_{k}\right)=\frac{N_{y k_{1}}+\alpha}{N_{y_{k}}+\alpha n}</script></li>
</ul>
</li>
<li><p>高斯模型：</p>
<ul>
<li>思想：是要得到一个概率值，但是直接把一个值当做一个类别会引起零概率问题，于是假设并模拟一个分布，求其参数，进而求其概率。</li>
<li>处理包含连续型变量的数据，使用高斯分布概率密度来计算类的条件概率密度</li>
</ul>
</li>
<li><p>伯努利模型：</p>
<ul>
<li>伯努利模型特征的取值为布尔型，即出现为true没有出现为false，在文本分类中，就是一个单词有没有在一个文档中出现。</li>
<li>伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次。<script type="math/tex; mode=display">
P( '代开'， '发票'， '发票'， '我' | S) = P('代开' | S)   P( '发票' | S) P('我' | S)</script>我们看到，”发票“出现了两次，但是我们只将其算作一次。我们看到，”发票“出现了两次，但是我们只将其算作一次。</li>
</ul>
</li>
</ul>
<h2 id="11-为什么说朴素贝叶斯是高偏差低方差？"><a href="#11-为什么说朴素贝叶斯是高偏差低方差？" class="headerlink" title="11.为什么说朴素贝叶斯是高偏差低方差？"></a>11.为什么说朴素贝叶斯是高偏差低方差？</h2><p>在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error=Bias +Variance$。</p>
<ul>
<li>$Error$反映的是整个模型的准确度，</li>
<li>$Bias$反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，</li>
<li>$Variance$反映的是模型每一次输出结果与模型输出期望(平均值)之间的误差，即模型的稳定性，数据是否集中。</li>
<li>对于复杂模型，充分拟合了部分数据，使得他们的偏差较小，而由于对部分数据的过度拟合，对于部分数据预测效果不好，整体来看可能引起方差较大。</li>
<li>对于朴素贝叶斯了。它简单的假设了各个数据之间是无关的，是一个被严重简化了的模型，简单模型与复杂模型相反，大部分场合偏差部分大于方差部分，也就是说高偏差而低方差。</li>
</ul>
<h2 id="12-朴素贝叶斯为什么适合增量计算？"><a href="#12-朴素贝叶斯为什么适合增量计算？" class="headerlink" title="12.朴素贝叶斯为什么适合增量计算？"></a>12.朴素贝叶斯为什么适合增量计算？</h2><p>因为朴素贝叶斯在训练过程中实际只需要计算出各个类别的概率和各个特征的类条件概率，这些概率值可以快速的根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算，该特性可以使用在超出内存的大量数据计算和按小时级等获取的数据计算中。</p>
<h2 id="13-高度相关的特征对朴素贝叶斯有什么影响？"><a href="#13-高度相关的特征对朴素贝叶斯有什么影响？" class="headerlink" title="13.高度相关的特征对朴素贝叶斯有什么影响？"></a>13.高度相关的特征对朴素贝叶斯有什么影响？</h2><p>假设有两个特征高度相关，相当于该特征在模型中发挥了两次作用(计算两次条件概率)，使得朴素贝叶斯获得的结果向该特征所希望的方向进行了偏移，影响了最终结果的准确性，所以朴素贝叶斯算法应先处理特征，把相关特征去掉。</p>
<h2 id="14-朴素贝叶斯的应用场景有哪些？"><a href="#14-朴素贝叶斯的应用场景有哪些？" class="headerlink" title="14.朴素贝叶斯的应用场景有哪些？"></a>14.朴素贝叶斯的应用场景有哪些？</h2><ul>
<li><strong>文本分类/垃圾文本过滤/情感判别</strong>：<br>这大概是朴素贝叶斯应用最多的地方了，即使在现在这种分类器层出不穷的年代，在文本分类场景中，朴素贝叶斯依旧坚挺地占据着一席之地。因为多分类很简单，同时在文本数据中，分布独立这个假设基本是成立的。而垃圾文本过滤(比如垃圾邮件识别)和情感分析(微博上的褒贬情绪)用朴素贝叶斯也通常能取得很好的效果。</li>
<li><strong>多分类实时预测</strong>：<br>对于文本相关的多分类实时预测，它因为上面提到的优点，被广泛应用，简单又高效。</li>
<li><strong>推荐系统</strong>：<br>朴素贝叶斯和协同过滤是一对好搭档，协同过滤是强相关性，但是泛化能力略弱，朴素贝叶斯和协同过滤一起，能增强推荐的覆盖度和效果。</li>
</ul>
<h2 id="15-朴素贝叶斯有什么优缺点？"><a href="#15-朴素贝叶斯有什么优缺点？" class="headerlink" title="15.朴素贝叶斯有什么优缺点？"></a>15.朴素贝叶斯有什么优缺点？</h2><ul>
<li>优点：<ul>
<li>对数据的训练快，分类也快</li>
<li>对缺失数据不太敏感，算法也比较简单</li>
<li>对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练</li>
</ul>
</li>
<li>缺点：<ul>
<li>对输入数据的表达形式很敏感</li>
<li>由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。</li>
<li>需要计算先验概率，分类决策存在错误率。</li>
</ul>
</li>
</ul>
<h2 id="16-朴素贝叶斯与-LR-区别？"><a href="#16-朴素贝叶斯与-LR-区别？" class="headerlink" title="16.朴素贝叶斯与 LR 区别？"></a>16.朴素贝叶斯与 LR 区别？</h2><ul>
<li><strong>朴素贝叶斯是生成模型</strong>，根据已有样本进行贝叶斯估计学习出先验概率 $P(Y)$ 和条件概率 $P(X|Y)$，进而求出联合分布概率 $P(X,Y)$，最后利用贝叶斯定理求解$P(Y|X)$， 而<strong>LR是判别模型</strong>，根据极大化对数似然函数直接求出条件概率 $P(Y|X)$</li>
<li>朴素贝叶斯是基于很强的<strong>条件独立假设</strong>(在已知分类Y的条件下，各个特征变量取值是相互独立的)，而 LR 则对此没有要求</li>
<li>朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。</li>
</ul>
<h2 id="17-贝叶斯优化算法-参数调优"><a href="#17-贝叶斯优化算法-参数调优" class="headerlink" title="17. 贝叶斯优化算法(参数调优)"></a>17. 贝叶斯优化算法(参数调优)</h2><ul>
<li><p>网格搜索和随机搜索：在测试一个新点时，会忽略前一个点的信息；</p>
</li>
<li><p>贝叶斯优化算法：充分利用了之前的信息。贝叶斯优化算法通过对目标函数形式进行学习，找到使目标函数向全局最优值提升的参数。</p>
</li>
<li><p>学习目标函数形式的方法：</p>
<ul>
<li>首先根据先验分布，假设一个搜集函数；</li>
<li>每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布</li>
<li>算法测试由后验分布给出的全局最值最可能出现的位置的点。</li>
</ul>
</li>
</ul>
<p>对于贝叶斯优化算法，有一个需要注意的地方，一旦找到了一个局部最优值，它会在该区域不断采样，所以很容易陷入局部最优值。为了弥补这个缺陷，贝叶斯优化算法会在探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利用”则是根据后验分布在最可能出现全局最值的区域进行采样。</p>
<h2 id="18-朴素贝叶斯分类器对异常值敏感吗"><a href="#18-朴素贝叶斯分类器对异常值敏感吗" class="headerlink" title="18.朴素贝叶斯分类器对异常值敏感吗?"></a>18.朴素贝叶斯分类器对异常值敏感吗?</h2><p>朴素贝叶斯是一种<strong>对异常值不敏感</strong>的分类器，保留数据中的异常值，常常可以保持贝叶斯算法的整体精度，如果对原始数据进行降噪训练，分类器可能会因为失去部分异常值的信息而导致泛化能力下降。</p>
<h2 id="19-朴素贝叶斯算法对缺失值敏感吗？"><a href="#19-朴素贝叶斯算法对缺失值敏感吗？" class="headerlink" title="19.朴素贝叶斯算法对缺失值敏感吗？"></a>19.朴素贝叶斯算法对缺失值敏感吗？</h2><p>朴素贝叶斯是一种<strong>对缺失值不敏感</strong>的分类器，朴素贝叶斯算法能够处理缺失的数据，在算法的建模时和预测时数据的属性都是单独处理的。因此<strong>如果一个数据实例缺失了一个属性的数值，在建模时将被忽略</strong>，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。</p>
<h2 id="20-一句话总结贝叶斯算法"><a href="#20-一句话总结贝叶斯算法" class="headerlink" title="20. 一句话总结贝叶斯算法"></a>20. 一句话总结贝叶斯算法</h2><p><strong>贝叶斯分类器直接用贝叶斯公式解决分类问题</strong>。假设样本的特征向量为$x$，类别标签为$y$，根据贝叶斯公式，样本属于每个类的条件概率（后验概率）为： </p>
<script type="math/tex; mode=display">
p(y | \mathbf{x})=\frac{p(\mathbf{x} | y) p(y)}{p(\mathbf{x})}</script><p> 分母$p(x)$对所有类都是相同的，<strong>分类的规则是将样本归到后验概率最大的那个类</strong>，不需要计算准确的概率值，只需要知道属于哪个类的概率最大即可，这样可以忽略掉分母。分类器的判别函数为： </p>
<script type="math/tex; mode=display">
\arg \max _{y} p(\mathrm{x} | y) p(y)</script><p>在实现贝叶斯分类器时，<strong>需要知道每个类的条件概率分布$p(x|y)$即先验概率</strong>。一般假设样本服从正态分布。训练时确定先验概率分布的参数，一般用最大似然估计，即最大化对数似然函数。</p>
<p><strong>贝叶斯分类器是一种生成模型，可以处理多分类问题，是一种非线性模型。</strong></p>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>NaiveBayes</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-Random Forest</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
    <content><![CDATA[<p>[TOC]</p>
<h2 id="1-简单介绍随机森林"><a href="#1-简单介绍随机森林" class="headerlink" title="1. 简单介绍随机森林"></a>1. 简单介绍随机森林</h2><ul>
<li><strong>多次随机取样，多次随机取属性，选取最优分割点，构建多个(CART)分类器，投票表决或简单平均</strong></li>
<li><strong>算法流程：</strong><ul>
<li>输入为样本集$D={(x，y_1)，(x_2，y_2) \dots (x_m，y_m)}$，弱分类器迭代次数$T$。</li>
<li>输出为最终的强分类器$f(x)$</li>
<li>对于$t=1，2 \dots T$<ul>
<li>对训练集进行第$t$次随机采样，共采集$m$次，得到包含$m$个样本的采样集Dt</li>
<li>用采样集$D_t$训练第$t$个决策树模型$G_t(x)$，在训练决策树模型的节点的时候，在节点上所有的样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分</li>
</ul>
</li>
<li>如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="2-随机森林的随机性体现在哪里？"><a href="#2-随机森林的随机性体现在哪里？" class="headerlink" title="2. 随机森林的随机性体现在哪里？"></a>2. 随机森林的随机性体现在哪里？</h2><p><strong>多次有放回的随机取样，多次随机取属性</strong></p>
<h2 id="3-随机森林为什么不容易过拟合？"><a href="#3-随机森林为什么不容易过拟合？" class="headerlink" title="3. 随机森林为什么不容易过拟合？"></a>3. 随机森林为什么不容易过拟合？</h2><ul>
<li><p>随机森林中的每一颗树都是过拟合的，拟合到非常小的细节上</p>
</li>
<li><p>随机森林通过引入随机性，使每一颗树拟合的细节不同</p>
</li>
<li><p>所有树组合在一起，过拟合的部分就会自动被消除掉。</p>
</li>
</ul>
<p>因此随机森林出现过拟合的概率相对低。</p>
<h2 id="4-为什么不用全样本训练？"><a href="#4-为什么不用全样本训练？" class="headerlink" title="4. 为什么不用全样本训练？"></a>4. 为什么不用全样本训练？</h2><p>全样本训练忽视了局部样本的规律（各个决策树趋于相同），对于模型的泛化能力是有害的，使随机森<br>林算法在样本层面失去了随机性。</p>
<h2 id="5-为什么要随机特征？"><a href="#5-为什么要随机特征？" class="headerlink" title="5. 为什么要随机特征？"></a>5. 为什么要随机特征？</h2><p>随机特征保证基分类器的多样性（差异性），最终集成的泛化性能可通过个体学习器之间的差异度而进<br>一步提升，从而提高泛化能力和抗噪能力。</p>
<h2 id="6-RF与-GBDT-的区别？"><a href="#6-RF与-GBDT-的区别？" class="headerlink" title="6. RF与 GBDT 的区别？"></a>6. RF与 GBDT 的区别？</h2><ul>
<li>随机森林将多棵决策树的结果进行投票后得到最终的结果，对不同的树的训练结果也没有做进一步的优化提升，将其称为<strong>Bagging算法。</strong></li>
<li>GBDT用到的是<strong>Boosting算法</strong>，在迭代的每一步构建弱学习器弥补原有模型的不足。GBDT中的Gradient Boost就是通过每次迭代的时候构建一个沿梯度下降最快的方向的学习器。并且通过设置不同的损失函数可以处理各类学习任务(多分类、回归等)。</li>
</ul>
<h2 id="7-RF为什么比Bagging效率高？"><a href="#7-RF为什么比Bagging效率高？" class="headerlink" title="7. RF为什么比Bagging效率高？"></a>7. RF为什么比Bagging效率高？</h2><p>Bagging无随机特征，使得训练决策树时效率更低</p>
<h2 id="8-你已经建了一个有10000棵树的随机森林模型。在得到0-001的训练误差后，你非常高兴。但是，验证错误是34-23。到底是怎么回事？你还没有训练好你的模型吗？"><a href="#8-你已经建了一个有10000棵树的随机森林模型。在得到0-001的训练误差后，你非常高兴。但是，验证错误是34-23。到底是怎么回事？你还没有训练好你的模型吗？" class="headerlink" title="8. 你已经建了一个有10000棵树的随机森林模型。在得到0.001的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？"></a>8. 你已经建了一个有10000棵树的随机森林模型。在得到0.001的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？</h2><p>训练误差为0：<strong>模型过度拟合</strong></p>
<p>验证错误为34.23：该分类器用于<strong>未看见的样本</strong>上时，找不到已有的模式</p>
<p>因此，为了避免这些情况，要用交叉验证来调整树的数量。</p>
<h2 id="9-如何使用随机森林对特征重要性进行评估？"><a href="#9-如何使用随机森林对特征重要性进行评估？" class="headerlink" title="9. 如何使用随机森林对特征重要性进行评估？"></a>9. 如何使用随机森林对特征重要性进行评估？</h2><p><strong>袋外数据(OOB)</strong>： 大约有1/3的训练实例没有参与第k棵树的生成，它们称为第$k$棵树的袋外数据样本。 </p>
<p>在随机森林中某个特征$X$的重要性的计算方法如下：</p>
<ul>
<li>对于随机森林中的每一颗决策树，使用相应的OOB(袋外数据)来计算它的袋外数据误差，记为$errooB1$。</li>
<li>随机地对袋外数据OOB所有样本的特征$X$加入噪声干扰(就可以随机的改变样本在特征X处的值)，再次计算它的袋外数据误差，记为$errooB2$。</li>
<li>假设随机森林中有$N$棵树，那么对于特征$X$的重要性为$(errOOB2-errOOB1)/N)$，之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后，袋外的准确率大幅度降低，则说明这个特征对于样本的分类结果影响很大，也就是说它的重要程度比较高。</li>
</ul>
<h2 id="10-随机森林算法训练时主要需要调整哪些参数？"><a href="#10-随机森林算法训练时主要需要调整哪些参数？" class="headerlink" title="10. 随机森林算法训练时主要需要调整哪些参数？"></a><em>10. 随机森林算法训练时主要需要调整哪些参数？</em></h2><ul>
<li><p><strong>n_estimators:</strong>随机森林建立子树的数量。<br>较多的子树一般可以让模型有更好的性能，但同时让你的代码变慢。需要选择最佳的随机森林子树数量</p>
</li>
<li><p><strong>max_features：</strong>随机森林允许单个决策树使用特征的最大数量。<br>增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。但是，可以肯定，你通过增加max_features会降低算法的速度。因此，你需要适当的平衡和选择最佳max_features。</p>
</li>
<li><p><strong>max_depth：</strong> 决策树最大深度</p>
<p>默认决策树在建立子树的时候不会限制子树的深度</p>
</li>
<li><p><strong>max_leaf_nodes：</strong> 最大叶子节点数</p>
<p>通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。</p>
</li>
<li><p><strong>min_samples_split：</strong>内部节点再划分所需最小样本数<br>内部节点再划分所需最小样本数，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。</p>
</li>
<li><p><strong>min_samples_leaf：</strong> 叶子节点最少样本</p>
<p>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。</p>
</li>
<li><p><strong>min_impurity_split：</strong> 节点划分最小不纯度<br>这个值限制了决策树的增长，如果某节点的不纯度（基于基尼系数，均方差）小于这个阈值，则该节点不再生成子节点。即为叶子节点。一般不推荐改动默认值1e-7。</p>
</li>
</ul>
<h2 id="11-随机森林的优缺点"><a href="#11-随机森林的优缺点" class="headerlink" title="11. 随机森林的优缺点"></a>11. 随机森林的优缺点</h2><ul>
<li>优点<ul>
<li>训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。</li>
<li>由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。</li>
<li>在训练后，可以给出各个特征对于输出的重要性</li>
<li>由于采用了随机采样，训练出的模型的方差小，泛化能力强。</li>
<li>相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。</li>
<li>对部分特征缺失不敏感。</li>
</ul>
</li>
<li>缺点<ul>
<li>在某些噪音比较大的样本集上，RF模型容易陷入过拟合。</li>
<li>取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。</li>
</ul>
</li>
</ul>
<h2 id="12-简述一下Adaboost原理"><a href="#12-简述一下Adaboost原理" class="headerlink" title="12. 简述一下Adaboost原理"></a>12. 简述一下Adaboost原理</h2><p>Adaboost算法利用同一种基分类器（弱分类器），<strong>基于分类器的错误率分配不同的权重参数，最后累加加权的预测结果</strong>作为输出。</p>
<ul>
<li>Adaboost算法流程：<ul>
<li>样本赋予权重，得到第一个分类器。</li>
<li>计算该分类器的错误率，根据错误率赋予分类器权重（注意这里是<strong>分类器的权重</strong>）。</li>
<li>增加分错样本的权重，减小分对样本的权重（注意这里是<strong>样本的权重</strong>）。</li>
<li>然后再用<strong>新的样本权重</strong>训练数据，得到新的分类器。</li>
<li>多次迭代，直到分类器错误率为0或者整体弱分类器错误为0，或者到达迭代次数。</li>
<li>将所有弱分类器的结果加权求和，得到一个较为准确的分类结果。错误率低的分类器获得更高的决定系数，从而在对数据进行预测时起关键作用。</li>
</ul>
</li>
</ul>
<h2 id="13-AdaBoost的优点和缺点"><a href="#13-AdaBoost的优点和缺点" class="headerlink" title="13. AdaBoost的优点和缺点"></a>13. AdaBoost的优点和缺点</h2><ul>
<li>优点<ul>
<li>Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也不存在过拟合的现象。</li>
<li>Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。</li>
<li>Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。</li>
<li>Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。</li>
<li>Adaboost对同一个训练样本集训练不同的弱分类器，按照一定的方法把这些弱分类器集合起来，构造一个分类能力很强的强分类器，即“三个臭皮匠赛过一个诸葛亮””。</li>
</ul>
</li>
<li>缺点<ul>
<li>在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。</li>
<li>Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。</li>
</ul>
</li>
</ul>
<h2 id="14-Adaboost对噪声敏感吗？"><a href="#14-Adaboost对噪声敏感吗？" class="headerlink" title="14. Adaboost对噪声敏感吗？"></a>14. Adaboost对噪声敏感吗？</h2><p>在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。</p>
<h2 id="15-Adaboost和随机森林算法的异同点"><a href="#15-Adaboost和随机森林算法的异同点" class="headerlink" title="15. Adaboost和随机森林算法的异同点"></a>15. Adaboost和随机森林算法的异同点</h2><p>随机森林和Adaboost算法都可以用来分类，它们都是优秀的基于决策树的组合算法。</p>
<ul>
<li>相同之处<ul>
<li>二者都是Bootsrap自助法选取样本。</li>
<li>二者都是要训练很多棵决策树。</li>
</ul>
</li>
<li>不同之处<ul>
<li>Adaboost是基于Bagging的算法，随机森林是基于Boosting的算法。</li>
<li>Adaboost后面树的训练，其在变量抽样选取的时候，对于上一棵树分错的样本，抽中的概率会加大。</li>
<li>随机森林在训练每一棵树的时候，随机挑选了部分特征作为拆分特征，而不是所有的特征都去作为拆分特征。</li>
<li>在预测新数据时，Adaboost中所有的树加权投票来决定因变量的预测值，每棵树的权重和错误率有关；随机森林按照所有树中少数服从多数树的分类值来决定因变量的预测值（或者求取树预测的平均值）。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>RandomForest</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-集成学习</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>[TOC]</p>
<h2 id="1-什么是集成学习算法？"><a href="#1-什么是集成学习算法？" class="headerlink" title="1. 什么是集成学习算法？"></a>1. 什么是集成学习算法？</h2><ul>
<li><strong>集成学习算法是一种优化手段或者策略</strong>，不算是一种机器学习算法。</li>
<li>集成方法是由多个较弱的模型集成模型组，一般的弱分类器可以是决策树，SVM，KNN等构成。其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。</li>
<li>该算法主要的问题是要找出哪些较弱的模型可以结合起来，以及如何结合的方法。</li>
</ul>
<a id="more"></a>
<h2 id="2-集成学习主要有哪几种框架？"><a href="#2-集成学习主要有哪几种框架？" class="headerlink" title="2. 集成学习主要有哪几种框架？"></a>2. 集成学习主要有哪几种框架？</h2><p>集成学习从集成思想的架构分为Bagging，Boosting，Stacking三种。</p>
<h2 id="3-简单介绍一下bagging，常用bagging算法有哪些？"><a href="#3-简单介绍一下bagging，常用bagging算法有哪些？" class="headerlink" title="3. 简单介绍一下bagging，常用bagging算法有哪些？"></a>3. 简单介绍一下bagging，常用bagging算法有哪些？</h2><ul>
<li>Bagging<ul>
<li><strong>多次采样，训练多个分类器，集体投票，旨在减小方差</strong>，</li>
</ul>
</li>
<li><p>基于数据<strong>随机重抽样</strong>的分类器构建方法。从训练集中进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。</p>
</li>
<li><p>算法流程：</p>
<ul>
<li>输入为样本集$D={(x_1，y_1)，(x_2，y_2) \dots (x_m，y_m)}$，弱学习器算法，弱分类器迭代次数$T$。</li>
<li>输出为最终的强分类器$f(x)$</li>
</ul>
</li>
<li><p>对于$t=1，2 \dots T$</p>
<ul>
<li>对训练集进行第t次随机采样，共采集$T$次，得到包含$T$个样本的采样集$D_t$</li>
<li>用采样集$D_t$训练第$t$个弱学习器$G_t(x)$</li>
</ul>
</li>
<li><p>如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。<br>常用bagging算法：随机森林算法</p>
</li>
</ul>
<h2 id="4-简单介绍一下boosting，常用boosting算法有哪些？"><a href="#4-简单介绍一下boosting，常用boosting算法有哪些？" class="headerlink" title="4. 简单介绍一下boosting，常用boosting算法有哪些？"></a>4. 简单介绍一下boosting，常用boosting算法有哪些？</h2><ul>
<li>Boosting<ul>
<li><strong>基分类器层层叠加，聚焦分错的样本，旨在减小方差</strong></li>
</ul>
</li>
<li>涉及到两个部分，加法模型和前向分步算法。<ul>
<li>加法模型就是说强分类器由一系列弱分类器线性相加而成。</li>
<li>前向分步就是说在训练过程中，下一轮迭代产生的分类器是在上一轮的基础上训练得来的。</li>
</ul>
</li>
<li><p>算法流程：</p>
<ul>
<li><p>给定初始训练数据，由此训练出第一个基学习器；</p>
</li>
<li><p>根据基学习器的表现对样本进行调整，在之前学习器做错的样本上投入更多关注；</p>
</li>
<li>用调整后的样本，训练下一个基学习器；</li>
<li>重复上述过程T次，将T个学习器加权结合。</li>
</ul>
</li>
<li><p>常用boosting算法：</p>
<ul>
<li>Adaboost<ul>
<li>样本权重：思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。——会因为错误分类样本的权重不断提高而使得模型对噪声敏感。</li>
<li>弱分类器权重：采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。</li>
</ul>
</li>
<li>GBDT</li>
<li>XGBoost</li>
</ul>
</li>
</ul>
<h2 id="5-boosting思想的数学表达式是什么？"><a href="#5-boosting思想的数学表达式是什么？" class="headerlink" title="5. boosting思想的数学表达式是什么？"></a>5. boosting思想的数学表达式是什么？</h2><script type="math/tex; mode=display">
f(x)=w_{0}+\sum_{m=1}^{M} w_{m} \phi_{m}(x)</script><p>其中$w$是权重，$\phi$是弱分类器的集合，可以看出最终就是基函数的线性组合。</p>
<h2 id="6-简单介绍一下stacking，常用stacking算法有哪些？"><a href="#6-简单介绍一下stacking，常用stacking算法有哪些？" class="headerlink" title="6. 简单介绍一下stacking，常用stacking算法有哪些？"></a>6. 简单介绍一下stacking，常用stacking算法有哪些？</h2><ul>
<li>Stacking<ul>
<li><strong>多次采样，训练多个分类器，将输出作为最后的输入特征</strong></li>
</ul>
</li>
<li><p>将训练好的所有基模型对训练集进行预测，第个$i$基模型对第$i$个训练样本的预测值将作为新的训练集中第$i$个样本的第$i$个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。</p>
</li>
<li><p>stacking常见的使用方式：</p>
<ul>
<li>由k-NN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的Loqistic 回归组合。</li>
</ul>
</li>
</ul>
<h2 id="7-你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？"><a href="#7-你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？" class="headerlink" title="7. 你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？"></a>7. 你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？</h2><p>低偏差意味着模型的预测值接近实际值。换句话说，该模型有足够的灵活性，以模仿训练数据的分布。貌似很好，但是别忘了，一个灵活的模型没有泛化能力。这意味着，当这个模型用在对一个未曾见过的数据集进行测试的时候，它会令人很失望。<br>在这种情况下，我们可以使用bagging算法（如随机森林），以解决高方差问题。bagging算法把数据集分成重复随机取样形成的子集。然后，这些样本利用单个学习算法生成一组模型。接着，利用投票（分类）或平均（回归）把模型预测结合在一起。另外，为了应对大方差，我们可以：</p>
<ul>
<li>使用正则化技术，惩罚更高的模型系数，从而降低了模型的复杂性。</li>
<li>使用可变重要性图表中的前n个特征。</li>
<li>可以用于当一个算法在数据集中的所有变量里很难寻找到有意义信号的时候。</li>
</ul>
<h2 id="8-常用的基分类器是什么？"><a href="#8-常用的基分类器是什么？" class="headerlink" title="8. 常用的基分类器是什么？"></a>8. 常用的基分类器是什么？</h2><p>最常用的基分类器是决策树,原因:</p>
<ul>
<li>决策树是低偏差高方差的，适合作为基分类器</li>
<li>决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重。</li>
<li>决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。</li>
<li>数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，<strong>很好地引入了随机性。</strong></li>
</ul>
<h2 id="9-可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？"><a href="#9-可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？" class="headerlink" title="9. 可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？"></a>9. 可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？</h2><p>不能：</p>
<ul>
<li>Bagging的主要好处是集成后的分类器的方差，比基分类器的方差小。</li>
<li><p>随机森林属于Bagging类的集成学习，对样本分布较为敏感的分类器更适用于Bagging。</p>
</li>
<li><p>线性分类器或者K-近邻都是较为稳定的分类器，本身方差就不大。</p>
</li>
<li>线性分类器或者K-近邻可能会由于Bagging的采样，导致在训练中更难收敛，增大偏差。</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>优化器</title>
    <url>/2022/01/01/%E4%BC%98%E5%8C%96%E5%99%A8/</url>
    <content><![CDATA[<h1 id="GD-amp-SGD-amp-Mini-batch-GD"><a href="#GD-amp-SGD-amp-Mini-batch-GD" class="headerlink" title="GD&amp;SGD&amp;Mini-batch GD"></a>GD&amp;SGD&amp;Mini-batch GD</h1><script type="math/tex; mode=display">
\theta=\theta-\eta \cdot \nabla_{\theta} J(\theta)</script><a id="more"></a>
<ul>
<li>GD<ul>
<li>用了全量样本，凸函数可以到全局最小值，非凸函数可以到局部最小值</li>
<li>慢，不能在线serving</li>
</ul>
</li>
<li>SGD<ul>
<li>快</li>
<li>损失函数震荡</li>
</ul>
</li>
<li>Mini-batch GD<ul>
<li>GD与SGD的结合：降低了参数更新时的方差，收敛较SGD更稳定</li>
<li>速度较快：可以利用矩阵运算</li>
</ul>
</li>
</ul>
<p>两个问题</p>
<ul>
<li>收敛能力<ul>
<li>lr太大，容易震荡，甚至偏离极小值；lr太小，收敛慢</li>
<li>非凸函数，容易陷于局部最小值/鞍点</li>
</ul>
</li>
<li>自适应lr调节<ul>
<li>SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。<br>Mini-batch gradient descent 不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）<br>对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是训练集全集带入即BGD，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动。）</li>
</ul>
</li>
</ul>
<h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><ul>
<li>加上惯性，尝试解决容易陷入局部最小值/鞍点的问题<ul>
<li>使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡<br>$ v<em>{t-1} $上加梯度，可以认为是对 $ v</em>{t-1} $ 的预判</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{array}{l}
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\
\theta=\theta-v_{t}
\end{array}</script><ul>
<li>由Polyak提出，也叫Polyak动量<br>Nesterov</li>
<li>优化Polyak动量，“预判我的预判”<script type="math/tex; mode=display">\begin{array}{ll}
v_{t} & =\beta v_{t-1}+\nabla_{\theta} J\left(\theta_{t}-\eta \beta v_{t-1}\right) \\
\theta_{t+1} & =\theta_{t}-\eta v_{t} \end{array}</script></li>
<li>从工程实现上，无法直接获得预判点的梯度，可得如下等价公式：<script type="math/tex; mode=display">\begin{array}{ll}
v_{t} & =\beta v_{t-1}+g_{t}+\beta\left(g_{t}-g_{t-1}\right) \\
\theta_{t+1} & =\theta_{t}-\eta v_{t}
\end{array}</script></li>
</ul>
<h1 id="AdaGrad-amp-RMSProp-amp-Adam"><a href="#AdaGrad-amp-RMSProp-amp-Adam" class="headerlink" title="AdaGrad&amp;RMSProp&amp;Adam"></a>AdaGrad&amp;RMSProp&amp;Adam</h1><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><script type="math/tex; mode=display">\Large \begin{cases} 
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
v_t = v_{t-1} + g_t^2 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{v_t + \epsilon}} g_t
\end{cases}</script><h2 id="RMSPropV2"><a href="#RMSPropV2" class="headerlink" title="RMSPropV2"></a>RMSPropV2</h2><script type="math/tex; mode=display">\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
v_t = \beta v_{t-1} + g_t^2 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{v_t + 1}} g_t
\end{cases}</script><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><script type="math/tex; mode=display">\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\gamma = \frac{\sqrt{1-\beta_2^{t}}}{1-\beta_1^t} \\
\theta_{t+1} = \theta_{t} - \eta \frac{\gamma}{\sqrt{v_t +  \epsilon}} m_t
\end{cases}</script><ul>
<li>AdaGrad: Adaptive Gradient<ul>
<li>学习率的自适应调节：学习率 除以 梯度平方累积项，更新次数越多（高频特征）的更新步长减小，稀疏特征步长增大</li>
<li>后期学不动</li>
</ul>
</li>
<li>RMSProp：Root Mean Square Propogation，为了解决AdaGrad后期学不动的问题<ul>
<li>累积梯度时加上衰减系数，越早的梯度会逐渐衰减</li>
</ul>
</li>
<li>Adam：Adaptive Moment Optimization, <ul>
<li>RMSProp + Momentum </li>
</ul>
</li>
</ul>
<p><strong>以下内容未解锁</strong></p>
<!-- # AdaMom: An optional optimizer for Dense Module
$$\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t = \beta_2 v_{t-1} + g_t^2 \\
c_t = \beta_2 c_{t-1} + 1 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{\frac{v_t}{c_t} +  \epsilon}} m_t
\end{cases}$$
- Adam的基础上，对二阶动量求累计更新次数的平均
- Momentum
 Adagrad仅使用二阶动量，导致累计梯度平方和单调增长，在某些维度变量频繁更新之后，会导致累计梯度平方和极大，参数几乎不再更新，也就是我们常说的学习不动了。
- 在AdaMom中，我们单纯对二阶动量做累计更新次数的均值，缓解上述学习不动的情况。同时继承Adam算法中引入一阶动量的优势。其次，为了更适应在线学习的场景，我们去除了计算二阶动量时在gradient平方上的decay。在这些调整的基础上，可以将学习率设置更小，对学习稳定性和快速收敛带来优势。同时我们观察到adamom训练过程产生的gradient远小于其他优化器，印证了对学习的稳定性和长期更新的判断。但目前没有进行数学论证。
- 和Adam的不同在于早期学习率的差距
AdaMom可以看成Adam的生产环境改进版，前期降低了m_t的方差/自带了warmup，有效缓解了Dense部分在异步更新场景初期可能出现的梯度爆炸

# AdaNest: an adaptive dense optimizer with Nesterov acceleration

- 公式
  - 1: Nesterov的工程实现
  - 2: 保证所有系数和为1，类似配分函数
  - 3: 继承于AdaMom，Vt除以梯度更新次数，借此解决AdaGrad的梯度衰减问题 
- 保留Adam/AdaMom二阶动量特性的同时，使用更加激进的Nesterov动量替换Polyak动量，提升了效果
- 从引入预判的角度看，AdaNest因为使用了预测点的梯度，对梯度的估计更加准确，总体比Adam更激进
- Adam与RMSProp的折衷，相比RMSProp引入了动量，相比Adam赋予$$g_t$$ 更大的权重


# GroupLasso
- FTRL的vector版本，为了增强vector的稀疏性，以此来减小PS内存
Summary -->
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>ML</tag>
        <tag>优化器</tag>
      </tags>
  </entry>
  <entry>
    <title>学习资料-会议&amp;期刊</title>
    <url>/2021/12/10/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99-%E4%BC%9A%E8%AE%AE&amp;%E6%9C%9F%E5%88%8A/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><ul>
<li><a href="https://recsys.acm.org/">RecSys</a> -The ACM Conference Series on Recommender Systems.</li>
</ul>
<a id="more"></a>
<h2 id="信息检索"><a href="#信息检索" class="headerlink" title="信息检索"></a>信息检索</h2><ul>
<li><a href="http://sigir.org/">SIGIR</a>- The ACM International Conference on Research and Development in Information Retrieval</li>
</ul>
<h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><ul>
<li><a href="http://www.kdd.org/">KDD</a> - The ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</li>
<li><a href="http://www.wsdm-conference.org/">WSDM</a> - The International Conference on Web Search and Data Mining.</li>
<li><a href="http://cs.uvm.edu/~icdm/">ICDM</a> - The IEEE International Conference on Data Mining.</li>
<li><a href="http://www.siam.org/meetings/sdm19/">SDM</a> -The SIAM International Conference on Data Mining.</li>
<li><a href="https://ecmlpkdd2020.net/">ECML-PKDD</a> - The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</li>
</ul>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ul>
<li><a href="https://icml.cc/">ICML</a> - The International Conference on Machine Learning.</li>
<li><a href="https://nips.cc/">NIPS</a> - The Conference on Neural Information Processing Systems</li>
</ul>
<h2 id="AI"><a href="#AI" class="headerlink" title="AI"></a>AI</h2><ul>
<li><a href="https://aaai.org/Conferences/AAAI-18/">AAAI</a> - The National Conference of the American Association for Artificial Intelligence.</li>
<li><a href="http://www.ijcai.org/">IJCAI</a> - The International Joint Conference on Artificial Intelligence.</li>
<li><a href="http://ecai2020.eu/">ECAI</a> -European Conference on Artificial Intelligence</li>
<li><a href="http://www.auai.org/uai2020/">UAI</a> - The Conference on Uncertainty in Artificial Intelligence</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Resource</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Feature Engineering</title>
    <url>/2021/12/10/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="特征离散的作用"><a href="#特征离散的作用" class="headerlink" title="特征离散的作用"></a>特征离散的作用</h2><a id="more"></a>
<ul>
<li><strong>离散化后的特征对异常数据有很强的鲁棒性</strong><br>比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；离散化后年龄300岁也只对应于一个权重，如果训练数据中没有出现特征”年龄-300岁”，那么在LR模型中，其权重对应于0，所以，即使测试数据中出现特征”年龄-300岁”,也不会对预测结果产生影响。特征离散化的过程，比如特征A，如果当做连续特征使用，在LR模型中，A会对应一个权重w,如果离散化，那么A就拓展为特征A-1，A-2，A-3…,每个特征对应于一个权重，如果训练样本中没有出现特征A-4，那么训练的模型对于A-4就没有权重，如果测试样本中出现特征A-4,该特征A-4也不会起作用。相当于无效。但是，如果使用连续特征，在LR模型中，y = w*a,a是特征，w是a对应的权重,比如a代表年龄，那么a的取值范围是[0..100]，如果测试样本中,出现了一个测试用例，a的取值是300，显然a是异常值，但是w*a还是有值，而且值还非常大，所以，异常值会对最后结果产生非常大的影响。</li>
<li><strong>特征离散化后，模型会更稳定</strong><br>比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；按区间离散化，划分区间是非常关键的。</li>
<li><strong>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险</strong><br>当使用连续特征时，一个特征对应于一个权重，那么，如果这个特征权重较大，模型就会很依赖于这个特征，这个特征的一个微小变化可能会导致最终结果产生很大的变化，这样子的模型很危险，当遇到新样本的时候很可能因为对这个特征过分敏感而得到错误的分类结果，也就是泛化能力差，容易过拟合。而使用离散特征的时候，一个特征变成了多个，权重也变为多个，那么之前连续特征对模型的影响力就被分散弱化了，从而降低了过拟合的风险。</li>
<li><strong>引入非线性</strong><br>单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；在LR模型中，特征A作为连续特征对应的权重是Wa。A是线性特征，因为y = Wa*A,y对于A的导数就是Wa,如果离散化后，A按区间离散化为A_1,A_2,A_3。那么y = w_1*A_1+w_2*A_2+w_3*A_3.那么y对于A的函数就相当于分段的线性函数，y对于A的导数也随A的取值变动，所以，相当于引入了非线性。</li>
<li><strong>离散化后方便进行特征交叉</strong><br>离散化后变为了类别，方便特征交叉。进一步引入非线性，提升表达能力。</li>
<li>离散特征的增加和减少都很容易，易于模型的快速迭代。(离散特征的增加和减少，模型也不需要调整，重新训练是必须的，相比贝叶斯推断方法或者树模型方法迭代快)</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>FeatureEngineering</tag>
      </tags>
  </entry>
  <entry>
    <title>相似度</title>
    <url>/2022/01/02/%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    <content><![CDATA[<h1 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h1><ul>
<li>可以分为距离类、角度类、角度+距离类<a id="more"></a>
</li>
</ul>
<h1 id="距离类"><a href="#距离类" class="headerlink" title="距离类"></a>距离类</h1><h2 id="欧式距离（Euclidean-Distance）"><a href="#欧式距离（Euclidean-Distance）" class="headerlink" title="欧式距离（Euclidean Distance）"></a>欧式距离（Euclidean Distance）</h2><ul>
<li>欧式距离：欧式空间中两点间的距离公式。</li>
<li>例如平面空间内的欧式距离：<script type="math/tex; mode=display">
d=\sqrt{(x 1-x 2)^{2}+(y 1-y 2)^{2}}</script></li>
</ul>
<h2 id="曼哈顿距离-Manhattan-Distance"><a href="#曼哈顿距离-Manhattan-Distance" class="headerlink" title="曼哈顿距离(Manhattan Distance)"></a>曼哈顿距离(Manhattan Distance)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Manhattan</span>(<span class="params">dataA,dataB</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(dataA - dataB))</span><br><span class="line">print(Manhattan(dataA,dataB))</span><br></pre></td></tr></table></figure>
<h2 id="汉明距离（Hamming-distance）"><a href="#汉明距离（Hamming-distance）" class="headerlink" title="汉明距离（Hamming distance）"></a>汉明距离（Hamming distance）</h2><ul>
<li>汉明距离表示的是两个字符串（相同长度）对应位不同的数量。比如有两个等长的字符串 str1 = “11111” 和 str2 = “10001” 那么它们之间的汉明距离就是3（这样说就简单多了吧。哈哈）。</li>
<li><strong>汉明距离多用于图像像素的匹配（同图搜索）、通信领域统计错误数据位的数量等</strong>。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hammingDistance</span>(<span class="params">dataA,dataB</span>):</span></span><br><span class="line">    distanceArr = dataA - dataB</span><br><span class="line">    <span class="keyword">return</span> dataA.shape[<span class="number">0</span>] - np.<span class="built_in">sum</span>(distanceArr == <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="角度类"><a href="#角度类" class="headerlink" title="角度类"></a>角度类</h1><h2 id="余弦相似度（Cosine）"><a href="#余弦相似度（Cosine）" class="headerlink" title="余弦相似度（Cosine）"></a>余弦相似度（Cosine）</h2><ul>
<li>首先，样本数据的夹角余弦并不是真正几何意义上的夹角余弦，只不过是借了它的名字，实际是借用了它的概念变成了是代数意义上的“夹角余弦”，<strong>用来衡量样本向量间的差异</strong>。</li>
<li>二维情况下，即几何意义上的夹角余弦。夹角越小，余弦值越接近于1，反之则趋于-1。<script type="math/tex; mode=display">
\cos (\theta)=\frac{\sum_{k=1}^{n} x_{1 k} x_{2 k}}{\sqrt{\sum_{k=1}^{n} x_{1 k}^{2}} \sqrt{\sum_{k=1}^{n} x_{2 k}{ }^{2}}}</script></li>
<li>余弦相似度对夹角敏感，对数值的相对大小不敏感</li>
</ul>
<h2 id="皮尔逊相关系数（Pearson-Correlation-Coefficient）"><a href="#皮尔逊相关系数（Pearson-Correlation-Coefficient）" class="headerlink" title="皮尔逊相关系数（Pearson Correlation Coefficient）"></a>皮尔逊相关系数（Pearson Correlation Coefficient）</h2><script type="math/tex; mode=display">
\operatorname{sim}\left(x_{1}, x_{2}\right)=\frac{\sum_{k=1}^{n}\left(x_{1 k}-\overline{x_{1}}\right)\left(x_{2 k}-\overline{x_{2}}\right)}{\sqrt{\sum_{k=1}^{n}\left(x_{1 k}-\overline{x_{1}}\right)^{2}} \sqrt{\sum_{k=1}^{n}\left(x_{2 k}-\overline{x_{2}}\right)^{2}}}</script><ul>
<li>在计算夹角余弦之前将两个向量减去各个样本的平均值，达到<strong>中心化</strong>的目的。<strong>皮尔逊相关函数是余弦相似度在维度缺失上面的一种改进方法</strong>。</li>
</ul>
<h1 id="角度-距离"><a href="#角度-距离" class="headerlink" title="角度+距离"></a>角度+距离</h1><h2 id="点积相似度（Dot）"><a href="#点积相似度（Dot）" class="headerlink" title="点积相似度（Dot）"></a>点积相似度（Dot）</h2><script type="math/tex; mode=display">
\vec{a} * \vec{b}=\overrightarrow{|a||b|} \mid \cos \theta</script><ul>
<li>常见于word2vec、deepwalk等算法中，用于作为loss（最小化向量ab的模以及两者的夹角），也可以理解为decoder（两向量的点积可以理解为原空间的两点相似度）</li>
<li>当向量模长固定时，点积相似度等同于余弦相似度</li>
</ul>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/33164335">from zhihu</a></li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-经验风险、期望风险、结构风险</title>
    <url>/2021/12/10/%E7%BB%8F%E9%AA%8C%20%E6%9C%9F%E6%9C%9B%20%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h3 id="经验风险"><a href="#经验风险" class="headerlink" title="经验风险"></a>经验风险</h3><ul>
<li>对训练样本误差的衡量，例如最典型的是每个样本损失函数的平均，Σ(loss(Yi, f(Xi))) / m</li>
<li>基于经验风险最小化的模型泛化能力差，只代表了局部最优 —&gt; 过拟合</li>
</ul>
<a id="more"></a>
<h3 id="期望风险"><a href="#期望风险" class="headerlink" title="期望风险"></a>期望风险</h3><ul>
<li>要寻找全局最优，可以考虑使用损失函数的期望，即E(loss(Yi, f(Xi))) = ∫(P(Xi,Yi)loss(Yi, f(Xi)))dxdy</li>
<li>此处假设了(x, y)的联合分布</li>
<li>问题：对于大部分样本无法获知其具体的分布 —&gt; 不现实</li>
</ul>
<h3 id="结构风险"><a href="#结构风险" class="headerlink" title="结构风险"></a>结构风险</h3><ul>
<li>在经验风险之上添加正则项，模拟期望风险</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>CV-SIFT</title>
    <url>/2021/12/10/CV-SIFT/</url>
    <content><![CDATA[<meta name="referrer" content="no-referrer"/>

<p>[TOC]</p>
<h2 id="SIFT-Scale-invariant-feature-transform-综述"><a href="#SIFT-Scale-invariant-feature-transform-综述" class="headerlink" title="SIFT(Scale-invariant feature transform)综述"></a>SIFT(Scale-invariant feature transform)综述</h2><ul>
<li>尺度不变特征转换：SIFT是一种在<strong>多尺度空间上提取局部特征</strong>的方法。它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由 David Lowe在1999年所发表，2004年完善总结。</li>
</ul>
<a id="more"></a>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性；</li>
<li>独特性（Distinctiveness）好，信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配；</li>
<li>多量性，即使少数的几个物体也可以产生大量的SIFT特征向量；</li>
<li>高速性，经优化的SIFT匹配算法甚至可以达到实时的要求；</li>
<li>可扩展性，可以很方便的与其他形式的特征向量进行联合。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>实时性不高</li>
<li>有时特征点数量较少</li>
<li>对边缘光滑的目标无法准确提取特征点</li>
</ul>
<h3 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h3><ol>
<li>建立尺度空间，即DoG金字塔</li>
<li>关键点检测及定位，获得每个关键点的：<strong>尺度、位置</strong></li>
<li>特征点方向赋值，获得每个特征点的：<strong>方向</strong></li>
<li>关键点特征描述</li>
</ol>
<p><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330204125547-535493662.png" alt="整体流程"></p>
<h2 id="建立DoG金字塔"><a href="#建立DoG金字塔" class="headerlink" title="建立DoG金字塔"></a>建立DoG金字塔</h2><ul>
<li>获取多尺度空间：Lindeberg等人已证明高斯卷积核是实现尺度变换的唯一变换核，并且是唯一的线性核</li>
</ul>
<h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><h4 id="二维高斯函数"><a href="#二维高斯函数" class="headerlink" title="二维高斯函数"></a>二维高斯函数</h4><ul>
<li>以(m/2, n/2)为中心的二维高斯分布<script type="math/tex; mode=display">
G(x, y)=\frac{1}{2 \pi \sigma^{2}} e^{-\frac{(x-m / 2)^{2}+(y-n / 2)^{2}}{2 \sigma^{2}}}</script></li>
<li>生成的曲面等高线是从中心开始呈正态分布的同心圆，权重值从中心向周围越来越小。这样进行模糊处理比其它的均衡模糊滤波器更高地保留了边缘效果。</li>
<li>实际应用中，在计算高斯函数的离散近似时，在大概3σ距离之外的像素都可以看作不起作用，因此图像处理程序只需要计算$<br>(6 \sigma+1) \times(6 \sigma+1)<br>$的矩阵</li>
</ul>
<h4 id="二维高斯函数的优化"><a href="#二维高斯函数的优化" class="headerlink" title="二维高斯函数的优化"></a>二维高斯函数的优化</h4><ol>
<li>若高斯模板大小为m*n，当图像大小为M*N时，计算复杂度为O(m*n*M*N)</li>
<li>$\sigma$越大，高斯模板分散性越强，处理边缘点时，丢失的图像信息越多，可能会造成黑边</li>
</ol>
<ul>
<li>可以证明，二维高斯运算可以优化为：水平方向进行一维高斯矩阵变换加上竖直方向的一维高斯矩阵变换得到。时间复杂度为：$<br>O(n \times M \times N)+O(m \times M \times N)<br>$。<a href="https://blog.csdn.net/lwx309025167/article/details/82761474">证明</a><br><img src="https://img-blog.csdn.net/20180919111255142?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x3eDMwOTAyNTE2Nw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="证明"></li>
</ul>
<h4 id="高斯拉普拉斯算子-Laplacion-of-Gaussian-LoG"><a href="#高斯拉普拉斯算子-Laplacion-of-Gaussian-LoG" class="headerlink" title="高斯拉普拉斯算子(Laplacion of Gaussian, LoG)"></a>高斯拉普拉斯算子(Laplacion of Gaussian, LoG)</h4><ul>
<li>作用：突出边缘，更清晰，对比度更强</li>
<li>拉普拉斯：拉普拉斯算子是图像二阶空间导数的二维各向同性测度。<strong>拉普拉斯算子可以突出图像中强度发生快速变化的区域</strong>，因此常用在边缘检测任务当中。</li>
<li>高斯：在进行Laplacian操作之前通常需要先用高斯平滑滤波器对图像进行平滑处理，以<strong>降低Laplacian操作对于噪声的敏感性</strong>。</li>
<li>输入输出均为灰度图</li>
<li>连续拉普拉斯：<script type="math/tex; mode=display">
L(x, y)=\frac{\partial^{2} I}{\partial x^{2}}+\frac{\partial^{2} I}{\partial y^{2}}</script></li>
<li>离散拉普拉斯，例如：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>0</th>
<th>-1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>-1</td>
<td>4</td>
<td>-1</td>
</tr>
<tr>
<td>0</td>
<td>-1</td>
<td>-1</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>-1</th>
<th>-1</th>
<th>-1</th>
</tr>
</thead>
<tbody>
<tr>
<td>-1</td>
<td>8</td>
<td>-1</td>
</tr>
<tr>
<td>-1</td>
<td>-1</td>
<td>-1</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>表达：以0为中心，$\sigma$为标准差的LoG<script type="math/tex; mode=display">
L o G(x, y)=-\frac{1}{\pi \sigma^{4}}\left[1-\frac{x^{2}+y^{2}}{2 \sigma^{2}}\right] e^{-\frac{x^{2}+y^{2}}{2 \sigma^{2}}}</script><img src="https://pic1.zhimg.com/80/v2-f824bd5eae07235ebf531fa0b546ba98_1440w.jpg" alt="LoG示意图"><blockquote>
<p>推导过程：<img src="https://pic4.zhimg.com/80/v2-b220086b94625a8a00ce68eb2a4bd0e3_1440w.jpg" alt="推导过程"></p>
</blockquote>
</li>
<li>示例：一维LoG滤波器对于边缘的响应：<img src="https://pic4.zhimg.com/80/v2-12ce9caee0278bf8855efdecfcba7fc7_1440w.jpg" alt="一维LoG滤波器对于边缘的响应"></li>
</ul>
<h4 id="高斯差分算子-Difference-of-Gaussian-DOG"><a href="#高斯差分算子-Difference-of-Gaussian-DOG" class="headerlink" title="高斯差分算子(Difference of Gaussian, DOG)"></a>高斯差分算子(Difference of Gaussian, DOG)</h4><ul>
<li>可以近似LoG算子，减少运算量，由Lindeberg在1994证明</li>
<li>DoG的运算如下，即DoG为两不同尺度的高斯算子平滑后的图像之差，具体证明见：<a href="https://blog.csdn.net/kieven2008/article/details/104309440">证明</a><script type="math/tex; mode=display">
\begin{aligned}
D(x, y, \sigma) &=(G(x, y, k \sigma)-G(x, y, \sigma)) * I(x, y) \\
&=L(x, y, k \sigma)-L(x, y, \sigma)
\end{aligned}</script></li>
</ul>
<h4 id="尺度空间理论"><a href="#尺度空间理论" class="headerlink" title="尺度空间理论"></a>尺度空间理论</h4><ul>
<li>定义<ul>
<li>单尺度 —-&gt; 多尺度</li>
<li>尺度空间中各尺度图像的模糊程度逐渐变大，能够模拟人在距离目标由近到远时目标在视网膜上的形成过程。</li>
</ul>
</li>
<li>要求：满足视觉不变性<ul>
<li>亮度/灰度不变性，对比度不变性</li>
<li>平移不变性、尺度不变性、欧几里得不变性、仿射不变性</li>
</ul>
</li>
<li>方法：<ul>
<li>Tony Lindeberg指出<strong>尺度规范化的LoG(Laplacion of Gaussian)算子具有真正的尺度不变性</strong>。Lowe使用高斯差分金字塔近似LoG算子，是尺度空间检测稳定的关键点。</li>
</ul>
</li>
<li>表示：一个图像的尺度空间$L(x,y,\sigma)$，可以表示为一个变化尺度的高斯函数与原图像的卷积<script type="math/tex; mode=display">
L(x, y, \sigma)=G(x, y, \sigma) * I(x, y)</script><script type="math/tex; mode=display">
G(x, y, \sigma)=\frac{1}{2 \pi \sigma^{2}} e^{-\frac{(x-m / 2)^{2}+(y-n / 2)^{2}}{2 \sigma^{2}}}</script><ul>
<li>m，n表示高斯模板中心，维度为$<br>(6 \sigma+1) \times(6 \sigma+1)<br>$，(x, y)代表图像的像素位置。$\sigma$是尺度空间因子，值越小表示图像被平滑的越少，相应的尺度也就越小。<strong>大尺度对应于图像的概貌特征，小尺度对应于图像的细节特征。</strong></li>
</ul>
</li>
</ul>
<h3 id="建立DoG金字塔-1"><a href="#建立DoG金字塔-1" class="headerlink" title="建立DoG金字塔"></a>建立DoG金字塔</h3><ul>
<li>极值：图像域和尺度域上的极值<br><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330204227104-18141123.png" alt="步骤"></li>
</ul>
<h4 id="构建高斯金字塔"><a href="#构建高斯金字塔" class="headerlink" title="构建高斯金字塔"></a>构建高斯金字塔</h4><ol>
<li>先将原图像扩大一倍之后作为高斯金字塔的第1组第1层，将第1组第1层图像经高斯卷积（其实就是高斯平滑或称高斯滤波）之后作为第1组金字塔的第2层。对于参数σ，在Sift算子中取的是固定值1.6。</li>
<li>将σ乘以一个比例系数k，得到一个新的平滑因子σ=k*σ，用它来平滑第1组第2层图像，结果图像作为第3层。</li>
<li>如此这般重复，最后得到L层图像，在同一组中，每一层图像的尺寸都是一样的，只是平滑系数不一样。它们对应的平滑系数分别为：0，σ，kσ，k^2σ,k^3σ……k^(L-2)σ。</li>
<li>将第1组<strong>倒数第三层</strong>图像作比例因子为2的降采样，得到的图像作为第2组的第1层，然后对第2组的第1层图像做平滑因子为σ的高斯平滑，得到第2组的第2层，就像步骤2中一样，如此得到第2组的L层图像，同组内它们的尺寸是一样的，对应的平滑系数分别为：0，σ，kσ，k^2σ,k^3σ……k^(L-2)σ。但是在尺寸方面第2组是第1组图像的一半。</li>
<li>反复执行，得到一共O组，每组L层，共计O*L个图像，这些图像一起就构成了高斯金字塔，结构如下：<br><img src="https://img-blog.csdn.net/20160917212318336" alt="高斯金字塔"></li>
</ol>
<h4 id="构建高斯差分-DOG-金字塔"><a href="#构建高斯差分-DOG-金字塔" class="headerlink" title="构建高斯差分(DOG)金字塔"></a>构建高斯差分(DOG)金字塔</h4><ul>
<li>方法<br><img src="https://img-blog.csdn.net/20160917223500317" alt="高斯差分金字塔"></li>
<li>经过归一化的高斯差分金字塔<br><img src="https://img-blog.csdn.net/20160917232151713" alt="一个示例"></li>
</ul>
<h4 id="具体构建过程中的参数计算"><a href="#具体构建过程中的参数计算" class="headerlink" title="具体构建过程中的参数计算"></a>具体构建过程中的参数计算</h4><ul>
<li>极值点：每一个像素点要和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。要获取S个尺度的极值点—-&gt;需要S+2个DOG空间—&gt;S+3层高斯金字塔</li>
<li>实际计算时S在3到5之间</li>
<li>输入：组数o，层数S，$k=2^{\frac{1}{S}}$，以及计算每组每层的尺度参数$\sigma(o, s)=\sigma_{0} 2^{\circ+\frac{s}{S}} o \in[0, \ldots, O-1], s \in[0, \ldots, S+2]$<ul>
<li>$\sigma_{0}$为初始尺度</li>
</ul>
</li>
<li>相同组间不同层的尺度参数$\sigma$为k倍之差，不同组相同层之间尺度参数$\sigma$为2倍之差</li>
</ul>
<h2 id="关键点定位"><a href="#关键点定位" class="headerlink" title="关键点定位"></a>关键点定位</h2><h3 id="离散极值点检测"><a href="#离散极值点检测" class="headerlink" title="离散极值点检测"></a>离散极值点检测</h3><ul>
<li>DoG金字塔中，每个点与周围8+前后两个尺度9*2=26个点比较，来确定极值点</li>
</ul>
<h3 id="关键点（连续极值点）定位"><a href="#关键点（连续极值点）定位" class="headerlink" title="关键点（连续极值点）定位"></a>关键点（连续极值点）定位</h3><ul>
<li>动机<ul>
<li>获得连续空间的极值点（位置、尺度、极值大小），并去除低对比度的关键点</li>
<li>去除不稳定的边缘响应点（DoG算子会产生较强的边缘响应），增强匹配稳定性、提高降噪能力</li>
</ul>
</li>
</ul>
<h4 id="关键点定位-1"><a href="#关键点定位-1" class="headerlink" title="关键点定位"></a>关键点定位</h4><ul>
<li>离散空间的极值点只是局部区域<br><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330205035199-488432412.jpg" alt="离散空间极值点与连续空间极值点的区别"></li>
<li>为了获得连续空间的极值点，需要对离散空间DoG函数进行曲线插值/拟合。拟合的方法是利用DoG函数在尺度空间上的泰勒展开。</li>
</ul>
<ol>
<li>在任意一个坐标为$X_0=(x_0,y_0,\sigma_0)$的极值点处做DoG函数的泰勒展开，舍掉2阶以后的项，结果如下：<script type="math/tex; mode=display">
f\left(\left[\begin{array}{l}
x \\
y \\
\sigma
\end{array}\right]\right) \approx f\left(\left[\begin{array}{l}
x_{0} \\
y_{0} \\
\sigma_{0}
\end{array}\right]\right) \mid+\left[\begin{array}{lll}
\frac{\partial f}{\partial x} & \frac{\partial f}{\partial y} & \frac{\partial f}{\partial \sigma}
\end{array}\right]\left(\left[\begin{array}{l}
x \\
y \\
\sigma
\end{array}\right]-\left[\begin{array}{l}
x_{0} \\
y_{0} \\
\sigma_{0}
\end{array}\right]\right) + 
\frac{1}{2}\left(\left[\begin{array}{llll}
x & y & \sigma
\end{array}\right]-\left[\begin{array}{lll}
x_{0} & y_{0} & \sigma_{0}
\end{array}\right]\right)\left[\begin{array}{lll}
\frac{\partial^{2} f}{\partial x \partial x} & \frac{\partial^{2} f}{\partial x \partial y} & \frac{\partial^{2} f}{\partial x \partial \sigma} \\
\frac{\partial^{2} f}{\partial x \partial y} & \frac{\partial^{2} f}{\partial y \partial y} & \frac{\partial^{2} f}{\partial y \partial \sigma} \\
\frac{\partial^{2} f}{\partial x \partial \sigma} & \frac{\partial^{2} f}{\partial y \partial \sigma} & \frac{\partial^{2} f}{\partial \sigma \partial \sigma}
\end{array}\right]\left(\left[\begin{array}{l}
x \\
y \\
\sigma
\end{array}\right]-\left[\begin{array}{l}
x_{0} \\
y_{0} \\
\sigma_{0}
\end{array}\right]\right)</script>其向量形式如下：<script type="math/tex; mode=display">
D(X)=D+\frac{\partial D^{T}}{\partial X} X+\frac{1}{2} X^{T} \frac{\partial^{2} D}{\partial X^{2}} X</script></li>
<li>求导并让方程==0，求得极值点相对$X_0=(x_0,y_0,\sigma_0)$的偏移量为$\hat{X}=-\frac{\partial^{2} D^{-1}}{\partial X^{2}} \frac{\partial D}{\partial X}$。</li>
<li>当此偏移量在任一维度（x, y, $\sigma$）上大于0.5时，意味着此极值点已经去临近的点上了，因此改变该极值点位置。</li>
<li>不断迭代1~3。终止条件：收敛/超出迭代次数（Lowe设定为5）/超出图像边界（此时应删除该点）。记录所有点的位置（原位置+偏移量）以及尺度$\sigma(o, s)$.</li>
<li>为加强抗噪声的能力，删除$|D(x)|$过小的点（Lowe论文中使用0.03，Rob Hess等人实现时使用0.04)。</li>
</ol>
<h4 id="边缘效应消除"><a href="#边缘效应消除" class="headerlink" title="边缘效应消除"></a>边缘效应消除</h4><ul>
<li>一个定义不好的DoG算子的极值在横跨边缘的地方有较大的主曲率，而在垂直边缘的方向有较小的主曲率。即<strong>两个主曲率的比值越大</strong>。<ul>
<li>主曲率：曲面的每个方向都有法曲率，那么就有最大最小的法曲率，这个最大最小值就是主曲率，对应的曲线在这点的切线方向就是主曲率方向。这两个方向是垂直的。</li>
</ul>
</li>
</ul>
<ol>
<li>获取特征点处的Hessian矩阵，主曲率通过一个2x2的Hessian矩阵H求出：<script type="math/tex; mode=display">
H=\left[\begin{array}{ll}
D_{x x} & D_{x y} \\
D_{xy} & D_{yy}
\end{array}\right]</script></li>
<li>H的特征值$\alpha$和$\beta$代表了x方向和y方向的梯度。<ol>
<li>先求出H的对角线元素之和以及H的行列式<script type="math/tex; mode=display">
\begin{array}{l}
\operatorname{Tr}(H)=D_{x x}+D_{x}=\alpha+\beta \\
\operatorname{Det}(H)=D_{x x} D_{y}-\left(D_{x}\right)^{2}=\alpha \beta
\end{array}</script></li>
<li>设$\alpha$较大，令$\alpha=r \beta$，则$<br>\frac{T r(H)^{2}}{D e t(H)}=\frac{(\alpha+\beta)^{2}}{\alpha \beta}=\frac{(r \beta+\beta)^{2}}{r \beta^{2}}=\frac{(r+1)^{2}}{r}<br>$</li>
<li>r值越大，说明两个特征值的比值越大，即在某一个方向的梯度值越大，而在另一个方向的梯度值越小，而边缘恰恰就是这种情况。所以为了剔除边缘响应点，需要让r小于一定的阈值，因此，为了检测主曲率是否在某域值r下，只需检测$\frac{\operatorname{Tr}(H)^{2}}{\operatorname{Det}(H)}&lt;\frac{(r+1)^{2}}{r}$，若此式子成立，保留关键点，反之剔除。<ol>
<li>Lowe取r=10.</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><ul>
<li>以上的关键点定位过程中用到了离散的导数，具体求导时应用了<a href="https://blog.csdn.net/qq_41679006/article/details/80975436">有限差分法</a>求导</li>
<li>以上过程中需要用到三阶矩阵求逆，可逆矩阵A及其逆如下<script type="math/tex; mode=display">
A=\left(\begin{array}{lll}
a_{00} & a_{01} & a_{02} \\
a_{10} & a_{11} & a_{12} \\
a_{20} & a_{21} & a_{22}
\end{array}\right)</script><script type="math/tex; mode=display">
A^{-1}=\frac{1}{|A|}\left(\begin{array}{ccc}
a_{11} a_{22}-a_{21} a_{12} & -\left(a_{01} a_{22}-a_{21} a_{02}\right) & a_{01} a_{12}-a_{02} a_{11} \\
a_{12} a_{20}-a_{22} a_{10} & -\left(a_{02} a_{20}-a_{22} a_{00}\right) & a_{02} a_{10}-a_{00} a_{12} \\
a_{10} a_{21}-a_{20} a_{11} & -\left(a_{00} a_{21}-a_{20} a_{01}\right) & a_{00} a_{11}-a_{01} a_{10}
\end{array}\right)</script></li>
</ul>
<h2 id="关键点方向匹配"><a href="#关键点方向匹配" class="headerlink" title="关键点方向匹配"></a>关键点方向匹配</h2><ul>
<li>为了使最后所得的描述符具有旋转不变性，需要利用图像的局部特征为给每一个关键点分配一个基准方向。使用图像梯度的方法求取局部结构的稳定方向。</li>
</ul>
<ol>
<li>对于在DOG金字塔中检测出的关键点点，采集其所在高斯金字塔图像3σ邻域窗口内像素的梯度和方向分布特征。梯度的模值和方向如下：<script type="math/tex; mode=display">
\begin{array}{l}
m(x, y)=\sqrt{(L(x+1, y)-L(x-1, y))^{2}+(L(x, y+1)-L(x, y-1))^{2}} \\
\left.\theta(x, y)=\tan ^{-1}((L(x, y+1)-L(x, y-1)) / L(x+1, y)-L(x-1, y))\right)
\end{array}</script>L为关键点所在的尺度空间值，按Lowe的建议，梯度的模值m(x,y)按$1.5\sigma(o,s)$的尺度进行高斯加权，按照常用的$3\sigma$原则，取邻域窗口的半径为$3 \times 1.5 \sigma(o,s)$</li>
<li>求得关键点及其邻域内像素的梯度和方向，并使用直方图进行统计。梯度直方图将0~360度的方向范围分为36个柱(bins)，其中每柱10度。<img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330205657098-53794026.png" alt="如图"></li>
<li>方向直方图的峰值则代表了该特征点处邻域梯度的方向，以直方图中最大值作为该关键点的主方向。为了增强匹配的鲁棒性，只保留峰值大于主方向峰值80％的方向作为该关键点的辅方向。因此，<strong>对于同一梯度值的多个峰值的关键点位置，在相同位置和尺度将会有多个关键点被创建但方向不同</strong>。仅有15％的关键点被赋予多个方向，但可以明显的提高关键点匹配的稳定性。实际编程实现中，就是把该关键点复制成多份关键点，并将方向值分别赋给这些复制后的关键点，并且，离散的梯度方向直方图要进行插值拟合处理，来求得更精确的方向角度值。<ol>
<li>梯度直方图的平滑处理：为了避免梯度方向受噪声的影响，还可以对梯度直方图进行平滑以及进行抛物线插值处理。<a href="https://www.cnblogs.com/Alliswell-WP/p/SIFT.html">具体方法</a>。</li>
</ol>
</li>
</ol>
<h2 id="特征描述子计算"><a href="#特征描述子计算" class="headerlink" title="特征描述子计算"></a>特征描述子计算</h2><ul>
<li>作用：<strong>表示关键点邻域高斯图像梯度统计结果</strong>。</li>
<li>方法：通过对关键点周围图像区域分块，计算块内梯度直方图，生成具有独特性的向量，这个向量是该区域图像信息的一种抽象，具有唯一性&amp;独特性。<ul>
<li>Lowe建议描述子使用在关键点尺度空间内4*4的窗口中计算的8个方向的梯度信息，共4*4*8=128维向量表征。</li>
</ul>
</li>
</ul>
<h3 id="确定计算描述子所需的图像区域"><a href="#确定计算描述子所需的图像区域" class="headerlink" title="确定计算描述子所需的图像区域"></a>确定计算描述子所需的图像区域</h3><ol>
<li>将关键点附近的邻域划分为d*d(Lowe建议d=4)个子区域，每个子区域做为一个种子点，每个种子点有8个方向。</li>
<li>每个子区域的大小，也使用$3\sigma$原则确定，即子区域边长为$3\sigma$。</li>
<li>则所需图像区域边长为$3\sigma\times(d+1)$</li>
<li>考虑到旋转因素(方便下一步将坐标轴旋转到关键点的方向)，用圆代替矩阵，实际计算所需的图像区域半径为：<br>$radius=\frac{3\sigma\times\sqrt{2}\times(d+1)}{2}$。计算结果四舍五入取整。<img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330210025431-395170345.jpg" alt=""></li>
</ol>
<h3 id="坐标轴旋转至主方向"><a href="#坐标轴旋转至主方向" class="headerlink" title="坐标轴旋转至主方向"></a>坐标轴旋转至主方向</h3><ul>
<li>坐标轴旋转<br><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330210107953-2078955000.png" alt="坐标轴旋转至主方向"></li>
<li>旋转后邻域内采样点的新坐标：<script type="math/tex; mode=display">
\left(\begin{array}{l}
x^{\prime} \\
y^{\prime}
\end{array}\right)=\left(\begin{array}{ll}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right)\left(\begin{array}{l}
x \\
y
\end{array}\right)(x, y \in[-\text {radius, radius}]</script></li>
</ul>
<h3 id="梯度直方图生成"><a href="#梯度直方图生成" class="headerlink" title="梯度直方图生成"></a>梯度直方图生成</h3><ul>
<li>如下图绿色部分的坐标系中，旋转后采样点$(x^{\prime}, y^{\prime})$的新坐标为<script type="math/tex; mode=display">
\left(\begin{array}{l}
x^{n} \\
y^{n}
\end{array}\right)=\frac{1}{3 \sigma_{-} o c t}\left(\begin{array}{l}
x^{\prime} \\
y^{t}
\end{array}\right)+\frac{d}{2}</script></li>
<li>Lowe建议子区域的像素的梯度大小按$\sigma=0.5d$进行高斯加权，(a, b)为关键点在DoG图像中的位置坐标，则<script type="math/tex; mode=display">
w=m(a+x, b+y)^{*} e^{-\frac{\left(x^{\prime}\right)^{2}+\left(y^{\prime}\right)^{2}}{2 \times(0.5 d)^{2}}}</script><img src="https://niecongchong.github.io/img/2019-08-06-24.jpg" alt=""></li>
<li>与求主方向不同，此时每个种子区域的梯度直方图在0-360之间划分为8个方向区间，每个区间为45度，即每个种子点有8个方向的梯度强度信息。所以共4*4*8=128个梯度。</li>
</ul>
<h3 id="三线性插值"><a href="#三线性插值" class="headerlink" title="三线性插值"></a>三线性插值</h3><ul>
<li>三线性：x, y, 方向<br><img src="https://img2018.cnblogs.com/blog/1471528/201903/1471528-20190330210313666-1453481217.png" alt=""></li>
<li>如图中的红色点，落在第0行和第1行之间，对这两行都有贡献。对第0行第3列种子点的贡献因子为dr，对第1行第3列的贡献因子为1-dr，同理，对邻近两列的贡献因子为dc和1-dc，对邻近两个方向的贡献因子为do和1-do。k,m,n为0或1，则最终累加在每个方向上的梯度大小为：<script type="math/tex; mode=display">
\text { weight }=w^{*} d r^{k} *(1-d r)^{1-k} * d c^{m *}(1-d c)^{1-m *} d o^{n *}(1-d o)^{1-n}</script></li>
</ul>
<h3 id="特征描述子以及归一化特征描述子"><a href="#特征描述子以及归一化特征描述子" class="headerlink" title="特征描述子以及归一化特征描述子"></a>特征描述子以及归一化特征描述子</h3><ul>
<li>如上统计的4*4*8=128个梯度信息即为该关键点的特征向量。特征向量形成后，为了去除光照变化的影响，需要对它们进行归一化处理，对于图像灰度值整体漂移，图像各点的梯度是邻域像素相减得到，所以也能去除。</li>
<li>得到的描述子向量为$H=\left(h<em>{1}, h</em>{2}, \ldots, h_{128}\right)$</li>
<li>归一化后的描述子向量为$L=\left(l<em>{1}, l</em>{2}, \ldots, l_{128}\right)$</li>
</ul>
<h3 id="特征描述子门限化"><a href="#特征描述子门限化" class="headerlink" title="特征描述子门限化"></a>特征描述子门限化</h3><ul>
<li>非线性光照，相机饱和度变化对造成某些方向的梯度值过大，而对方向的影响微弱。因此设置门限值(向量归一化后，一般取0.2)截断较大的梯度值。然后，再进行一次归一化处理，提高特征的鉴别性。</li>
</ul>
<h3 id="特征描述向量排序"><a href="#特征描述向量排序" class="headerlink" title="特征描述向量排序"></a>特征描述向量排序</h3><ul>
<li>按特征点的尺度对特征描述向量进行排序。</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://www.cnblogs.com/Alliswell-WP/p/SIFT.html">https://www.cnblogs.com/Alliswell-WP/p/SIFT.html</a></li>
<li><a href="https://blog.csdn.net/zddblog/article/details/7521424">https://blog.csdn.net/zddblog/article/details/7521424</a></li>
<li><a href="https://blog.csdn.net/qq_41679006/article/details/80975436">https://blog.csdn.net/qq_41679006/article/details/80975436</a></li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
        <category>CV</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>ML</tag>
        <tag>SIFT</tag>
      </tags>
  </entry>
  <entry>
    <title>word2vec_seq2seq_attention_transformer_bert</title>
    <url>/2022/03/17/word2vec-seq2seq-attention-transformer/</url>
    <content><![CDATA[<p>word2vec_seq2seq_attention_transformer_bert<br><a id="more"></a></p>
<h1 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h1><ul>
<li>优点：<ul>
<li>1）由于 Word2vec 会考虑上下文，跟之前的 Embedding 方法相比，效果要更好（但不如 18 年之后的方法）</li>
<li>2）比之前的 Embedding方法维度更少，所以速度更快</li>
<li>3）通用性很强，可以用在各种 NLP 任务中</li>
</ul>
</li>
<li>缺点：<ul>
<li>1）由于词和向量是一对一的关系，所以多义词的问题无法解决。</li>
<li>2）Word2vec 是一种静态的方式，虽然通用性强，但是无法针对特定任务做动态优化</li>
</ul>
</li>
</ul>
<h2 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h2><ul>
<li>高频词：<ul>
<li>1）例如the等词不会提供更多语义</li>
<li>2）训练时用不到那么多样本对</li>
<li>因此以以某种概率删掉一些词，这个概率与词出现的频数有关</li>
</ul>
</li>
</ul>
<h2 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h2><ul>
<li>对于负样本，只选择一部分。因为负样本的label为0，因此每次更新的参数也会较少。回忆一下我们的隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新300*5个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。</li>
<li>单词被选中的概率与其频度有关：<img src="/2022/03/17/word2vec-seq2seq-attention-transformer/word2vec1.png" class="" title="word2vec1">
</li>
</ul>
<h2 id="CBOW和skip-gram"><a href="#CBOW和skip-gram" class="headerlink" title="CBOW和skip-gram"></a>CBOW和skip-gram</h2><ul>
<li>CBOW：周围词预测中心词，每次迭代都在调整周围词，因此其迭代的次数与语料库大小相同，复杂度为O(V)</li>
<li>S-G：中心词预测周围词，每次迭代都在调整中心词，每个中心词需要调整K次，因此其迭代的次数为K*V次，这里K为窗口大小，复杂度为O(KV)</li>
<li>即CBOW快于S-G；而SG更适用于生僻词以及语料库较小的情况的训练，因为生僻词出现的次数少，SG能够对其进行较多次的训练<blockquote>
<p>在cbow方法中，是用周围词预测中心词，从而利用中心词的预测结果情况，使用GradientDesent方法，不断的去调整周围词的向量。当训练完成之后，每个词都会作为中心词，把周围词的词向量进行了调整，这样也就获得了整个文本里面所有词的词向量。要注意的是， cbow的对周围词的调整是统一的：求出的gradient的值会同样的作用到每个周围词的词向量当中去。可以看到，cbow预测行为的次数跟整个文本的词数几乎是相等的（每次预测行为才会进行一次backpropgation, 而往往这也是最耗时的部分），复杂度大概是O(V).<br>而skip-gram是用中心词来预测周围的词。在skip-gram中，会利用周围的词的预测结果情况，使用GradientDecent来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。<br>可以看出，skip-gram进行预测的次数是要多于cbow的：因为每个词在作为中心词时，都要使用周围词进行预测一次。这样相当于比cbow的方法多进行了K次（假设K为窗口大小），因此时间的复杂度为O(KV)，训练时间要比cbow要长。<br>但是在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此，当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。因为尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。<br>从更通俗的角度来说：在skip-gram里面，每个词在作为中心词的时候，实际上是 1个学生 VS K个老师，K个老师（周围词）都会对学生（中心词）进行“专业”的训练，这样学生（中心词）的“能力”（向量结果）相对就会扎实（准确）一些，但是这样肯定会使用更长的时间；cbow是 1个老师 VS K个学生，K个学生（周围词）都会从老师（中心词）那里学习知识，但是老师（中心词）是一视同仁的，教给大家的一样的知识。至于你学到了多少，还要看下一轮（假如还在窗口内），或者以后的某一轮，你还有机会加入老师的课堂当中（再次出现作为周围词），跟着大家一起学习，然后进步一点。因此相对skip-gram，你的业务能力肯定没有人家强，但是对于整个训练营（训练过程）来说，这样肯定效率高，速度更快。</p>
</blockquote>
</li>
</ul>
<h1 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h1><h2 id="encoder-decoder模型"><a href="#encoder-decoder模型" class="headerlink" title="encoder-decoder模型"></a>encoder-decoder模型</h2><img src="/2022/03/17/word2vec-seq2seq-attention-transformer/encoder_decoder.png" class="" title="encoder_decoder">
<ul>
<li>其中，encoder和decoder的具体架构可选，CNN/RNN/LSTM/GRU…</li>
<li>C由encoder中所有时间的隐藏状态决定，设共Tx个状态，则C = q(h1,h2,…hTx)</li>
<li>decoder为一个语言模型，则 <script type="math/tex">y_t = \mathop{argmax} P(y_t)=\prod_{t=1}^Tp(y_t|\{y_1,\ldots,y_{t-1}\},C)</script></li>
<li><strong>注意，这个C是不变的，即对于decoder中的序列，输入的C是相同的</strong></li>
<li>局限：encoder和decoder的联系只依靠一个固定长度的语义向量c<ul>
<li><strong>语义向量可能无法完全表达encoder中整个序列的信息，且输入序列越长，此现象越严重；</strong></li>
<li><strong>encoder中序列先输入的内容会被后输入的内容稀释掉</strong></li>
</ul>
</li>
</ul>
<h2 id="seq2seq-1"><a href="#seq2seq-1" class="headerlink" title="seq2seq"></a>seq2seq</h2><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><ul>
<li>第一种结构：将c当做decoder每一时刻的输入，如上encoder-decoder</li>
<li>第二种结构：c只作为decoder初始时刻的输入</li>
</ul>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ul>
<li>测试时，decoder中前序输出输入到下一个时刻</li>
<li>训练时，decoder前序输出不作为下一时刻的输入，而使用真实序列作为输入</li>
<li>端对端训练：<ul>
<li>两个分模型分别训练得到最优后合在一起不一定能获得全局最优</li>
</ul>
</li>
</ul>
<h3 id="decoding"><a href="#decoding" class="headerlink" title="decoding"></a>decoding</h3><ul>
<li>本质是求条件概率</li>
<li>greedy decoding：decoder中每个序列会接一层softmax，输出语料库中每个单词的概率，测试时，每次取概率最大的单词作为下一序列的输入，称为<strong>greedy decoding</strong>。<ul>
<li>greedy的问题：<strong>无法回退</strong>，预测出错之后会一直错下去。</li>
</ul>
</li>
<li>Exhaustive search decoding: 每一步都计算，设语料库大小为V，则第T时刻的时间复杂度为O(V^T)</li>
<li>Beam Search decoding: <ul>
<li>每次track k个最有可能的值。</li>
<li>如果遇到结束的序列，先保存，等所有序列结束后，比较其条件概率除以序列长度的最大值（为了避免倾向于选择较短的序列，除以序列长度）。</li>
<li>终止条件：序列长度阈值、可选结果个数阈值。</li>
</ul>
</li>
</ul>
<h3 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h3><ul>
<li>encoder和decoder的联系只依靠一个固定长度的语义向量c：<ul>
<li>语义向量可能无法完全表达encoder中整个序列的信息，且输入序列越长，此现象越严重；</li>
<li>encoder中序列先输入的内容会被后输入的内容稀释掉</li>
</ul>
</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><h4 id="应用seq2seq做NMT-Neural-Machine-Translation"><a href="#应用seq2seq做NMT-Neural-Machine-Translation" class="headerlink" title="应用seq2seq做NMT(Neural Machine Translation)"></a>应用seq2seq做NMT(Neural Machine Translation)</h4><ul>
<li>优点：<ul>
<li>better performance：更流利；应用了更多的上下文信息；对短语相似性利用的更好</li>
<li>端对端训练</li>
<li>更少的人工</li>
</ul>
</li>
<li>劣势：<ul>
<li>解释性</li>
<li>更难debug</li>
<li>更难控制（比起SMT（可以直接用规则））</li>
<li>安全性</li>
</ul>
</li>
<li>遗留问题：<ul>
<li>out-of-vocabulary words：语料库里没有的单词无法翻译</li>
<li>domain dismatch：用维基百科训练的模型对Facebook不一定好用</li>
</ul>
</li>
</ul>
<h1 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h1><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><ul>
<li>1）语义向量C有语义表达的瓶颈，attention可以使decoder的每个时刻与encoder相连；</li>
<li>2）attention可以使得decoder的每一步只关注encoder的特定部分。</li>
</ul>
<h2 id="本质思想"><a href="#本质思想" class="headerlink" title="本质思想"></a>本质思想</h2><ul>
<li>target中给定一个query，计算该query与source中每个key之间的相似度，即sim(Query, Key i)；然后对每个sim值进行softmax；使用softmax值来加权平均source中每个key对应的value，并得到attention score：<script type="math/tex">\text { Attention(Query, Source) }=\sum_{i=1}^{L_{x}} \text { Similarity }\left(\text { Query }, \text { Key }_{i}\right) * \text { Value }_{i}</script></li>
</ul>
<img src="/2022/03/17/word2vec-seq2seq-attention-transformer/attention1.jpeg" class="" title="attention1">
<h2 id="method"><a href="#method" class="headerlink" title="method"></a>method</h2><ul>
<li>核心：<strong>==个性化encoder中每个时刻对decoder中每个时刻的影响，并且直接连接到encoder==</strong>：<img src="/2022/03/17/word2vec-seq2seq-attention-transformer/attention2.jpeg" class="" title="attention2">
<ul>
<li>参考文章<a href="https://zhuanlan.zhihu.com/p/61816483">attention</a></li>
</ul>
</li>
<li>decoder中每个时间点语义编码对其贡献是一样的Y1=g(C,h(0)),Y2=g(C,Y1)…</li>
<li>attention中，decoder不再将整个序列编码成固定长度的语义向量C，而是根据生成的新单词计算新的Ci，使得不同时刻输入不同的C，这样就解决了单词信息丢失的问题：<script type="math/tex">p(y_i|y_1,\ldots,y_{i-1},X)=g(y_{i-1},s_i,c_i) 其中，s_i=f(s_{i-1},y_{i-1},c_i)</script><ul>
<li>即decoder中每个时刻的结果是上个时刻输出、该时刻状态、该时刻语义向量C的函数，而此时刻的状态为上个时刻输出、上个时刻状态以及Ci的函数。</li>
</ul>
</li>
<li>attention，Ci的计算为：<script type="math/tex; mode=display">
\begin{array}{c}
c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j} \\
\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T_{x}} \exp \left(e_{i k}\right)} \\
e_{i j}=a\left(s_{i-1}, h_{j}\right)
\end{array}</script></li>
<li>即对于decoder的每个状态Si，Ci为encoder中每个状态的hi的加权平均，其中权重为decoder中si-1与encoder中的每个隐藏状态决定（softmax）</li>
<li>整体：<br>$e<em>{i j}=a\left(s</em>{i-1}, h_{j}\right)$ 这里 $a$ 可以是内积, 即 $S i-1$ 和 $h j$ 的内积<script type="math/tex; mode=display">
\begin{array}{c}
\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T x} \exp \left(e_{i k}\right)} \\
c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j} \\
s_{i}=f\left(s_{i-1}, y_{i-1}, c_{i}\right) \\
y_{i}=g\left(y_{i-1}, s_{i}, c_{i}\right)
\end{array}</script></li>
<li>原始attention论文中，encoder使用了BiRNN，因此可以获取到某个输入上下文的信息</li>
</ul>
<h3 id="attention-score的多种计算方式"><a href="#attention-score的多种计算方式" class="headerlink" title="attention score的多种计算方式"></a>attention score的多种计算方式</h3><ul>
<li>参考文章<a href="https://zhuanlan.zhihu.com/p/61816483">attention</a></li>
<li>本质是计算query和source中每个key的相似度或相关度：<ul>
<li>点积</li>
<li>cosine相似性</li>
<li>MLP网络<img src="/2022/03/17/word2vec-seq2seq-attention-transformer/attention3.jpeg" class="" title="attention3">
</li>
</ul>
</li>
</ul>
<h3 id="self-Attention-intra-Attention"><a href="#self-Attention-intra-Attention" class="headerlink" title="self Attention/ intra Attention"></a>self Attention/ intra Attention</h3><ul>
<li>Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。</li>
<li>例如可以捕捉同一个句子中的句法特征和语义特征</li>
<li>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</li>
<li>此外，<strong>可以增加计算并行化</strong>，原因是对于self-attention来说，不需要等待序列生成。</li>
</ul>
<h2 id="attention的好处"><a href="#attention的好处" class="headerlink" title="attention的好处"></a>attention的好处</h2><ul>
<li>解决了bottleneck问题<ul>
<li>允许decoder直接获取到source的信息，而不是经过一个bottleneck</li>
<li>个性化encoder中每个时刻的状态对当前decoder状态的影响</li>
</ul>
</li>
<li>类似于VGG中的残差，有助于解决vanish gradient</li>
<li>增强了解释性</li>
<li>差不多算是一种软对齐</li>
<li>并行：<a href="https://www.jianshu.com/p/6b698bb6a486">https://www.jianshu.com/p/6b698bb6a486</a><ul>
<li>encoder：层间无法并行，层内可以同时feed所有输入向量，同时更新梯度，实现并行。而RNN只能并行实现所有时刻的输入，而不能实现梯度的并行（梯度需要从n层到n-1层）。</li>
<li>decoder：第n个输出依赖第n-1个输出，因此不可以并行。</li>
</ul>
</li>
</ul>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><ul>
<li>Paper: <a href="https://arxiv.org/pdf/1706.03762.pdf">attention is all you need</a></li>
<li>初衷：解决传统RNN等序列模型中无法并行化的问题；CNN可以并行但无法用于变长序列</li>
</ul>
<h2 id="Encoder-and-decoder-stacks"><a href="#Encoder-and-decoder-stacks" class="headerlink" title="Encoder and decoder stacks"></a>Encoder and decoder stacks</h2><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><img src="/2022/03/17/word2vec-seq2seq-attention-transformer/transformer1.jpeg" class="" title="transformer1">
<ul>
<li>简化为两层的Encoder Decoder示意图：<img src="/2022/03/17/word2vec-seq2seq-attention-transformer/transformer2.jpeg" class="" title="transformer2"></li>
<li>注意是Encoder的输出连到decoder中的encoder-decoder attention层，即第六层的encoder输出的每个序列加权求attention的source</li>
</ul>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul>
<li>由6层组成，<strong>每层由2个子层组成，每个子层都使用了residual（残差）连接以及加入了正则化层，即每个子层的输出为LayerNorm(x+Sublayer(x))，x就是这个层的输出。为了确保连接，所有sub-layer和embedding layer的输出维度相同。</strong><ul>
<li><strong>第一个子层为multi-head self-attention mechanism</strong></li>
<li><strong>第二个子层为全连接前馈网络。</strong></li>
</ul>
</li>
</ul>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul>
<li>6层组成。<strong>每层三个子层，除与Encoder相同的两层外，加入了一个masked multi-head self-attention。同样适用残差操作。</strong></li>
</ul>
<h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><ul>
<li><strong>是在Batch size方向的正则化，即对于一个向量（样本）的每个位置，求其在Batch Size上的均值和方差，并做正则化。</strong></li>
</ul>
<h4 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h4><ul>
<li><strong>在每个样本上计算均值和方差</strong></li>
</ul>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><ul>
<li>query Q和keys K的维度为dk，values V的维度为dv（意思是一个query或一个key，即一个时刻的key的维度是dv）。设source有s个时刻，target有t个时刻。有：Q—t*dk, K—s*dk, V—s*dv</li>
<li>指定了一种方法来计算query Q和key K之间的相似性：<script type="math/tex">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script>. 这里比平常的点积相似度多除一个{\sqrt{d_{k}}，dk是K的维度，原因为：dk很大的时候，点积得到的相似度值非常大，因为每个相似度都是dk个乘机的和，此时相似度之间差异会<strong>相对</strong>比较小，即相似度的梯度区域会比较小。</li>
</ul>
<h4 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h4><p>具体计算过程很繁琐，参照<a href="https://zhuanlan.zhihu.com/p/63191028">文中的图解</a></p>
<ul>
<li>多头作用：获取不同子空间的信息；增加并行化</li>
<li>单头<ul>
<li>embedding为dmodel=512维度，对其做线性映射以Q为例：Q * W = t <em> dmodel </em> dmodel <em> dk = t </em> dk，即每个query从dmodel到了dk=64维度，即新的Q为dk=64维</li>
<li>Q * K^T = t * dk* dk <em> s = t </em> s</li>
<li>softmax(Q * K^T / sqrt(dk)) = t * s，即以行做softmax</li>
<li>softmax * V = t * s * s * dv = t * dv</li>
</ul>
</li>
<li>多头self-attention：做多个单头，每个单头的映射矩阵随机初始化，然后得到h个attention得分，transforms中用了h=8个头，即8个t * dv 的attention得分向量</li>
<li>多头self-attention输入前馈神经网络：先把8个矩阵按列拼接： t * (h dv)；再随机初始化一个(h dv) * dmodel的矩阵，最后结果为t * dmodel，最终得到dmodel是使得其跟原始的输入同纬度，方便相加操作（残差中要用到）</li>
</ul>
<script type="math/tex; mode=display">
MultiHead(Q, K, V) = Concat \left(\right. head_{1}, \ldots, head \left._{\mathrm{h}}\right) W^{O} \\
\text{where } head_{\mathrm{i}}= Attention \left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \\
\text{Where the projections are parameter matrices } W_{i}^{Q} \in \mathbb{R}^{d_{\text {bouth }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text {math }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text {matal }} \times d_{v}} and W^{O} \in \mathbb{R}^{h d v \times d_{\text {matal }}}</script><h4 id="Attention在transform中的应用"><a href="#Attention在transform中的应用" class="headerlink" title="Attention在transform中的应用"></a>Attention在transform中的应用</h4><ul>
<li>有三种不同的应用：<ul>
<li>在encoder-decoder attention层，query来自于前序decoder层，key和value来自于encoder的输出。This allows every position in the decoder to attend over all positions in the input sequence。<strong>是典型的seq2seq中的attention</strong>。</li>
<li>encoder中的self-attention层，key/value/query都来自于之前层（大层，不是子层，也不是attention中的序列）的输出，encoder中每个位置可以attend to之前层的输出<br><img src="https://pic1.zhimg.com/80/v2-ac8272e1505602d3f56334fd49d08ab8_hd.jpg" alt="image"></li>
<li>decoder中的self-attention层，用了mask来掩盖某些值，使其在参数更新中不产生效果。这里self attention在应用的时候<ul>
<li>Padding Mask: <strong>每个batch输入的序列长度不同，因此要对收入序列对齐。较短序列在后面填充0，较长序列只截取左边内容。填充0时，会把这些位置的值加上负无穷，经过softmax之后这些地方的概率会接近于0(e的负无穷大次方为0).</strong></li>
<li>Sequence Mask: <strong>使得decoder不能看见未来的信息。做法为：使用一个上三角全为0的矩阵</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><ul>
<li>两层：两个线性变换和一个ReLU激活输出: <script type="math/tex">FFN(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script></li>
</ul>
<h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><h3 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h3><ul>
<li>模型中没有使用循环神经网络或者卷积，所以位置信息需要被编码Position Encoding，该向量和embedding的维度一致，并把该向量与embedding相加输入到下一层。</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{c}{P E_{(p o s, 2 i)}=\sin \left(\text {pos} / 10000^{2 i / d_{\text {madel }}}\right)} \\ {P E_{(\text {pos}, 2 i+1)}=\cos \left(\text {pos} / 10000^{2 i / d_{\text {madal }}}\right)}\end{array}</script><ul>
<li>其中pos是位置，i是向量中每个值的index。即在偶数位置使用正弦编码；奇数位置，使用余弦编码。也就是说对于向量的同一个index，每个pos处的向量被编码后是一个sin函数，该函数的波长为2π~10000*2π。选择这个函数的目的是因为这个函数能学习到相对位置，即pos+k可以由pos的线性函数得到，这些相对位置的信息会使用在self-attention中。</li>
</ul>
<h3 id="为何使用self-attention"><a href="#为何使用self-attention" class="headerlink" title="为何使用self-attention"></a>为何使用self-attention</h3><ul>
<li>简单来说，我们需要一个对原序列进行编码以提取信息，在进行编码的过程中需要考虑：<ul>
<li>每层的计算复杂度。</li>
<li>最小序列操作，即并行化的程度。</li>
<li>网络中远程依赖关系之间的路径长度（path length between long-range dependencies in the network）。路径越短越能学习到长距离的依赖关系。</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Layer Type</th>
<th>Complexity per Layer</th>
<th>Sequential Operations</th>
<th>Maximum Path Length</th>
</tr>
</thead>
<tbody>
<tr>
<td>Self-Attention</td>
<td>O(n^2*d)</td>
<td>O(1)</td>
<td>O(1)</td>
</tr>
<tr>
<td>Recurrent</td>
<td>O(n*d^2)</td>
<td>O(n)</td>
<td>O(n)</td>
</tr>
<tr>
<td>Convolutional</td>
<td>O(k*n*d^2)</td>
<td>O(1)</td>
<td>O(logk(n))</td>
</tr>
<tr>
<td>Self-Attention(restricted)</td>
<td>O(r*n*d)</td>
<td>O(n/r)</td>
<td>O(1)</td>
</tr>
</tbody>
</table>
</div>
<p>n: 序列长度，d: 表达维度，k: 卷积核size，r: Self-Attention(restricted)中的领域size</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul>
<li>Residual Dropout</li>
<li>Attention Dropout: embeddings + positional encodings</li>
<li>Label Smoothing</li>
</ul>
<h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><ul>
<li>beam search</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>attention代替了传统的RNN层。<ul>
<li><strong>RNN每层的信息会随着序列的增加而衰减，因此难以获得较长序列跨度的信息；attention层对其他词的信息不取决于距离而取决于相关性/表示的相似度</strong></li>
<li><strong>Transform可以获得双向信息</strong></li>
<li><strong>并行化</strong></li>
</ul>
</li>
<li><strong>attention无需参数，只使用超参数，也即无监督，使得模型很简洁</strong></li>
<li>self-attention：使用dot-product的相似性计算方式，实际上是一种无监督求解相似性的方式。<strong>并行化非常好</strong>。</li>
<li>为何点积可以求得相似性：点积可以用来求解两向量的夹角。</li>
<li><em>decoder第一层的输入是什么</em></li>
<li><em>decoder中的attention层的key和query</em></li>
<li><em>mask具体</em></li>
<li><em>为何不用BN用LN，LN的作用</em>是：<strong>BN使得输入到下一层的样本分布为正态分布，也就是特征的每个维度是正则化的，以改善训练，加速收敛（独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力）；LN使得每个样本的不同index处的分布为正态分布</strong></li>
</ul>
<h2 id="Transformer-VS-CNN"><a href="#Transformer-VS-CNN" class="headerlink" title="Transformer VS CNN"></a>Transformer VS CNN</h2><ul>
<li><strong>CNN可以并行化输出序列</strong></li>
<li><strong>CNN的感受野有限，不能感知所有上下文</strong></li>
<li><strong>CNN每层的卷积权重共享，而Attention每一个decoder对应的attention是不一样的</strong></li>
</ul>
<h1 id="Bert-Bidirectional-Encoder-Representations-from-Transformers"><a href="#Bert-Bidirectional-Encoder-Representations-from-Transformers" class="headerlink" title="Bert(Bidirectional Encoder Representations from Transformers)"></a>Bert(Bidirectional Encoder Representations from Transformers)</h1><h2 id="两个任务"><a href="#两个任务" class="headerlink" title="两个任务"></a>两个任务</h2><ul>
<li>Masked Language Model(MLM)</li>
<li>Next Sentence Prediction</li>
</ul>
<h2 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h2><ul>
<li>L: transformer blocks; H: hidden size; A: self-attention heads;</li>
<li>Bert-base: L=12, H=768, A=12, total-Params=110M</li>
<li>Bert-large: L=24, H=1024, A=16, total-Params=340M</li>
<li>输入输出表达：<ul>
<li>sentence：一段连续文本，并不是语言学中的句子</li>
<li>sequence：BERT的输入tocken sequence，1 sentence或2 sentences(2 sentences用于后续的NSP)</li>
<li>使用tocken个数为30000的WordPiece embeddings，每个<strong>sequence</strong>的第一个tocken为[CLS]</li>
<li>不同sentence区分的方式：1. 通过[SEP]分隔；2. 学习一个embeddings，不同的sentence用不同的embedding来区分</li>
<li>一个给定的tocken，其input representation是相应的tocken、segment和position embeddings的和<img src="/2022/03/17/word2vec-seq2seq-attention-transformer/bert1.png" class="" title="bert1">
</li>
</ul>
</li>
</ul>
<h2 id="Pre-training-BERT"><a href="#Pre-training-BERT" class="headerlink" title="Pre-training BERT"></a>Pre-training BERT</h2><ul>
<li>两种无监督训练任务</li>
</ul>
<h3 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a>Masked LM</h3><ul>
<li>在每个<strong>sequence</strong>中随机mask 15%的wordpiece tockens</li>
<li>缺点：[MASK] tocken不会出现在fine-tuning中，所以会造成pre-training和fine-tuning的mismatch。解决：not always replace “masked” words with the actual [MASK] token：<ul>
<li>80%的时候用[MASK] tocken</li>
<li>10%的random tocken</li>
<li>10%的unchanged token </li>
<li>Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示。此外，因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。</li>
</ul>
</li>
</ul>
<h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p><strong>LM无法获取两句之间的相关性</strong></p>
<ul>
<li>label：每个样本对AB，如果50%的时间B是A的下一句，则为IsNext，否则NotNext</li>
</ul>
<h3 id="Pre-training-data"><a href="#Pre-training-data" class="headerlink" title="Pre-training data"></a>Pre-training data</h3><p>BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)<br><strong>使用文件级的语料库而不是语句级的，这样次啊能够提取到长的连续语句</strong></p>
<h1 id="word2vec-vs-glove-vs-elmo-vs-gpt-vs-bert"><a href="#word2vec-vs-glove-vs-elmo-vs-gpt-vs-bert" class="headerlink" title="word2vec vs glove vs elmo vs gpt vs bert"></a>word2vec vs glove vs elmo vs gpt vs bert</h1><ul>
<li>word2vec, Glove只与预训练预料有关</li>
<li>ELMo<ul>
<li>属于Context word embedding</li>
<li>ELMo不是对每个单词使用固定嵌入，而是在为其中的每个单词分配嵌入之前查看整个句子，它使用在特定任务上训练的双向LSTM来创建这些嵌入</li>
<li>训练时，采用语言模型的方法</li>
</ul>
</li>
<li>OpenAI Transformer<ul>
<li>使用无attention层的decoder</li>
<li>只使用了单向信息</li>
<li>pre-train和fine-tuning不匹配，例如对于多句输入的情况</li>
</ul>
</li>
<li>BERT<ul>
<li>使用了encoder</li>
<li>Masked LM解决了只使用单向信息的问题</li>
<li>使用NSP来解决pre-train和fine-tuning不匹配的问题</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Word2vec</tag>
        <tag>Seq2seq</tag>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>Bert</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习面试题-RF&amp;GBDT&amp;XGBoost&amp;LightGBM</title>
    <url>/2021/12/10/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91%E2%80%94%E2%80%94XGBoost%20GBDT%20lightgbm/</url>
    <content><![CDATA[<p><meta name="referrer" content="no-referrer"/><br>[TOC]</p>
<h2 id="1-RF和GBDT的区别"><a href="#1-RF和GBDT的区别" class="headerlink" title="1. RF和GBDT的区别"></a>1. RF和GBDT的区别</h2><p><strong>相同点：</strong></p>
<ul>
<li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li>
</ul>
<p><strong>不同点：</strong></p>
<ul>
<li><strong>集成学习</strong>：$RF$属于$Bagging$思想，而$GBDT$是$Boosting$思想</li>
<li><strong>偏差-方差权衡</strong>：$RF$不断的降低模型的方差，而$GBDT$不断的降低模型的偏差</li>
<li><strong>训练样本</strong>：$RF$每次迭代的样本是从全部训练集中有放回抽样形成的，而$GBDT$每次使用全部样本</li>
<li><strong>并行性</strong>：$RF$的树可以并行生成，而$GBDT$只能顺序生成(需要等上一棵树完全生成)</li>
<li><strong>最终结果</strong>：$RF$最终是多棵树进行多数表决（回归问题是取平均），而$GBDT$是加权融合</li>
<li><strong>数据敏感性</strong>：$RF$对异常值不敏感，而$GBDT$对异常值比较敏感</li>
<li><strong>泛化能力</strong>：$RF$不易过拟合，而$GBDT$容易过拟合</li>
</ul>
<a id="more"></a>
<h2 id="2-比较LR和GBDT，说说什么情景下GBDT不如LR"><a href="#2-比较LR和GBDT，说说什么情景下GBDT不如LR" class="headerlink" title="2. 比较LR和GBDT，说说什么情景下GBDT不如LR"></a>2. 比较LR和GBDT，说说什么情景下GBDT不如LR</h2><p>先说说$LR$和$GBDT$的区别： </p>
<ul>
<li>$LR$是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程</li>
<li>$GBDT$是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；</li>
</ul>
<p>当在<strong>高维稀疏特征的场景下，$LR$的效果一般会比$GBDT$好</strong>。原因如下：</p>
<p>先看一个例子：</p>
<blockquote>
<p>假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。</p>
</blockquote>
<p>因为现在的模型普遍都会带着正则项，<strong>而 $LR$ 等线性模型的正则项是对权重的惩罚</strong>，也就是 $w_1$一旦过大，惩罚就会很大，进一步压缩 $w_1$的值，使他不至于过大。但是，树模型则不一样，<strong>树模型的惩罚项通常为叶子节点数和深度</strong>等，而我们都知道，对于上面这种<code>case</code>，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。</p>
<p>这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：<strong>带正则化的线性模型比较不容易对稀疏特征过拟合。</strong></p>
<h2 id="3-简单介绍一下XGBoost​"><a href="#3-简单介绍一下XGBoost​" class="headerlink" title="3. 简单介绍一下XGBoost​"></a>3. 简单介绍一下XGBoost​</h2><p>$XGBoost$是一种集成学习算法，属于3类常用的集成方法($Bagging$，$Boosting$，$Stacking$)中的$Boosting$算法类别。它是一个加法模型，基模型一般选择树模型，但也可以选择其它类型的模型如逻辑回归等。</p>
<p>$XGBoost$对$GBDT$进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行、默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。</p>
<h2 id="4-XGBoost与GBDT有什么不同"><a href="#4-XGBoost与GBDT有什么不同" class="headerlink" title="4. XGBoost与GBDT有什么不同"></a>4. XGBoost与GBDT有什么不同</h2><ul>
<li><strong>基分类器</strong>：$XGBoost$的基分类器不仅支持$CART$决策树，还支持线性分类器，此时$XGBoost$相当于带$L1$和$L2$正则化项的$LR$回归（分类问题）或者线性回归（回归问题）。</li>
<li><strong>导数信息</strong>：$XGBoost$对损失函数做了二阶泰勒展开，可以更为精准的逼近真实的损失函数，$GBDT$只用了一阶导数信息，并且$XGBoost$还支持自定义损失函数，只要损失函数一阶、二阶可导。</li>
<li><strong>正则项</strong>：$XGBoost$的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。</li>
<li><strong>列抽样</strong>：$XGBoost$支持列采样，与随机森林类似，用于防止过拟合。</li>
<li><strong>缺失值处理</strong>：对树中的每个非叶子结点，$XGBoost$可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。</li>
<li><strong>并行化</strong>：注意不是树维度的并行，而是特征维度的并行。$XGBoost$预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</li>
<li><strong>可扩展性</strong>：损失函数支持自定义，只需要新的损失函数二阶可导。</li>
</ul>
<h2 id="5-XGBoost​为什么可以并行训练"><a href="#5-XGBoost​为什么可以并行训练" class="headerlink" title="5. XGBoost​为什么可以并行训练"></a>5. XGBoost​为什么可以并行训练</h2><ul>
<li><strong>不是说每棵树可以并行训练</strong>，$XGBoost$本质上仍然采用$Boosting$思想，每棵树训练前需要等前面的树训练完成才能开始训练。</li>
<li><strong>而是特征维度的并行</strong>：1)训练之前，每个特征按特征值对样本进行预排序，并存储为<code>block</code>结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个<code>block</code>结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个<code>block</code>并行计算。2)也可以进行列的并行。</li>
</ul>
<h2 id="6-XGBoost​为什么快？"><a href="#6-XGBoost​为什么快？" class="headerlink" title="6. XGBoost​为什么快？"></a>6. XGBoost​为什么快？</h2><ul>
<li><strong>分块并行</strong>：训练前每个特征按特征值进行排序并存储为<code>block</code>结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点</li>
<li><strong><code>block</code> 处理优化</strong>：<code>block</code>预先放入内存；<code>block</code>按列进行解压缩；将<code>block</code>划分到不同硬盘来提高吞吐</li>
<li><strong>候选分位点</strong>：每个特征采用常数个分位点作为候选分割点</li>
<li><strong>CPU cache 命中优化</strong>： 使用缓存预取的方法，对每个线程分配一个连续的<code>buffer</code>，读取每个<code>block</code>中样本的梯度信息并存入连续的<code>buffer</code>中。</li>
</ul>
<h2 id="7-XGBoost​中如何处理过拟合的情况？"><a href="#7-XGBoost​中如何处理过拟合的情况？" class="headerlink" title="7. XGBoost​中如何处理过拟合的情况？"></a>7. XGBoost​中如何处理过拟合的情况？</h2><ul>
<li><strong>目标函数中增加了正则项</strong>：使用叶子结点的数目和叶子结点权重的$L2$模的平方，控制树的复杂度。</li>
<li><strong>设置目标函数的增益阈值</strong>：如果分裂后目标函数的增益小于该阈值，则不分裂。</li>
<li><strong>设置最小样本权重和的阈值</strong>：当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。</li>
<li><strong>设置树的最大深度</strong>：$XGBoost$ 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。</li>
</ul>
<ul>
<li>调参： <ul>
<li>第一类参数：用于直接控制模型的复杂度。包括<code>max_depth，min_child_weight，gamma</code> 等参数</li>
<li>第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括<code>subsample，colsample_by树</code></li>
<li>还有就是直接减小<code>learning rate</code>，但需要同时增加<code>estimator</code> 参数。</li>
</ul>
</li>
</ul>
<h2 id="8-XGBoost​如何处理缺失值？"><a href="#8-XGBoost​如何处理缺失值？" class="headerlink" title="8. XGBoost​如何处理缺失值？"></a>8. XGBoost​如何处理缺失值？</h2><p>$XGBoost$模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：</p>
<ul>
<li>在特征<code>k</code>上寻找最佳划分点时，不会对该列特征缺失的样本进行遍历，而只对该列特征值为无缺失的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找划分点的时间开销。 </li>
<li>在逻辑实现上，为了保证完备性，会<strong>将该特征值缺失的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后</strong>，<strong>选择分裂后增益最大</strong>的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。 </li>
<li>$XGBoost$<strong>在构建树的节点过程中只考虑非缺失值的数据遍历</strong>，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。 </li>
<li>如果在<strong>训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点</strong>。缺失值处理的伪代码如下：</li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/8166116-d771bed713a887d8.png?imageMogr2/auto-orient/strip|imageView2/2/w/802/format/webp" alt=""></p>
<ul>
<li><p><strong>树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用</strong>。</p>
<p>原因就是：<strong>一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值）</strong>，完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。</p>
</li>
</ul>
<h2 id="9-XGBoost​如何处理不平衡数据？"><a href="#9-XGBoost​如何处理不平衡数据？" class="headerlink" title="9. XGBoost​如何处理不平衡数据？"></a>9. XGBoost​如何处理不平衡数据？</h2><ul>
<li><p>设置<code>scale_pos_weight</code>来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，<code>scale_pos_weight</code>可以取10；</p>
</li>
<li><p>你不能重新平衡数据集(会破坏数据的真实分布)的情况下，应该设置<code>max_delta_step</code>为一个有限数字来帮助收敛（基模型为$LR$时有效）。</p>
</li>
</ul>
<h2 id="10-XGBoost​如何选择最佳分裂点？"><a href="#10-XGBoost​如何选择最佳分裂点？" class="headerlink" title="10. XGBoost​如何选择最佳分裂点？"></a>10. XGBoost​如何选择最佳分裂点？</h2><ul>
<li>训练前预先将特征<strong>对特征值进行排序</strong>，存储为<code>block</code>结构，以便在结点分裂时可以重复使用</li>
<li>采用<strong>特征并行</strong>的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，<strong>最终选择增益最大的那个特征的特征值</strong>作为最佳分裂点。</li>
<li>$XGBoost$使用<strong>直方图近似算法</strong>，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。</li>
</ul>
<h2 id="11-XGBoost​的Scalable性如何体现？"><a href="#11-XGBoost​的Scalable性如何体现？" class="headerlink" title="11. XGBoost​的Scalable性如何体现？"></a>11. XGBoost​的Scalable性如何体现？</h2><ul>
<li><strong>基分类器的scalability</strong>：弱分类器可以支持$CART$决策树，也可以支持$LR$和Linear。</li>
<li><strong>目标函数的scalability</strong>：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。</li>
<li><strong>学习方法的scalability</strong>：<code>block</code>结构支持并行化，支持 Out-of-core计算。</li>
</ul>
<h2 id="12-XGBoost​如何评价特征的重要性？"><a href="#12-XGBoost​如何评价特征的重要性？" class="headerlink" title="12. XGBoost​如何评价特征的重要性？"></a>12. XGBoost​如何评价特征的重要性？</h2><p>常用的三种方法来评判模型中特征的重要程度：</p>
<ul>
<li><code>freq</code> ： 频率是表示特定特征在模型树中发生分裂的相对次数的百分比</li>
<li><code>gain</code> ： 增益意味着相应的特征对通过对模型中的每个树采取每个特征的贡献而计算出的模型的相对贡献。与其他特征相比，此度量值的较高值意味着它对于生成预测更为重要。 </li>
<li><code>cover</code> ：覆盖度量指的是与此功能相关的观测的相对数量。例如，如果您有100个观察值，4个特征和3棵树，并且假设特征1分别用于决定树1，树2和树3中10个，5个和2个观察值的叶节点;那么该度量将计算此功能的覆盖范围为$10 + 5 + 2 = 17$个观测值。这将针对所有4项功能进行计算，并将以17个百分比表示所有功能的覆盖指标。</li>
</ul>
<p><strong>$XGBoost$是根据<code>gain</code>来做重要性判断的。</strong> </p>
<h2 id="13-XGBooost​参数调优的一般步骤"><a href="#13-XGBooost​参数调优的一般步骤" class="headerlink" title="13. XGBooost​参数调优的一般步骤"></a>13. XGBooost​参数调优的一般步骤</h2><ul>
<li><p>确定<code>learning rate</code>和<code>estimator</code>的数量</p>
<p><code>learning rate</code>可以先用0.1，用cv来寻找最优的<code>estimators</code></p>
</li>
<li><p><code>max_depth</code>和 <code>min_child_weight</code></p>
<p>我们调整这两个参数是因为，这两个参数对输出结果的影响很大。我们首先将这两个参数设置为较大的数，然后通过迭代的方式不断修正，缩小范围。</p>
<p><code>max_depth</code>: 每棵子树的最大深度，check from range(3，10，2)。</p>
<p><code>min_child_weight</code>: 子节点的权重阈值，check from range(1，6，2)。</p>
<p>如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分。</p>
</li>
<li><p><code>gamma</code></p>
<p>也称作最小划分损失<code>min_split_loss</code>，check from 0.1 to 0.5，指的是，对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。</p>
<ul>
<li>如果大于该阈值，则该叶子节点值得继续划分</li>
<li>如果小于该阈值，则该叶子节点不值得继续划分</li>
</ul>
</li>
<li><p><code>subsample</code>、 <code>colsample_by tree</code></p>
<p><code>subsample</code>是对训练的采样比例</p>
<p><code>colsample_by tree</code>是对特征的采样比例both check from 0.6 to 0.9</p>
</li>
<li><p>正则化参数</p>
<p><code>alpha</code> 是$L1$正则化系数，try <code>1e-5， 1e-2， 0.1， 1， 100</code></p>
<p><code>lambda</code> 是$L2$正则化系数</p>
</li>
<li><p>降低学习率</p>
<p>降低学习率的同时增加树的数量，通常最后设置学习率为<code>0.01~0.1</code></p>
</li>
</ul>
<h2 id="14-XGBoost​的优缺点"><a href="#14-XGBoost​的优缺点" class="headerlink" title="14. XGBoost​的优缺点"></a>14. XGBoost​的优缺点</h2><ul>
<li><p>优点</p>
<ul>
<li><strong>精度更高：</strong> $GBDT$ 只用到一阶泰勒展开，而 $XGBoost$ 对损失函数进行了二阶泰勒展开。$XGBoost$ 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；</li>
<li><strong>灵活性更强：</strong> $GBDT$ 以 $CART$ 作为基分类器，$XGBoost$ 不仅支持 $CART$ 还支持线性分类器，使用线性分类器的 $XGBoost$ 相当于带 和 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，$XGBoost$ 工具支持自定义损失函数，只需函数支持一阶和二阶求导；</li>
<li><strong>正则化：</strong> $XGBoost$ 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是$XGBoost$优于传统$GBDT$的一个特性。</li>
<li><strong>Shrinkage（缩减）：</strong> 相当于学习速率。$XGBoost$ 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。传统$GBDT$的实现也有学习速率；</li>
<li><strong>列抽样：</strong> $XGBoost$ 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。这也是$XGBoost$异于传统$GBDT$的一个特性；</li>
<li><strong>缺失值处理：</strong> 对于特征的值有缺失的样本，$XGBoost$ 采用的稀疏感知算法可以自动学习出它的分裂方向；</li>
<li><strong>$XGBoost$工具支持并行：</strong> $Boosting$不是一种串行的结构吗?怎么并行的？注意$XGBoost$的并行不是树粒度的并行，$XGBoost$也是一次迭代完才能进行下一次迭代的（第次迭代的代价函数里包含了前面次迭代的预测值）。$XGBoost$的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），$XGBoost$在训练之前，预先对数据进行了排序，然后保存为<code>block</code>结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个<code>block</code>结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>
<li><strong>可并行的近似算法：</strong> 树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以$XGBoost$还提出了一种可并行的近似算法，用于高效地生成候选的分割点。</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但<strong>在节点分裂过程中仍需要遍历数据集</strong>；</li>
<li><strong>预排序过程的空间复杂度过高</strong>，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于<strong>消耗了两倍的内存</strong>。</li>
</ul>
</li>
</ul>
<h2 id="15-XGBoost​和LightGBM​的区别"><a href="#15-XGBoost​和LightGBM​的区别" class="headerlink" title="15. XGBoost​和LightGBM​的区别"></a>15. XGBoost​和LightGBM​的区别</h2><p><img src="https://ask.qcloudimg.com/http-save/yehe-1622140/btc3oj2txs.jpeg?imageView2/2/w/1620" alt="img"></p>
<p>（1）树生长策略：XGB采用<code>level-wise</code>的分裂策略，LGB采用<code>leaf-wise</code>的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。 </p>
<p>（2）分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下：</p>
<ul>
<li>减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。</li>
<li>计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可。</li>
<li>LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算</li>
</ul>
<blockquote>
<p>但实际上$XGBoost$的近似直方图算法也类似于$LightGBM$这里的直方图算法，为什么$XGBoost$的近似算法比$LightGBM$还是慢很多呢？ $XGBoost$在每一层都动态构建直方图， 因为$XGBoost$的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而$LightGBM$中对每个特征都有一个直方图，所以构建一次直方图就够了。</p>
</blockquote>
<p>（3）支持离散变量：无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而$LightGBM$可以直接处理类别型变量。</p>
<p>（4）缓存命中率：$XGBoost$使用<code>block</code>结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</p>
<p>（5）$LightGBM$ 与 $XGBoost$ 的并行策略不同：</p>
<ul>
<li><strong>特征并行</strong> ：LGB特征并行的前提是每个<code>worker</code>留有一份完整的数据集，但是每个<code>worker</code>仅在特征子集上进行最佳切分点的寻找；<code>worker</code>之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个<code>worker</code>进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个<code>worker</code>节点中仅有部分的列数据，也就是垂直切分，每个<code>worker</code>寻找局部最佳切分点，<code>worker</code>之间相互通信，然后在具有最佳切分点的<code>worker</code>上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他<code>worker</code>才能开始分裂。二者的区别就导致了LGB中<code>worker</code>间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。</li>
<li><strong>数据并行</strong> ：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个<code>worker</code>上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得<code>worker</code>间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个<code>worker</code>建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个<code>worker</code>上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个<code>worker</code>间的通信量也就变得很大。</li>
<li><strong>投票并行（LGB）</strong>：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个<code>worker</code>首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。</li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
        <tag>LightGBM</tag>
        <tag>RandomForset</tag>
      </tags>
  </entry>
  <entry>
    <title>排序算法</title>
    <url>/2022/03/10/sort/</url>
    <content><![CDATA[<h1 id="经典排序算法"><a href="#经典排序算法" class="headerlink" title="经典排序算法"></a>经典排序算法</h1><ul>
<li>下文中的稳定是指：若a=b，而排序后的ab顺序与原来的ab顺序一样</li>
<li>交换排序：冒泡、快排；</li>
<li>选择排序：选择、堆；</li>
<li>插入排序：插入、希尔；</li>
<li>归并排序、基数排序<img src="/2022/03/10/sort/complixty.png" class="" title="complixty">
</li>
</ul>
<a id="more"></a>
<h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><ul>
<li>迭代n-1次，两个相邻元素两两相比，每次迭代将最大的元素放在该迭代序列的顶端。</li>
<li>稳定</li>
<li>优化后，对于最优的情况，即已经正序排列的，算法复杂度为O(N)<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(num)<span class="number">-1</span>):</span><br><span class="line">        flag = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(num)-i<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> num[j] &gt; num[j+<span class="number">1</span>]:</span><br><span class="line">                num[j], num[j+<span class="number">1</span>] = num[j+<span class="number">1</span>], num[j]</span><br><span class="line">                flag = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> num</span><br><span class="line">    <span class="keyword">return</span> num</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h2><ul>
<li>迭代n-1次，每次选择出最小的，并将最小位置处的值与当前位置值交换</li>
<li>不稳定，因为选择出最小的之后会跟原有数交换顺序，因此会破坏原来的顺序，例如5 3 5 2，第一次之后为2 3 5 5，此时5跟5的顺序变了<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_sort</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(num)<span class="number">-1</span>):</span><br><span class="line">        min_index = i   </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i,<span class="built_in">len</span>(num)<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> num[j+<span class="number">1</span>] &lt; num[min_index]:</span><br><span class="line">                min_index = j+<span class="number">1</span></span><br><span class="line">        num[i], num[min_index] = num[min_index], num[i]</span><br><span class="line">    <span class="keyword">return</span> num</span><br></pre></td></tr></table></figure>
<h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2></li>
<li>插入排序如同打扑克一样，每次将后面的牌插到前面已经排好序的牌中。</li>
<li>最快情况是O(N)，如果大部分数据已经排好序了，while pre_index &gt;= 0 and cur_num &lt; num[pre_index]这句的迭代次数会大大减少，会比较快</li>
<li>稳定<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(num)<span class="number">-1</span>):</span><br><span class="line">        pre_index = i <span class="comment"># 前一个数</span></span><br><span class="line">        cur_num = num[i+<span class="number">1</span>] <span class="comment"># 当前待插入数 </span></span><br><span class="line">        <span class="keyword">while</span> pre_index &gt;= <span class="number">0</span> <span class="keyword">and</span> cur_num &lt; num[pre_index]:</span><br><span class="line">            num[pre_index+<span class="number">1</span>] = num[pre_index] <span class="comment"># 往后挪一位，前面的已经排好序了</span></span><br><span class="line">            pre_index -= <span class="number">1</span>   </span><br><span class="line">        num[pre_index+<span class="number">1</span>] = cur_num</span><br><span class="line">    <span class="keyword">return</span> num</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h2><ul>
<li>对插入排序的优化，使用了递减的增量序列</li>
<li>如上所述插入排序中：如果大部分数据已经排好序了，while pre_index &gt;= 0 and cur_num &lt; num[pre_index]这句的迭代次数会大大减少，会比较快 –&gt; 所以希尔排序就是针对这个做了优化，即先减少需要排序的数量，再逐步对其排序</li>
<li>希尔排序是不稳定的算法，它满足稳定算法的定义。对于相同的两个数，可能由于分在不同的组中而导致它们的顺序发生变化。</li>
<li>希尔排序的性能根据其选取的序列而变化</li>
<li>不稳定</li>
<li>使用动态增量序列的代码如下：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shell_sort</span>(<span class="params">num</span>):</span></span><br><span class="line">    gap = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> gap &lt; <span class="built_in">len</span>(num) // <span class="number">3</span>: <span class="comment">#gap &lt; (3a,3a+1,3a+2)//3: (a-1)*3+1=3a-2</span></span><br><span class="line">        gap = gap*<span class="number">3</span> + <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> gap &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(gap,<span class="built_in">len</span>(num)):</span><br><span class="line">            cur_num = num[gap]</span><br><span class="line">            pre_index = i-gap</span><br><span class="line">            <span class="keyword">while</span> pre_index &gt;= <span class="number">0</span> <span class="keyword">and</span> cur_num &lt; num[pre_index]:</span><br><span class="line">                num[pre_index+gap] = num[pre_index] </span><br><span class="line">                pre_index -= gap</span><br><span class="line">            num[pre_index+gap] = cur_num</span><br><span class="line">        gap //= <span class="number">3</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><ul>
<li>分治</li>
<li>递归<ul>
<li>终止条件：剩一个元素时，返回该元素；再上一层对返回的两个元素比较排序</li>
</ul>
</li>
<li>稳定<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge</span>(<span class="params">left,right</span>):</span> <span class="comment"># left和right本身是已经排序好的</span></span><br><span class="line">        result = []</span><br><span class="line">        i=j=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(left) <span class="keyword">and</span> j &lt; <span class="built_in">len</span>(right):</span><br><span class="line">            <span class="keyword">if</span> left[i] &lt;= right[j]:</span><br><span class="line">                result.append(left)</span><br><span class="line">                i+=<span class="number">1</span>            </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                result.append(right)</span><br><span class="line">                j+=<span class="number">1</span></span><br><span class="line">        result = result + left[i:] + right[j:]</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(num) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> num</span><br><span class="line">    mid = <span class="built_in">len</span>(num) // <span class="number">2</span></span><br><span class="line">    left = merge_sort(num[:mid])</span><br><span class="line">    right = merge_sort(num[mid:])</span><br><span class="line">    <span class="keyword">return</span> merge(left,right)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><ul>
<li>冒泡+二分+分治</li>
<li>核心：每次迭代使选取的基准值插入到序列中，该序列中基准值左边的值小于基准值，右边的值大于基准值，然后再对两边分别迭代</li>
<li>基准数选择以及指针移动顺序：最终两个指针相遇时，要把基准数和相遇的位置交换，此时该位置左边的数小于基准数，右边大于基准数；若选择最左边的数为基准数，肯定要跟比它小的数交换，因此只有右指针先动才能找到比它小的（例如算法2里右指针相遇时找到的一定是上一轮左指针的交换结果，一定是小于基准数的）</li>
<li>与归并的区别：<ul>
<li>归并是二分，再递归——快排是找到pivot后冒泡，再递归</li>
<li>归并是out-place，快排是in-place</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 二分之后不断地递归，每次递归求出基准值的具体位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort1</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(num) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> num</span><br><span class="line">    pivot = num[<span class="number">0</span>]</span><br><span class="line">    left = [num[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(num)) <span class="keyword">if</span> num[i] &lt;= pivot]</span><br><span class="line">    right = [num[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(num)) <span class="keyword">if</span> num[i] &gt; pivot]</span><br><span class="line">    <span class="keyword">return</span> quick_sort1(left) + pivot + quick_sort2(right)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort2</span>(<span class="params">num,left,right</span>):</span></span><br><span class="line">    <span class="keyword">if</span> left &gt;= right: <span class="comment"># 两指针相遇，则终止</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    low = left</span><br><span class="line">    high = right</span><br><span class="line">    pivot = num[left]</span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> num[right] &gt; pivot:<span class="comment"># 因为记录了left，所以要从right开始</span></span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        num[left] = num[right]</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> num[left] &lt;= pivot:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        num[right] = num[left]</span><br><span class="line">    num[right] = pivot        </span><br><span class="line"></span><br><span class="line">    quick_sort2(num, low, left<span class="number">-1</span>)</span><br><span class="line">    quick_sort2(num, right+<span class="number">1</span>, high)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort3</span>(<span class="params">num,left,right</span>):</span></span><br><span class="line">    <span class="keyword">if</span> left &lt; right:</span><br><span class="line">        p = partition(num, left, right)</span><br><span class="line">        quick_sort3(num, left, p<span class="number">-1</span>)</span><br><span class="line">        quick_sort3(num, p+<span class="number">1</span>, right)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span>(<span class="params">num, left, right</span>):</span></span><br><span class="line">    pivot = num[right] <span class="comment"># 这里是从左边开始移动指针的，因此是right</span></span><br><span class="line">    i = left - <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(left, right):</span><br><span class="line">        <span class="keyword">if</span> num[j] &lt;= pivot:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            num[i], num[j] = num[j], num[i]</span><br><span class="line">    num[i+<span class="number">1</span>], num[right] = num[right], num[i+<span class="number">1</span>] <span class="comment"># 把基准数移过来</span></span><br><span class="line">    <span class="keyword">return</span> i+<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><ul>
<li>利用堆进行选择排序</li>
<li>大根堆：根节点的值大于叶子结点的值。用于升序排列。</li>
<li>小根堆：根节点的值小于叶子结点的值。用于降序排列。</li>
</ul>
<h3 id="排序流程"><a href="#排序流程" class="headerlink" title="排序流程"></a>排序流程</h3><ul>
<li>以大根堆排序为例：<ul>
<li>1）先构建大根堆</li>
<li>2）将堆顶元素与原序列最后一个元素交换</li>
<li>3）排除最后一个元素，堆的尺寸减1，更新大根堆</li>
<li>4）重复1~3，直到序列有序</li>
</ul>
</li>
</ul>
<h4 id="堆的构造："><a href="#堆的构造：" class="headerlink" title="堆的构造："></a>堆的构造：</h4><ul>
<li>堆是一个完全二叉树，可以用数组来表示。例如a可以表示为[99,66,45,33,37,10,22,13]，所以对于i处的节点，其子节点坐标为2*i+1, 2*i+2</li>
<li>在构造最大堆时，可以先构造一颗完全二叉树，再对每个根节点及其两个子节点进行位置调整，在调整时，从最后一个非叶子节点开始调整</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adjust_heap</span>(<span class="params">nums, i, size</span>):</span> </span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    nums[:size]: 完全二叉树</span></span><br><span class="line"><span class="string">    i: 从第i个节点开始调整直到根节点</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    left = <span class="number">2</span>*i+<span class="number">1</span></span><br><span class="line">    right = <span class="number">2</span>*i+<span class="number">2</span></span><br><span class="line">    largest = i</span><br><span class="line">    <span class="keyword">if</span> left &lt; size <span class="keyword">and</span> nums[largest] &lt; nums[left]: <span class="comment"># 要保证小于size</span></span><br><span class="line">        largest = left</span><br><span class="line">   <span class="keyword">if</span> right &lt; size <span class="keyword">and</span> nums[largest] &lt; nums[right]:</span><br><span class="line">        largest = right</span><br><span class="line">   <span class="keyword">if</span> largest != i: <span class="comment"># 如果该节点需要调整，则对其进行调整，并沿着被改变的那个叶子节点不断调整</span></span><br><span class="line">        nums[largest], nums[i] = nums[i], nums[largest]</span><br><span class="line">        adjust_heap(nums, largest, size)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_heap</span>(<span class="params">nums</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)//<span class="number">2</span>)[::<span class="number">-1</span>]: <span class="comment">#从最后一个非叶子结点开始调整，往上的节点都是非叶子结点</span></span><br><span class="line">        adjust_heap(nums, i, <span class="built_in">len</span>(nums))      </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heap_sort</span>(<span class="params">nums</span>):</span></span><br><span class="line">    build_heap(nums) <span class="comment"># 先建size大小的堆</span></span><br><span class="line">    size = <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size)[::<span class="number">-1</span>]:</span><br><span class="line">        nums[<span class="number">0</span>], nums[i] = nums[i], nums[<span class="number">0</span>] <span class="comment">#最后的元素一定小于堆顶元素，把堆顶元素放在最后</span></span><br><span class="line">        adjust_heap(nums, <span class="number">0</span>, i) <span class="comment"># 堆顶元素被改变，需要调整</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
<h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul>
<li>需要做n次循环，每次从根节点调整堆，并且沿着其中一个子树往下调整，因此为nlogn</li>
</ul>
<h2 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_sort</span>(<span class="params">nums</span>):</span></span><br><span class="line">    bucket = [<span class="number">0</span>]*<span class="built_in">len</span>(<span class="built_in">max</span>(nums)+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        bucket[num] += <span class="number">1</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(bucket)):</span><br><span class="line">        <span class="keyword">while</span> bucket[j]:</span><br><span class="line">            bucket[j] -= <span class="number">1</span></span><br><span class="line">            nums[i] = j</span><br><span class="line">            i+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> nums           </span><br></pre></td></tr></table></figure>
<h2 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h2><ul>
<li>计数排序的升级版：计数排序每个桶为值本身即y=x，桶排序加了一个映射关系，即y=f(x).该映射关系应做到：在额外空间充足的情况下，尽量增大桶的数量</li>
<li>使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中</li>
<li>什么时候最快（Best Cases）：当输入的数据可以均匀的分配到每一个桶中</li>
<li>什么时候最慢（Worst Cases）：当输入的数据被分配到了同一个桶中<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucketSort</span>(<span class="params">nums, defaultBucketSize = <span class="number">5</span></span>):</span></span><br><span class="line">    maxVal, minVal = <span class="built_in">max</span>(nums), <span class="built_in">min</span>(nums)</span><br><span class="line">    bucketSize = defaultBucketSize  <span class="comment"># 如果没有指定桶的大小，则默认为5</span></span><br><span class="line">    bucketCount = (maxVal - minVal) // bucketSize + <span class="number">1</span>  <span class="comment"># 数据分为 bucketCount 组</span></span><br><span class="line">    buckets = []  <span class="comment"># 二维桶</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(bucketCount):</span><br><span class="line">        buckets.append([])</span><br><span class="line">    <span class="comment"># 利用函数映射将各个数据放入对应的桶中</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        buckets[(num - minVal) // bucketSize].append(num)</span><br><span class="line">    nums.clear()  <span class="comment"># 清空 nums</span></span><br><span class="line">    <span class="comment"># 对每一个二维桶中的元素进行排序</span></span><br><span class="line">    <span class="keyword">for</span> bucket <span class="keyword">in</span> buckets:</span><br><span class="line">        insertionSort(bucket)  <span class="comment"># 假设使用插入排序</span></span><br><span class="line">        nums.extend(bucket)    <span class="comment"># 将排序好的桶依次放入到 nums 中</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h2><ul>
<li>桶排序的推广</li>
<li>基数排序是桶排序的一种推广，它所考虑的待排记录包含不止一个关键字。例如对一副牌的整理，可将每张牌看作一个记录，包含两个关键字：花色、面值。一般我们可以将一个有序列是先按花色划分为四大块，每一块中又再按面值大小排序。这时“花色”就是一张牌的“最主位关键字”，而“面值”是“最次位关键字”。</li>
<li>基数排序有两种方法：<ul>
<li>MSD （主位优先法）：从高位开始进行排序</li>
<li>LSD （次位优先法）：从低位开始进行排序<h3 id="LSD-Radix-Sort"><a href="#LSD-Radix-Sort" class="headerlink" title="LSD Radix Sort"></a>LSD Radix Sort</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">radixSort</span>(<span class="params">nums</span>):</span></span><br><span class="line">    mod = <span class="number">10</span></span><br><span class="line">    div = <span class="number">1</span></span><br><span class="line">    mostBit = <span class="built_in">len</span>(<span class="built_in">str</span>(<span class="built_in">max</span>(nums)))  <span class="comment"># 最大数的位数决定了外循环多少次</span></span><br><span class="line">    buckets = [[] <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(mod)] <span class="comment"># 构造 mod 个空桶</span></span><br><span class="line">    <span class="keyword">while</span> mostBit:</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:  <span class="comment"># 将数据放入对应的桶中</span></span><br><span class="line">            buckets[num // div % mod].append(num)</span><br><span class="line">        i = <span class="number">0</span>  <span class="comment"># nums 的索引</span></span><br><span class="line">        <span class="keyword">for</span> bucket <span class="keyword">in</span> buckets:  <span class="comment"># 将数据收集起来</span></span><br><span class="line">            <span class="keyword">while</span> bucket:</span><br><span class="line">                nums[i] = bucket.pop(<span class="number">0</span>) <span class="comment"># 依次取出</span></span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        div *= <span class="number">10</span></span><br><span class="line">        mostBit -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h1 id="大数据排序"><a href="#大数据排序" class="headerlink" title="大数据排序"></a>大数据排序</h1><ul>
<li>top k/ bottom k/ 中位数：堆</li>
<li>排序：bitmap</li>
<li>去重：bitmap</li>
</ul>
<h2 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h2><ul>
<li>作用：求前n大（top k)，前n小，中位数</li>
<li>前n大：维护一个size为n的最小堆，当有新的元素时，与堆顶元素比较，若比堆顶元素大，则替换堆顶元素</li>
<li>前n小：维护size为n的最大堆</li>
<li>问题实例：100w个数中找最大的前100个数。用一个100个元素大小的最小堆即可。</li>
</ul>
<h2 id="Bitmap"><a href="#Bitmap" class="headerlink" title="Bitmap"></a>Bitmap</h2><ul>
<li>用一位代表一个数字</li>
<li>例如N=10000，则需要使用int a[9999//32 + 1]的内存，因为python的int默认是符合数，所以第一位不能用，- 则数组个数为(N-1)//31+1</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- encoding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bitmap</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,<span class="built_in">max</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;确定所需数组个数&#x27;</span></span><br><span class="line">        self.size = <span class="built_in">int</span> ((<span class="built_in">max</span> + <span class="number">31</span> - <span class="number">1</span>) / <span class="number">31</span>)</span><br><span class="line">        self.array = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.size)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bitindex</span>(<span class="params">self,num</span>):</span></span><br><span class="line">        <span class="string">&#x27;确定数组中元素的位索引&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> num % <span class="number">31</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_1</span>(<span class="params">self,num</span>):</span></span><br><span class="line">        <span class="string">&#x27;将元素所在的位 置1&#x27;</span></span><br><span class="line">        elemindex = num // <span class="number">31</span></span><br><span class="line">        byteindex = self.bitindex(num)</span><br><span class="line">        ele = self.array[elemindex]</span><br><span class="line">        self.array[elemindex] = ele | (<span class="number">1</span> &lt;&lt; byteindex)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_1</span>(<span class="params">self,i</span>):</span></span><br><span class="line">        <span class="string">&#x27;检测元素存在的位置&#x27;</span></span><br><span class="line">        elemindex = i / <span class="number">31</span></span><br><span class="line">        byteindex = self.bitindex(i)</span><br><span class="line">        <span class="keyword">if</span> self.array[elemindex] &amp; (<span class="number">1</span> &lt;&lt; byteindex):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    Max = <span class="built_in">ord</span>(<span class="string">&#x27;z&#x27;</span>)</span><br><span class="line">    suffle_array = [x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="string">&#x27;qwelmfg&#x27;</span>]</span><br><span class="line">    result = []</span><br><span class="line">    bitmap = Bitmap(Max)</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> suffle_array:</span><br><span class="line">        bitmap.set_1(<span class="built_in">ord</span>(c))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Max+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> bitmap.test_1(i):</span><br><span class="line">            result.append(<span class="built_in">chr</span>(i))</span><br><span class="line">    <span class="built_in">print</span> <span class="string">u&#x27;原始数组为:    %s&#x27;</span> % suffle_array</span><br><span class="line">    <span class="built_in">print</span> <span class="string">u&#x27;排序后的数组为: %s&#x27;</span> % result</span><br></pre></td></tr></table></figure>
<h2 id="外部排序"><a href="#外部排序" class="headerlink" title="外部排序"></a>外部排序</h2><h3 id="外部排序基础步骤"><a href="#外部排序基础步骤" class="headerlink" title="外部排序基础步骤"></a>外部排序基础步骤</h3><ul>
<li>两个步骤：<ul>
<li>1）将需要排序的文件多次读入内存进行排序，并写入到多个文件里；</li>
<li>2）多路归并</li>
</ul>
</li>
<li>例如：2路归并，每次归并后，可以使m个归并段变为m/2(取上界)个归并段。在实际归并的过程中，由于内存容量的限制不能满足同时将 2 个归并段全部完整的读入内存进行归并，只能不断地取 2 个归并段中的每一小部分进行归并，通过不断地读数据和向外存写数据，直至 2 个归并段完成归并变为 1 个大的有序文件。</li>
</ul>
<h3 id="败者树-胜者树"><a href="#败者树-胜者树" class="headerlink" title="败者树/胜者树"></a>败者树/胜者树</h3><ul>
<li>与外部排序的性能有关的因素有：k。增加k，会减少外存数据的时间（需要存取的文件变少了），但k的增加会增加内部归并的时间。例如，10个文件，若k=2，则第一次归并时，需要外存5个文件，查找最小值时的归并次数为1；若k=5，则第一次归并时，需要外存2个文件，但内部归并时需要比较4次才能找到最小值。</li>
<li>k路归并时使用败者树可以避免k的增加引起的排序效率的降低。</li>
<li>复杂度分析：k路、u个记录、s趟：k个归并段中选取最小的记录需要比较k-1次， 为得到u个记录的一个有序段共需要(u-1)(k-1)次；若归并趟数为s次，那么对n个记录的文件进行外排时，内部归并过程中进行的总的比较次数为 s(n-1)(k-1)，也即(向上取整)(log(k)m)(k-1)(n-1)=(向上取整)(log2m/log2k)(k-1)(n-1)。而(k- 1)/log2k随k增而增因此内部归并时间随k增长而增长了，抵消了外存读写减少的时间，这样做不行，由此引出了“败者树”tree of loser的使用。在内部归并过程中利用败者树将k个归并段中选取最小记录比较的次数降为(向上取整)(log2k)次使总比较次数为(向上取整) (log2m)(n-1)，与k无关。</li>
</ul>
<h4 id="败者树"><a href="#败者树" class="headerlink" title="败者树"></a>败者树</h4><ul>
<li>胜败：两数相比，小者为胜，大者为败。</li>
<li>败者树和胜者树为一颗完全二叉树。</li>
<li>败者树：其双亲结点存储的是左右孩子比较之后的失败者，而胜利者则继续同其它的胜者去比较。</li>
<li>需要两个数组来表示，内部节点用ls数组，叶子结点为需要排序的序列。</li>
<li>构造过程：<ul>
<li>a：b3 Vs b4，b3胜b4负，内部结点ls[4]的值为4，表示b4为败者；胜者b3继续参与竞争。</li>
<li>b：b3 Vsb0，b3胜b0负，内部结点ls[2]的值为0，表示b0为败者；胜者b3继续参与竞争。</li>
<li>c：b1 Vs b2，b1胜b2负，内部结点ls[3]的值为2，表示b2为败者；胜者b1继续参与竞争。</li>
<li>d：b3 Vs b1，b3胜b1负，内部结点ls[1]的值为1，表示b1为败者；胜者b3为最终冠军，用ls[0]=3，记录的最后的胜者索引。</li>
</ul>
</li>
<li>重构过程：当b3变为13时，败者树的重构过程如下：<ul>
<li>a：b3 Vs b[ls[4]],也就是b3 Vsb4,b4胜，继续参加下面的竞争，ls[4]=3记录败者。</li>
<li>b：b4Vs b[ls[2]],也就是b4 Vs b0, b0胜，继续参加下面的竞争，ls[2]=4记录败者。</li>
<li>c：b0Vs b[ls[1]],也就是b0 Vs b1, b1胜，b1为最终冠军，所以ls[0]=1记录冠军，ls[1]=0记录败者。</li>
</ul>
</li>
<li>败者树简化了重构。败者树的重构只是与该结点的父结点的记录有关，而胜者树的重构还与该结点的兄弟结点有关。所以败者树在外排序的k路平衡归并中使用。</li>
</ul>
<h4 id="胜者树"><a href="#胜者树" class="headerlink" title="胜者树"></a>胜者树</h4><ul>
<li>b3变为11后，重构为：即胜者树在重构时，既需要与父节点比较还要与兄弟节点比较</li>
</ul>
<h3 id="利用败者树的外部排序算法"><a href="#利用败者树的外部排序算法" class="headerlink" title="利用败者树的外部排序算法"></a>利用败者树的外部排序算法</h3><ul>
<li>1）假设有一个72KB的文件，其中存储了18K个整数，磁盘中物理块的大小为4KB，将文件分成18组，每组刚好4KB。首先通过18次内部排序，把18组数据排好序，得到初始的18个归并段R1~R18，每个归并段有1024个整数。</li>
<li>2）对这18个归并段使用4路平衡归并排序：</li>
<li>第1次归并：产生5个归并段R11   R12    R13    R14    R15<ul>
<li>其中R11是由{R1,R2,R3,R4}中的数据合并而来</li>
<li>R12是由{R5,R6,R7,R8}中的数据合并而来</li>
<li>R13是由{R9,R10,R11,R12}中的数据合并而来</li>
<li>R14是由{R13,R14,R15,R16}中的数据合并而来</li>
<li>R15是由{R17,R18}中的数据合并而来</li>
<li>把这5个归并段的数据写入5个文件：foo_1.dat    foo_2.dat    foo_3.dat     foo_4.dat     foo_5.dat</li>
</ul>
</li>
<li>第2次归并：从第1次归并产生的5个文件中读取数据，合并，产生2个归并段R21  R22<ul>
<li>其中R21是由{R11,R12,R13,R14}中的数据合并而来</li>
<li>其中R22是由{R15}中的数据合并而来</li>
<li>把这2个归并段写入2个文件bar_1.dat   bar_2.dat</li>
</ul>
</li>
<li>第3次归并：从第2次归并产生的2个文件中读取数据，合并，产生1个归并段R31<ul>
<li>R31是由{R21,R22}中的数据合并而来</li>
<li>把这个文件写入1个文件foo_1.dat</li>
</ul>
</li>
<li>此即为最终排序好的文件。</li>
<li>其中，归并过程中使用了败者树。</li>
</ul>
<h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><ul>
<li>共n条记录，对于有m个初始归并段，k路平衡的归并排序，磁盘读写次数为：|log(k)m|</li>
<li>不使用败者树归并：每次需要比较k-1次，则O((n-1)*(k-1))</li>
<li>使用败者树归并：每次需要比较O(log(k))（这个值是每次得到最小值后重构败者树的时间复杂度），即0O((n-1)*log(k))</li>
<li>为啥要乘n-1：每次归并只能产生一个记录，共有n个记录，最后一条记录为最大值</li>
</ul>
<h2 id="Trie树"><a href="#Trie树" class="headerlink" title="Trie树"></a>Trie树</h2><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><ul>
<li>1）根节点不包含字符，除根节点外的每一个子节点都包含一个字符。</li>
<li>2）从根节点到某一节点，路径上经过的字符连接起来，就是该节点对应的字符串。</li>
<li>3）每个单词的公共前缀作为一个字符节点保存。</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ul>
<li>适用范围：数据量大，重复多，但是数据种类小可以放入内存</li>
<li>字符串检索<ul>
<li>从根节点开始一个一个字符进行比较：如果沿路比较，发现不同的字符，则表示该字符串在集合中不存在。如果所有的字符全部比较完并且全部相同，还需判断最后一个节点的标志位（标记该节点是否代表一个关键字）。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">struct trie_node</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">bool</span> isKey;   // 标记该节点是否代表一个关键字</span><br><span class="line">    trie_node *children[<span class="number">26</span>]; // 各个子节点 </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li>词频统计</li>
<li>为了实现词频统计，我们修改了节点结构，用一个整型变量count来计数。对每一个关键字执行插入操作，若已存在，计数加1，若不存在，插入后count置1。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">struct trie_node</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">int</span> count;   // 记录该节点代表的单词的个数</span><br><span class="line">    trie_node *children[<span class="number">26</span>]; // 各个子节点 </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="大数据处理相关例题"><a href="#大数据处理相关例题" class="headerlink" title="大数据处理相关例题"></a>大数据处理相关例题</h2><ul>
<li>K: 千 2^10</li>
<li>M：百万 2&amp;20</li>
<li>G：亿 2^30</li>
</ul>
<h3 id="给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？"><a href="#给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？" class="headerlink" title="给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？"></a>给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？</h3><ul>
<li>方案1：可以估计每个文件安的大小为50G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。<ul>
<li>Step1: 遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为 a0, a1…a999）中。这样每个小文件的大约为300M。</li>
<li>Step2: 遍历文件b，采取和a相同的方式将url分别存储到1000各小文件（记为 a0, a1…a999）。这样处理后，所有可能相同的url都在对应的小文件中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。</li>
<li>Step3: 求每对小文件ai和bi中相同的url时，可以把ai的url存储到hash_set/hash_map中。然后遍历bi的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。</li>
</ul>
</li>
<li>方案2：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。</li>
</ul>
<h3 id="有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。"><a href="#有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。" class="headerlink" title="有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。"></a>有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。</h3><ul>
<li>方案1： <ul>
<li>Step1: 顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为 ）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。</li>
<li>Step2: 找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件（记为 ）。</li>
<li>Step3: 对 这10个文件进行归并排序（内排序与外排序相结合）。 </li>
</ul>
</li>
<li>方案2：一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/ hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。</li>
<li>方案3：与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。 </li>
</ul>
<h3 id="有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。"><a href="#有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。" class="headerlink" title="有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。"></a>有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。</h3><ul>
<li>Step1: 顺序读文件中，对于每个词x，取hash(x)%5000 ，然后按照该值存到5000个小文件（记为f0,f1,…,f4999） 中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，知道分解得到的小文件的大小都不超过1M。</li>
<li>Step2: 对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100词及相应的频率存入文件，这样又得到了5000个文件。</li>
<li>Step3: 把这5000个文件进行归并（类似与归并排序）的过程了。</li>
</ul>
<h3 id="海量日志数据，提取出某日访问百度次数最多的那个IP。"><a href="#海量日志数据，提取出某日访问百度次数最多的那个IP。" class="headerlink" title="海量日志数据，提取出某日访问百度次数最多的那个IP。"></a>海量日志数据，提取出某日访问百度次数最多的那个IP。</h3><ul>
<li>Step1：从这一天的日志数据中把访问百度的IP取出来，逐个写入到一个大文件中;</li>
<li>Step2：注意到IP是32位的，最多有2^32=4G个IP。同样可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件;</li>
<li>Step3：对于每一个小文件，可以构建一个IP为key，出现次数为value的Hashmap，同时记录当前出现次数最多的那个IP地址;</li>
<li>Step4：在这1000个最大的IP中，找出那个频率最大的IP，即为所求。    </li>
</ul>
<h3 id="寻找热门查询："><a href="#寻找热门查询：" class="headerlink" title="寻找热门查询："></a>寻找热门查询：</h3><ul>
<li>搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录，这些查询串的重复 读比较高，虽然总数是1千万，但是如果去除重复和，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就越热门。请你统计最热门的10个查询串，要求使用的内存不能超过1G。<ul>
<li>(1) 请描述你解决这个问题的思路； </li>
<li>(2) 请给出主要的处理流程，算法，以及算法的复杂度。 </li>
</ul>
</li>
<li>方案1：采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。 <ul>
<li>Step1: 先对这批海量数据预处理，在O(N)的时间内用Hash表完成统计(之前写成了排序，特此订正。);</li>
<li>Step2: 借助堆这个数据结构，找出TopK，时间复杂度为N‘logK。</li>
<li>即，借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比- 所以，我们最终的时间复杂度是：O(N)+N’*O(logK)，(N为1000万，N’为300万)。</li>
</ul>
</li>
<li>或者：采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。</li>
</ul>
<h3 id="在2-5亿个整数中找出不重复的整数，内存不足以容纳这2-5亿个整数。"><a href="#在2-5亿个整数中找出不重复的整数，内存不足以容纳这2-5亿个整数。" class="headerlink" title="在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数。"></a>在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数。</h3><ul>
<li>方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存 内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。</li>
<li>方案2：也可采用上题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。 </li>
</ul>
<h3 id="海量数据分布在100台电脑中，想个办法高校统计出这批数据的TOP10。"><a href="#海量数据分布在100台电脑中，想个办法高校统计出这批数据的TOP10。" class="headerlink" title="海量数据分布在100台电脑中，想个办法高校统计出这批数据的TOP10。"></a>海量数据分布在100台电脑中，想个办法高校统计出这批数据的TOP10。</h3><ul>
<li>方案1： <ul>
<li>在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆）。比如求TOP10大，我们首先取前 10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元 素就是TOP10大。</li>
<li>求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 </li>
</ul>
</li>
</ul>
<h3 id="怎么在海量数据中找出重复次数最多的一个？"><a href="#怎么在海量数据中找出重复次数最多的一个？" class="headerlink" title="怎么在海量数据中找出重复次数最多的一个？"></a>怎么在海量数据中找出重复次数最多的一个？</h3><ul>
<li>方案1：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求（具体参考前面的题）。</li>
</ul>
<h3 id="上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。"><a href="#上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。" class="headerlink" title="上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。"></a>上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。</h3><ul>
<li>方案1：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后就是取出前N个出现次数最多的数据了，可以用第6题提到的堆机制完成。</li>
</ul>
<h3 id="1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？"><a href="#1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？" class="headerlink" title="1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？"></a>1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？</h3><ul>
<li>方案1：这题用trie树比较合适，hash_map也应该能行。 </li>
</ul>
<h3 id="一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。"><a href="#一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。" class="headerlink" title="一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。"></a>一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。</h3><ul>
<li>方案1：这题是考虑时间效率。用trie树统计每个词出现的次数，时间复杂度是O(n<em>le)（le表示单词的平准长度）。然后是找出出现最频繁的 前10个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是O(n</em>lg10)。所以总的时间复杂度，是O(n<em>le)与O(n</em>lg10)中较大 的哪一个。</li>
</ul>
<h3 id="一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。"><a href="#一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。" class="headerlink" title="一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。"></a>一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。</h3><ul>
<li>方案1：首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词。</li>
</ul>
<h3 id="100w个数中找出最大的100个数。"><a href="#100w个数中找出最大的100个数。" class="headerlink" title="100w个数中找出最大的100个数。"></a>100w个数中找出最大的100个数。</h3><ul>
<li>方案1：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。 </li>
<li>方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。</li>
<li>方案3：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个 最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。</li>
</ul>
<h3 id="一共有N个机器，每个机器上有N个数。每个机器最多存O-N-个数并对它们操作。如何找到-个数中的中数？"><a href="#一共有N个机器，每个机器上有N个数。每个机器最多存O-N-个数并对它们操作。如何找到-个数中的中数？" class="headerlink" title="一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到 个数中的中数？"></a>一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到 个数中的中数？</h3><ul>
<li>方案1：先大体估计一下这些数的范围，比如这里假设这些数都是32位无符号整数（共有 个）。我们把0到 的整数划分为N个范围段，每个段包含 个整数。比如，第一个段位0到 ，第二段为 到 ，…，第N个段为 到 。 然后，扫描每个机器上的N个数，把属于第一个区段的数放到第一个机器上，属于第二个区段的数放到第二个机器上，…，属于第N个区段的数放到第N个机器上。 注意这个过程每个机器上存储的数应该是O(N)的。下面我们依次统计每个机器上数的个数，一次累加，直到找到第k个机器，在该机器上累加的数大于或等于 ，而在第k-1个机器上的累加数小于 ，并把这个数记为x。那么我们要找的中位数在第k个机器中，排在第 位。然后我们对第k个机器的数排序，并找出第 个数，即为所求的中位数。复杂度是 的。 </li>
<li>方案2：先对每台机器上的数进行排序。排好序后，我们采用归并排序的思想，将这N个机器上的数归并起来得到最终的排序。找到第n个便是所求。复杂度是n(i)的。 </li>
</ul>
<h3 id="最大间隙问题"><a href="#最大间隙问题" class="headerlink" title="最大间隙问题"></a>最大间隙问题</h3><ul>
<li>给定n个实数 ，求着n个实数在实轴上向量2个数之间的最大差值，要求线性的时间算法。 </li>
<li>方案1：最先想到的方法就是先对这n个数据进行排序，然后一遍扫描即可确定相邻的最大间隙。但该方法不能满足线性时间的要求。故采取如下方法： <ul>
<li>找到n个数据中最大和最小数据max和min。 </li>
<li>用n-2个点等分区间[min, max]，即将[min, max]等分为n-1个区间（前闭后开区间），将这些区间看作桶，编号为 ，且桶 的上界和桶i+1的下届相同，即每个桶的大小相同。每个桶的大小为： 。实际上，这些桶的边界构成了一个等差数列（首项为min，公差为 ），且认为将min放入第一个桶，将max放入第n-1个桶。</li>
<li>将n个数放入n-1个桶中：将每个元素 分配到某个桶（编号为index），其中 ，并求出分到每个桶的最大最小数据。 </li>
<li>最大间隙：除最大最小数据max和min以外的n-2个数据放入n-1个桶中，由抽屉原理可知至少有一个桶是空的，又因为每个桶的大小相同，所以最大间隙 不会在同一桶中出现，一定是某个桶的上界和气候某个桶的下界之间隙，且该量筒之间的桶（即便好在该连个便好之间的桶）一定是空桶。也就是说，最大间隙在桶 i的上界和桶j的下界之间产生 。一遍扫描即可完成。</li>
</ul>
</li>
</ul>
<h3 id="将多个集合合并成没有交集的集合：给定一个字符串的集合，格式如：-。要求将其中交集不为空的集合合并，要求合并完成的集合之间无交集，例如上例应输出-。"><a href="#将多个集合合并成没有交集的集合：给定一个字符串的集合，格式如：-。要求将其中交集不为空的集合合并，要求合并完成的集合之间无交集，例如上例应输出-。" class="headerlink" title="将多个集合合并成没有交集的集合：给定一个字符串的集合，格式如： 。要求将其中交集不为空的集合合并，要求合并完成的集合之间无交集，例如上例应输出 。"></a>将多个集合合并成没有交集的集合：给定一个字符串的集合，格式如： 。要求将其中交集不为空的集合合并，要求合并完成的集合之间无交集，例如上例应输出 。</h3><ul>
<li>(1) 请描述你解决这个问题的思路； </li>
<li>(2) 给出主要的处理流程，算法，以及算法的复杂度； </li>
<li>(3) 请描述可能的改进。 </li>
<li>方案1：采用并查集。首先所有的字符串都在单独的并查集中。然后依扫描每个集合，顺序合并将两个相邻元素合并。例如，对于 ， 首先查看aaa和bbb是否在同一个并查集中，如果不在，那么把它们所在的并查集合并，然后再看bbb和ccc是否在同一个并查集中，如果不在，那么也把 它们所在的并查集合并。接下来再扫描其他的集合，当所有的集合都扫描完了，并查集代表的集合便是所求。复杂度应该是O(NlgN)的。改进的话，首先可以 记录每个节点的根结点，改进查询。合并的时候，可以把大的和小的进行合，这样也减少复杂度。</li>
</ul>
<h3 id="最大子序列与最大子矩阵问题"><a href="#最大子序列与最大子矩阵问题" class="headerlink" title="最大子序列与最大子矩阵问题"></a>最大子序列与最大子矩阵问题</h3><ul>
<li>数组的最大子序列问题：给定一个数组，其中元素有正，也有负，找出其中一个连续子序列，使和最大。 <ul>
<li>方案1：这个问题可以动态规划的思想解决。设 表示以第i个元素 结尾的最大子序列，那么显然 。基于这一点可以很快用代码实现。 </li>
</ul>
</li>
<li>最大子矩阵问题：给定一个矩阵（二维数组），其中数据有大有小，请找一个子矩阵，使得子矩阵的和最大，并输出这个和。 <ul>
<li>方案1：可以采用与最大子序列类似的思想来解决。如果我们确定了选择第i列和第j列之间的元素，那么在这个范围内，其实就是一个最大子序列问题。如何确定第i列和第j列可以词用暴搜的方法进行。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode-查找表</title>
    <url>/2021/12/10/LeetCode%20%E6%9F%A5%E6%89%BE%E8%A1%A8/</url>
    <content><![CDATA[<p>leetcode-查找表，忘了从哪里来的，侵删<br><a id="more"></a></p>
<h1 id="考虑的基本数据结构"><a href="#考虑的基本数据结构" class="headerlink" title="考虑的基本数据结构"></a>考虑的基本数据结构</h1><ul>
<li><strong>第一类： 查找有无—set</strong>：元素’a’是否存在，通常用set：集合。<ul>
<li>set只存储键，而不需要对应其相应的值。 </li>
<li>set中的键不允许重复</li>
</ul>
</li>
<li><strong>第二类： 查找对应关系(键值对应)—dict</strong>：元素’a’出现了几次：dict—&gt;字典<ul>
<li>dict中的键不允许重复</li>
</ul>
</li>
<li><strong>第三类： 改变映射关系—map</strong>:通过将原有序列的关系映射统一表示为其他</li>
</ul>
<h2 id="LeetCode-349-Intersection-Of-Two-Arrays-1"><a href="#LeetCode-349-Intersection-Of-Two-Arrays-1" class="headerlink" title="LeetCode 349 Intersection Of Two Arrays 1"></a>LeetCode 349 Intersection Of Two Arrays 1</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><ul>
<li>给定两个数组nums,求两个数组的公共元素。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如nums1 &#x3D; [1,2,2,1],nums2 &#x3D; [2,2]</span><br><span class="line">结果为[2]</span><br><span class="line">结果中每个元素只能出现一次</span><br><span class="line">出现的顺序可以是任意的</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="分析实现"><a href="#分析实现" class="headerlink" title="分析实现"></a>分析实现</h3><ul>
<li>由于每个元素只出现一次，因此不需要关注每个元素出现的次数，用set的数据结构就可以了。记录元素的有和无。</li>
<li>把nums1记录为set，判断nums2的元素是否在set中，是的话，就放在一个公共的set中，最后公共的set就是我们要的结果。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersection</span>(<span class="params">self, nums1: List[<span class="built_in">int</span>], nums2: List[<span class="built_in">int</span>]</span>) -&gt; List[int]:</span></span><br><span class="line">        nums1 = <span class="built_in">set</span>(nums1)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">set</span>([i <span class="keyword">for</span> i <span class="keyword">in</span> nums2 <span class="keyword">if</span> i <span class="keyword">in</span> nums1])</span><br></pre></td></tr></table></figure>
<ul>
<li>也可以通过set的内置方法来实现，直接求set的交集：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersection</span>(<span class="params">self, nums1: List[<span class="built_in">int</span>], nums2: List[<span class="built_in">int</span>]</span>) -&gt; List[int]:</span></span><br><span class="line">        set1 = <span class="built_in">set</span>(nums1)</span><br><span class="line">        set2 = <span class="built_in">set</span>(nums2)</span><br><span class="line">        <span class="keyword">return</span> set2 &amp; set1</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="LeetCode-350-Intersection-Of-Two-Arrays-2"><a href="#LeetCode-350-Intersection-Of-Two-Arrays-2" class="headerlink" title="LeetCode 350 Intersection Of Two Arrays 2"></a>LeetCode 350 Intersection Of Two Arrays 2</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定两个数组nums,求两个数组的交集。</p>
<p>— 如nums1=[1,2,2,1],nums=[2,2]</p>
<p>— 结果为[2,2]</p>
<p>— 出现的顺序可以是任意的</p>
<h3 id="分析实现-1"><a href="#分析实现-1" class="headerlink" title="分析实现"></a>分析实现</h3><p>元素出现的次数有用，那么对于存储次数就是有意义的，所以选择数据结构时，就应该选择dict的结构，通过字典的比较来判断；</p>
<p>记录每个元素的同时要记录这个元素的频次。</p>
<p>记录num1的字典，遍历nums2，比较nums1的字典的nums的key是否大于零，从而进行判断。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intersect</span>(<span class="params">self, nums1: List[<span class="built_in">int</span>], nums2: List[<span class="built_in">int</span>]</span>) -&gt; List[int]:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        nums1_dict = Counter(nums1)</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums2:</span><br><span class="line">            <span class="keyword">if</span> nums1_dict[num] &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 说明找到了一个元素即在num1也在nums2</span></span><br><span class="line">                res.append(num)</span><br><span class="line">                nums1_dict[num] -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res        </span><br></pre></td></tr></table></figure>
<h2 id="LeetCode-242-Intersection-Of-Two-Arrays-2"><a href="#LeetCode-242-Intersection-Of-Two-Arrays-2" class="headerlink" title="LeetCode 242 Intersection Of Two Arrays 2"></a>LeetCode 242 Intersection Of Two Arrays 2</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。</p>
<p>示例1:</p>
<p>输入: s = “anagram”, t = “nagaram”<br>输出: true</p>
<p>示例 2:</p>
<p>输入: s = “rat”, t = “car”<br>输出: false</p>
<h3 id="分析实现-2"><a href="#分析实现-2" class="headerlink" title="分析实现"></a>分析实现</h3><p>判断异位词即判断变换位置后的字符串和原来是否相同，那么不仅需要存储元素，还需要记录元素的个数。可以选择dict的数据结构，将字符串s和t都用dict存储，而后直接比较两个dict是否相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isAnagram</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; bool:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        s = Counter(s)</span><br><span class="line">        t = Counter(t)</span><br><span class="line">        <span class="keyword">if</span> s == t:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="LeetCode-202-Happy-number"><a href="#LeetCode-202-Happy-number" class="headerlink" title="LeetCode 202 Happy number"></a>LeetCode 202 Happy number</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>编写一个算法来判断一个数是不是“快乐数”。</p>
<p>一个“快乐数”定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是无限循环但始终变不到 1。如果可以变为 1，那么这个数就是快乐数。</p>
<p>示例: </p>
<p>输入: 19</p>
<p>输出: true</p>
<p>解释:<br>1^2 + 9^2 = 82<br>8^2 + 2^2 = 68<br>6^2 + 8^2 = 100<br>1^2 + 0^2 + 0^2 = 1</p>
<h3 id="分析实现-3"><a href="#分析实现-3" class="headerlink" title="分析实现"></a>分析实现</h3><p>这道题目思路很明显，当n不等于1时就循环，每次循环时，将其最后一位到第一位的数依次平方求和，比较求和是否为1。</p>
<p>难点在于，什么时候跳出循环？</p>
<p>开始笔者的思路是，循环个100次，还没得出结果就false，但是小学在算无限循环小数时有一个特征，就是当除的数中，和之前历史的得到的数有重合时，这时就是无限循环小数。</p>
<p>那么这里也可以按此判断，因为只需要判断有或无，不需要记录次数，故用set的数据结构。每次对求和的数进行append，当新一次求和的值存在于set中时，就return false.</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isHappy</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        already = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">while</span> n != <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> n &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 取n的最后一位数</span></span><br><span class="line">                tmp = n % <span class="number">10</span>   </span><br><span class="line">                <span class="built_in">sum</span> += tmp ** <span class="number">2</span></span><br><span class="line">                <span class="comment"># 将n的最后一位截掉</span></span><br><span class="line">                n //= <span class="number">10</span></span><br><span class="line">            <span class="comment"># 如果求的和在过程中出现过</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span> <span class="keyword">in</span> already:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                already.add(<span class="built_in">sum</span>)</span><br><span class="line">            n = <span class="built_in">sum</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h3 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一般对多位数计算的套路是：</span></span><br><span class="line"><span class="comment">#循环从后向前取位数</span></span><br><span class="line"><span class="keyword">while</span> n &gt;<span class="number">0</span> :</span><br><span class="line"><span class="comment">#取最后一位： </span></span><br><span class="line">tmp = n % <span class="number">10</span></span><br><span class="line"><span class="comment">#再截掉最后一位：</span></span><br><span class="line">n = n // <span class="number">10</span></span><br></pre></td></tr></table></figure>
<h2 id="LeetCode-290-Word-Pattern"><a href="#LeetCode-290-Word-Pattern" class="headerlink" title="LeetCode 290 Word Pattern"></a>LeetCode 290 Word Pattern</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出一个模式(pattern)以及一个字符串，判断这个字符串是否符合模式</p>
<p>示例1:</p>
<p>输入: pattern = “abba”,<br>str = “dog cat cat dog”</p>
<p>输出: true</p>
<p>示例 2:</p>
<p>输入:pattern = “abba”,<br>str = “dog cat cat fish”</p>
<p>输出: false</p>
<p>示例 3:</p>
<p>输入: pattern = “aaaa”, str = “dog cat cat dog”</p>
<p>输出: false</p>
<p>示例 4:</p>
<p>输入: pattern = “abba”, str = “dog dog dog dog”</p>
<p>输出: false</p>
<h3 id="分析实现-4"><a href="#分析实现-4" class="headerlink" title="分析实现"></a>分析实现</h3><p>抓住变与不变，笔者开始的思路是选择了dict的数据结构，比较count值和dict对应的keys的个数是否相同，但是这样无法判断顺序的关系，如测试用例：’aba’,’cat cat dog’。</p>
<p>那么如何能<strong>既考虑顺序</strong>，也考虑<strong>键值对应的关系</strong>呢？</p>
<p>抓住变与不变，变的是键，但是不变的是各个字典中，对应的相同index下的值，如dict1[index] = dict2[index]，那么我们可以创建两个新的字典，遍历index对两个新的字典赋值，并比较value。</p>
<p>还有一个思路比较巧妙，既然不同，那么可以考虑怎么让它们相同，将原来的dict通过map映射为相同的key，再比较相同key的dict是否相同。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wordPattern</span>(<span class="params">self,pattern, <span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="built_in">str</span> = <span class="built_in">str</span>.split()</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(pattern.index,pattern)) == <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>.index,<span class="built_in">str</span>))</span><br></pre></td></tr></table></figure>
<h3 id="tips-1"><a href="#tips-1" class="headerlink" title="tips"></a>tips</h3><ol>
<li><p>因为str是字符串，不是由单个字符组成，所以开始需要根据空格拆成字符list：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">str</span> = <span class="built_in">str</span>.split()</span><br></pre></td></tr></table></figure>
</li>
<li><p>通过map将字典映射为index的list:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">map</span>(pattern.index, pattern)</span><br></pre></td></tr></table></figure></li>
<li>map是通过hash存储的，不能直接进行比较，需要转换为list比较list</li>
</ol>
<h2 id="LeetCode-205-Isomorphic-Strings"><a href="#LeetCode-205-Isomorphic-Strings" class="headerlink" title="LeetCode 205 Isomorphic Strings"></a>LeetCode 205 Isomorphic Strings</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定两个字符串 s 和 t，判断它们是否是同构的。</p>
<p>如果 s 中的字符可以被替换得到 t ，那么这两个字符串是同构的。</p>
<p>所有出现的字符都必须用另一个字符替换，同时保留字符的顺序。两个字符不能映射到同一个字符上，但字符可以映射自己本身。</p>
<p>示例 1:</p>
<p>输入: s = “egg”, t = “add”</p>
<p>输出: true</p>
<p>示例 2:</p>
<p>输入: s = “foo”, t = “bar”</p>
<p>输出: false</p>
<p>示例 3:</p>
<p>输入: s = “paper”, t = “title”</p>
<p>输出: true</p>
<h3 id="分析实现-5"><a href="#分析实现-5" class="headerlink" title="分析实现"></a>分析实现</h3><p>思路与上题一致，可以考虑通过建两个dict，比较怎样不同，也可以将不同转化为相同。</p>
<p>直接用上题的套路代码：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isIsomorphic</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; bool:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">map</span>(s.index,s)) == <span class="built_in">list</span>(<span class="built_in">map</span>(t.index,t))</span><br></pre></td></tr></table></figure></p>
<h2 id="LeetCode-451-Sort-Characters-By-Frequency"><a href="#LeetCode-451-Sort-Characters-By-Frequency" class="headerlink" title="LeetCode 451 Sort Characters By Frequency"></a>LeetCode 451 Sort Characters By Frequency</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个字符串，请将字符串里的字符按照出现的频率降序排列。</p>
<p>示例 1:</p>
<p>输入:<br>“tree”</p>
<p>输出:<br>“eert”</p>
<p>示例 2:</p>
<p>输入:<br>“cccaaa”</p>
<p>输出:<br>“cccaaa”</p>
<p>示例 3:</p>
<p>输入:<br>“Aabb”</p>
<p>输出:<br>“bbAa”</p>
<h3 id="分析实现-6"><a href="#分析实现-6" class="headerlink" title="分析实现"></a>分析实现</h3><p>对于相同频次的字母，顺序任意，需要考虑大小写，返回的是字符串。</p>
<p>使用字典统计频率，对字典的value进行排序，最终根据key的字符串乘上value次数，组合在一起输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">frequencySort</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; str:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        s_dict = Counter(s)</span><br><span class="line">        <span class="comment"># sorted返回的是列表元组</span></span><br><span class="line">        s = <span class="built_in">sorted</span>(s_dict.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse = <span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 因为返回的是字符串</span></span><br><span class="line">        res = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> s:</span><br><span class="line">            res += key * value   </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="tips-2"><a href="#tips-2" class="headerlink" title="tips"></a>tips</h3><ol>
<li>通过sorted的方法进行value排序，对字典排序后无法直接按照字典进行返回，返回的为列表元组：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对value值由大到小排序</span></span><br><span class="line">s = <span class="built_in">sorted</span>(s_dict.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>], reverse = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对key由小到大排序</span></span><br><span class="line">s = <span class="built_in">sorted</span>(s_dict.items(), key=<span class="keyword">lambda</span> item:item[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></li>
<li>输出为字符串的情况下，可以由字符串直接进行拼接:<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 由key和value相乘进行拼接</span></span><br><span class="line"><span class="string">&#x27;s&#x27;</span> * <span class="number">5</span> + <span class="string">&#x27;d&#x27;</span>*<span class="number">2</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="对撞指针"><a href="#对撞指针" class="headerlink" title="对撞指针"></a>对撞指针</h1><h2 id="LeetCode-1-Two-Sum"><a href="#LeetCode-1-Two-Sum" class="headerlink" title="LeetCode 1 Two Sum"></a>LeetCode 1 Two Sum</h2><h3 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出一个整型数组nums，返回这个数组中两个数字的索引值i和j，使得nums[i] + nums[j]等于一个给定的target值，两个索引不能相等。</p>
<p>如：nums= [2,7,11,15],target=9<br>返回[0,1]</p>
<h3 id="审题"><a href="#审题" class="headerlink" title="审题:"></a>审题:</h3><p>需要考虑：</p>
<ol>
<li>开始数组是否有序；</li>
<li>索引从0开始计算还是1开始计算？</li>
<li>没有解该怎么办？</li>
<li>有多个解怎么办？保证有唯一解。</li>
</ol>
<h3 id="分析实现-7"><a href="#分析实现-7" class="headerlink" title="分析实现"></a>分析实现</h3><h3 id="暴力法O-n-2"><a href="#暴力法O-n-2" class="headerlink" title="暴力法O(n^2)"></a>暴力法O(n^2)</h3><p>时间复杂度为O(n^2),第一遍遍历数组，第二遍遍历当前遍历值之后的元素，其和等于target则return。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        len_nums = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_nums):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,len_nums):</span><br><span class="line">                <span class="keyword">if</span> nums[i] + nums[j] == target:</span><br><span class="line">                    <span class="keyword">return</span> [i,j]</span><br></pre></td></tr></table></figure>
<h3 id="排序-指针对撞-O-n-O-nlogn-O-n"><a href="#排序-指针对撞-O-n-O-nlogn-O-n" class="headerlink" title="排序+指针对撞(O(n)+O(nlogn)=O(n))"></a>排序+指针对撞(O(n)+O(nlogn)=O(n))</h3><p>在数组篇的LeetCode 167题中，也遇到了找到两个数使得它们相加之和等于目标数，但那是对于排序的情况，因此也可以使用上述的思路来完成。</p>
<p>因为问题本身不是有序的，因此需要对原来的数组进行一次排序，排序后就可以用O(n)的指针对撞进行解决。</p>
<p>但是问题是，返回的是数字的索引，如果只是对数组的值进行排序，那么数组原来表示的索引的信息就会丢失，所以在排序前要进行些处理。</p>
<p><strong>错误代码示例—只使用dict来进行保存：</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        record = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            record[nums[index]] = index </span><br><span class="line">        nums.sort()</span><br><span class="line">        l,r = <span class="number">0</span>,<span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            <span class="keyword">if</span> nums[l] + nums[r] == target:</span><br><span class="line">                <span class="keyword">return</span> [record[nums[l]],record[nums[r]]]</span><br><span class="line">            <span class="keyword">elif</span> nums[l] + nums[r] &lt; target:</span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                r -= <span class="number">1</span></span><br></pre></td></tr></table></figure><br>当遇到<strong>相同的元素的索引</strong>问题时，会不满足条件：</p>
<p>如：[3,3]  6</p>
<p>在排序前先使用一个额外的数组<strong>拷贝</strong>一份原来的数组，对于两个相同元素的索引问题，使用一个<strong>bool型变量</strong>辅助将两个索引都找到，总的时间复杂度为O(n)+O(nlogn) = O(nlogn)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        record = <span class="built_in">dict</span>()</span><br><span class="line">        nums_copy = nums.copy()</span><br><span class="line">        sameFlag = <span class="literal">True</span>;</span><br><span class="line">        nums.sort()</span><br><span class="line">        l,r = <span class="number">0</span>,<span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            <span class="keyword">if</span> nums[l] + nums[r] == target:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">elif</span> nums[l] + nums[r] &lt; target:</span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                r -= <span class="number">1</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums_copy[i] == nums[l] <span class="keyword">and</span> sameFlag:</span><br><span class="line">                res.append(i)</span><br><span class="line">                sameFlag = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">elif</span> nums_copy[i] == nums[r]:</span><br><span class="line">                res.append(i)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h4 id="小套路"><a href="#小套路" class="headerlink" title="小套路:"></a>小套路:</h4><p>如果只是对数组的值进行排序，那么数组原来表示的索引的信息就会丢失的情况，可以在排序前：</p>
<h4 id="更加pythonic的实现"><a href="#更加pythonic的实现" class="headerlink" title="更加pythonic的实现"></a>更加pythonic的实现</h4><p>通过list(enumerate(nums))开始实现下标和值的绑定，不用专门的再copy加bool判断。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = <span class="built_in">list</span>(<span class="built_in">enumerate</span>(nums))</span><br><span class="line">nums.sort(key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line">i,j = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> i &lt; j:</span><br><span class="line">    <span class="keyword">if</span> nums[i][<span class="number">1</span>] + nums[j][<span class="number">1</span>] &gt; target:</span><br><span class="line">        j -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> nums[i][<span class="number">1</span>] + nums[j][<span class="number">1</span>] &lt; target:</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> nums[j][<span class="number">0</span>] &lt; nums[i][<span class="number">0</span>]:</span><br><span class="line">            nums[j],nums[i] = nums[i],nums[j]</span><br><span class="line">        <span class="keyword">return</span> num[i][<span class="number">0</span>],nums[j][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><strong>拷贝数组 + bool型变量辅助</strong></p>
<h3 id="查找表—O-n"><a href="#查找表—O-n" class="headerlink" title="查找表—O(n)"></a>查找表—O(n)</h3><p>遍历数组过程中，当遍历到元素v时，可以只看v前面的元素，是否含有target-v的元素存在。</p>
<ol>
<li>如果查找成功，就返回解；</li>
<li>如果没有查找成功，就把v放在查找表中，继续查找下一个解。</li>
</ol>
<p>即使v放在了之前的查找表中覆盖了v，也不影响当前v元素的查找。因为只需要找到两个元素，只需要找target-v的另一个元素即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        record = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            complement = target - nums[i]</span><br><span class="line">            <span class="comment">## 已经在之前的字典中找到这个值</span></span><br><span class="line">            <span class="keyword">if</span> record.get(complement) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                res = [i,record[complement]]</span><br><span class="line">                <span class="keyword">return</span> res</span><br><span class="line">            record[nums[i]] = i</span><br></pre></td></tr></table></figure>
<p>只进行一次循环，故时间复杂度O(n),空间复杂度为O(n)</p>
<h3 id="补充思路："><a href="#补充思路：" class="headerlink" title="补充思路："></a>补充思路：</h3><p>通过enumerate来把索引和值进行绑定，进而对value进行sort，前后对撞指针进行返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[int]:</span></span><br><span class="line">        nums = <span class="built_in">list</span>(<span class="built_in">enumerate</span>(nums))</span><br><span class="line">        <span class="comment">## 根据value来排序</span></span><br><span class="line">        nums.sort(key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line">        l,r = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            <span class="keyword">if</span> nums[l][<span class="number">1</span>] + nums[r][<span class="number">1</span>] == target:</span><br><span class="line">                <span class="keyword">return</span> nums[l][<span class="number">0</span>],nums[r][<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">elif</span> nums[l][<span class="number">1</span>] + nums[r][<span class="number">1</span>] &lt; target:</span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                r -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="LeetCode-15-3Sum"><a href="#LeetCode-15-3Sum" class="headerlink" title="LeetCode 15 3Sum"></a>LeetCode 15 3Sum</h2><h3 id="题目描述-8"><a href="#题目描述-8" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出一个整型数组，寻找其中的所有不同的三元组(a,b,c)，使得a+b+c=0</p>
<p>注意：答案中不可以包含重复的三元组。</p>
<p>如：nums = [-1, 0, 1, 2, -1, -4]，</p>
<p>结果为：[[-1, 0, 1],[-1, -1, 2]]</p>
<h3 id="审题-1"><a href="#审题-1" class="headerlink" title="审题"></a>审题</h3><ol>
<li>数组不是有序的；</li>
<li>返回结果为全部解，多个解的顺序是否需要考虑？—不需要考虑顺序</li>
<li>什么叫不同的三元组？索引不同即不同，还是值不同？—题目定义的是，值不同才为不同的三元组</li>
<li>没有解时怎么返回？—空列表</li>
</ol>
<h3 id="分析实现-8"><a href="#分析实现-8" class="headerlink" title="分析实现"></a>分析实现</h3><p>因为上篇中已经实现了Two Sum的问题，因此对于3Sum，首先想到的思路就是，开始固定一个k，然后在其后都当成two sum问题来进行解决，但是这样就ok了吗？</p>
<h4 id="没有考虑重复元素导致错误"><a href="#没有考虑重复元素导致错误" class="headerlink" title="没有考虑重复元素导致错误"></a>没有考虑重复元素导致错误</h4><p>直接使用Two Sum问题中的查找表的解法，根据第一层遍历的i，将i之后的数组作为two sum问题进行解决。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSum</span>(<span class="params">self, nums: [<span class="built_in">int</span>]</span>) -&gt; [[int]]:</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            num = <span class="number">0</span> - nums[i]</span><br><span class="line">            record = <span class="built_in">dict</span>()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">                complement = num - nums[j]</span><br><span class="line">                <span class="comment"># 已经在之前的字典中找到这个值</span></span><br><span class="line">                <span class="keyword">if</span> record.get(complement) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    res_lis = [nums[i], nums[j], complement]</span><br><span class="line">                    res.append(res_lis)</span><br><span class="line">                record[nums[j]] = i</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>但是这样会导致一个错误，错误用例如下:</p>
<p>输入：<br>[-1,0,1,2,-1,-4]</p>
<p>输出：<br>[[-1,1,0],[-1,-1,2],[0,-1,1]]</p>
<p>预期结果：<br>[[-1,-1,2],[-1,0,1]]</p>
<p>代码在实现的过程中没有把第一次遍历的i的索引指向相同元素的情况排除掉，于是出现了当i指针后面位置的元素有和之前访问过的相同的值，于是重复遍历。</p>
<p>那么可以考虑，开始时对nums数组进行排序，排序后，当第一次遍历的指针k遇到下一个和前一个指向的值重复时，就将其跳过。为了方便计算，在第二层循环中，可以使用<strong>对撞指针</strong>的套路：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对撞指针套路</span></span><br><span class="line">l,r = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> l &lt; r:</span><br><span class="line">    <span class="keyword">if</span> nums[l] + nums[r] == target:</span><br><span class="line">        <span class="keyword">return</span> nums[l],nums[r]</span><br><span class="line">    <span class="keyword">elif</span> nums[l] + nums[r] &lt; target:</span><br><span class="line">        l += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        r -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>其中需要注意的是，在里层循环中，也要考虑重复值的情况，因此当值相等时，再次移动指针时，需要保证其指向的值和前一次指向的值不重复，因此可以：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对撞指针套路</span></span><br><span class="line">l,r = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> l &lt; r:</span><br><span class="line">    <span class="built_in">sum</span> = nums[i] + nums[l] + nums[r]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">sum</span> == target:</span><br><span class="line">        res.append([nums[i],nums[l],nums[r])</span><br><span class="line">        l += <span class="number">1</span></span><br><span class="line">        r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[l<span class="number">-1</span>]: l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[r] == nums[r+<span class="number">1</span>]: r -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; target:</span><br><span class="line">        l += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        r -= <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>再调整下遍历的范围，因为设了3个索引：i，l，r。边界情况下，r索引指向len-1, l指向len-2，索引i遍历的边界为len-3，故for循环是从0到len-2。</p>
<p>代码实现如下：</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSum</span>(<span class="params">self, nums: [<span class="built_in">int</span>]</span>) -&gt; [[int]]:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)<span class="number">-2</span>):</span><br><span class="line">            <span class="comment"># 因为是排序好的数组，如果最小的都大于0可以直接排除</span></span><br><span class="line">            <span class="keyword">if</span> nums[i] &gt; <span class="number">0</span>: <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 排除i的重复值</span></span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line">            l,r = i+<span class="number">1</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">            <span class="keyword">while</span> l &lt; r:</span><br><span class="line">                <span class="built_in">sum</span> = nums[i] + nums[l] + nums[r]</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">sum</span> == <span class="number">0</span>:</span><br><span class="line">                    res.append([nums[i],nums[l],nums[r]])</span><br><span class="line">                    l += <span class="number">1</span></span><br><span class="line">                    r -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[l<span class="number">-1</span>]: l += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[r] == nums[r+<span class="number">1</span>]: r -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; <span class="number">0</span>:</span><br><span class="line">                    l += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h3 id="小套路-1"><a href="#小套路-1" class="headerlink" title="小套路"></a>小套路</h3><ol>
<li>采用<strong>for + while</strong>的形式来处理三索引；</li>
<li>当数组不是有序时需要注意，有序的特点在哪里，有序就可以用哪些方法解决？无序的话不便在哪里？</li>
<li>对撞指针套路：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对撞指针套路</span></span><br><span class="line">l,r = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> l &lt; r:</span><br><span class="line">    <span class="keyword">if</span> nums[l] + nums[r] == target:</span><br><span class="line">        <span class="keyword">return</span> nums[l],nums[r]</span><br><span class="line">    <span class="keyword">elif</span> nums[l] + nums[r] &lt; target:</span><br><span class="line">        l += <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        r -= <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
<li>处理重复值的套路：先转换为有序数组，再循环判断其与上一次值是否重复：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line"><span class="comment"># 2.</span></span><br><span class="line"><span class="keyword">while</span> l &lt; r:</span><br><span class="line">    <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[l<span class="number">-1</span>]: l += <span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="LeetCode-18-4Sum"><a href="#LeetCode-18-4Sum" class="headerlink" title="LeetCode 18 4Sum"></a>LeetCode 18 4Sum</h2><h3 id="题目描述-9"><a href="#题目描述-9" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出一个整形数组，寻找其中的所有不同的四元组(a,b,c,d)，使得a+b+c+d等于一个给定的数字target。</p>
<p>—如:</p>
<p>nums = [1, 0, -1, 0, -2, 2]，target = 0</p>
<p>—结果为：</p>
<p>[[-1,  0, 0, 1],[-2, -1, 1, 2],[-2,  0, 0, 2]]</p>
<h3 id="题目分析"><a href="#题目分析" class="headerlink" title="题目分析"></a>题目分析</h3><p>4Sum可以当作是3Sum问题的扩展，注意事项仍是一样的，同样是不能返回重复值得解。首先排序。接着从[0,len-1]遍历i，跳过i的重复元素，再在[i+1,len-1]中遍历j，得到i，j后，再选择首尾的l和r，通过对撞指针的思路，四数和大的话r—，小的话l++,相等的话纳入结果list，最后返回。</p>
<p>套用3Sum得代码，在其前加一层循环，对边界情况进行改动即可:</p>
<ol>
<li>原来3个是到len-2,现在外层循环是到len-3;</li>
<li>在中间层得迭代中，当第二个遍历得值在第一个遍历得值之后且后项大于前项时，认定为重复；</li>
<li>加些边界条件判断：当len小于4时，直接返回；当只有4个值且长度等于target时，直接返回本身即可。</li>
</ol>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[List[int]]:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) &lt; <span class="number">4</span>: <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) == <span class="number">4</span> <span class="keyword">and</span> <span class="built_in">sum</span>(nums) == target:</span><br><span class="line">            res.append(nums)</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)<span class="number">-3</span>):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(nums)<span class="number">-2</span>):</span><br><span class="line">                <span class="keyword">if</span> j &gt; i+<span class="number">1</span> <span class="keyword">and</span> nums[j] == nums[j<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line">                l,r = j+<span class="number">1</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">                <span class="keyword">while</span> l &lt; r:</span><br><span class="line">                    sum_value = nums[i] + nums[j] + nums[l] + nums[r]</span><br><span class="line">                    <span class="keyword">if</span> sum_value == target:</span><br><span class="line">                        res.append([nums[i],nums[j],nums[l],nums[r]])</span><br><span class="line">                        l += <span class="number">1</span></span><br><span class="line">                        r -= <span class="number">1</span></span><br><span class="line">                        <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[l] == nums[l<span class="number">-1</span>]: l += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> nums[r] == nums[r+<span class="number">1</span>]: r -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">elif</span> sum_value &lt; target:</span><br><span class="line">                        l += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>还可以使用combinations(nums, 4)来对原数组中得4个元素全排列，在开始sort后，对排列得到得元素进行set去重。但单纯利用combinations实现会超时。</p>
<h3 id="超出时间限制"><a href="#超出时间限制" class="headerlink" title="超出时间限制"></a>超出时间限制</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSum</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; List[List[int]]:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        <span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> combinations(nums, <span class="number">4</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span>(i) == target:</span><br><span class="line">                res.append(i)</span><br><span class="line">        res = <span class="built_in">set</span>(res)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">                </span><br></pre></td></tr></table></figure>
<h2 id="LeetCode-16-3Sum-Closest"><a href="#LeetCode-16-3Sum-Closest" class="headerlink" title="LeetCode 16 3Sum Closest"></a>LeetCode 16 3Sum Closest</h2><h3 id="题目描述-10"><a href="#题目描述-10" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出一个整形数组，寻找其中的三个元素a,b,c，使得a+b+c的值最接近另外一个给定的数字target。</p>
<p>如：给定数组 nums = [-1，2，1，-4], 和 target = 1.</p>
<p>与 target 最接近的三个数的和为 2. (-1 + 2 + 1 = 2).</p>
<h3 id="分析实现-9"><a href="#分析实现-9" class="headerlink" title="分析实现"></a>分析实现</h3><p>这道题也是2sum,3sum等题组中的，只不过变形的地方在于不是找相等的target，而是找最近的。</p>
<p>那么开始时可以随机设定一个三个数的和为结果值，在每次比较中，先判断三个数的和是否和target相等，如果相等直接返回和。如果不相等，则判断三个数的和与target的差是否小于这个结果值时，如果小于则进行则进行替换，并保存和的结果值。</p>
<h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先排序</span></span><br><span class="line">nums.sort()</span><br><span class="line"><span class="comment"># 随机选择一个和作为结果值</span></span><br><span class="line">res = nums[<span class="number">0</span>] + nums[<span class="number">1</span>] + nums[<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 记录这个差值</span></span><br><span class="line">diff = <span class="built_in">abs</span>(nums[<span class="number">0</span>]+nums[<span class="number">1</span>]+nums[<span class="number">2</span>]-target)</span><br><span class="line"><span class="comment"># 第一遍遍历</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="comment"># 标记好剩余元素的l和r</span></span><br><span class="line">    l,r = i+<span class="number">1</span>, <span class="built_in">len</span>(nums<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">while</span> l &lt; r:</span><br><span class="line">        <span class="keyword">if</span> 后续的值等于target:</span><br><span class="line">            <span class="keyword">return</span> 三个数值得和</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> 差值小于diff:</span><br><span class="line">                更新diff值</span><br><span class="line">                更新res值</span><br><span class="line">            <span class="keyword">if</span> 和小于target:</span><br><span class="line">                将l移动</span><br><span class="line">            <span class="keyword">else</span>:(开始已经排除了等于得情况，要判断和大于target)</span><br><span class="line">                将r移动</span><br></pre></td></tr></table></figure>
<h4 id="3Sum问题两层遍历得套路代码："><a href="#3Sum问题两层遍历得套路代码：" class="headerlink" title="3Sum问题两层遍历得套路代码："></a>3Sum问题两层遍历得套路代码：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums.sort()</span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)<span class="number">-2</span>):</span><br><span class="line">    l,r = i+<span class="number">1</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> l &lt; r:</span><br><span class="line">        <span class="built_in">sum</span> = nums[i] + nums[l] + nums[r]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span> == <span class="number">0</span>:</span><br><span class="line">            res.append([nums[i],nums[l],nums[r]])</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; <span class="number">0</span>:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            r -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSumClosest</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; int:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        diff = <span class="built_in">abs</span>(nums[<span class="number">0</span>]+nums[<span class="number">1</span>]+nums[<span class="number">2</span>]-target)</span><br><span class="line">        res = nums[<span class="number">0</span>] + nums[<span class="number">1</span>] + nums[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            l,r = i+<span class="number">1</span>,<span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">            t = target - nums[i]</span><br><span class="line">            <span class="keyword">while</span> l &lt; r:</span><br><span class="line">                <span class="keyword">if</span> nums[l] + nums[r] == t:</span><br><span class="line">                    <span class="keyword">return</span> nums[i] + t</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">abs</span>(nums[l]+nums[r]-t) &lt; diff:</span><br><span class="line">                        diff = <span class="built_in">abs</span>(nums[l]+nums[r]-t)</span><br><span class="line">                        res = nums[i]+nums[l]+nums[r]</span><br><span class="line">                    <span class="keyword">if</span> nums[l]+nums[r] &lt; t:</span><br><span class="line">                        l += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>时间复杂度为O(n^2)，空间复杂度为O(1);</p>
<h2 id="LeetCode-454-4SumⅡ"><a href="#LeetCode-454-4SumⅡ" class="headerlink" title="LeetCode 454 4SumⅡ"></a>LeetCode 454 4SumⅡ</h2><h3 id="题目描述-11"><a href="#题目描述-11" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出四个整形数组A,B,C,D,寻找有多少i,j,k,l的组合,使得A[i]+B[j]+C[k]+D[l]=0。其中,A,B,C,D中均含有相同的元素个数N，且0&lt;=N&lt;=500；</p>
<p>输入:</p>
<p>A = [ 1, 2]<br>B = [-2,-1]<br>C = [-1, 2]<br>D = [ 0, 2]</p>
<p>输出:2</p>
<h3 id="分析实现-10"><a href="#分析实现-10" class="headerlink" title="分析实现"></a>分析实现</h3><p>这个问题同样是Sum类问题得变种，其将同一个数组的条件，变为了四个数组中，依然可以用查找表的思想来实现。</p>
<p>首先可以考虑把D数组中的元素都放入查找表，然后遍历前三个数组，判断target减去每个元素后的值是否在查找表中存在，存在的话，把结果值加1。那么查找表的数据结构选择用set还是dict？考虑到数组中可能存在重复的元素，而重复的元素属于不同的情况，因此用dict存储，最后的结果值加上dict相应key的value，代码如下：</p>
<h4 id="O-n-3-代码"><a href="#O-n-3-代码" class="headerlink" title="O(n^3)代码"></a>O(n^3)代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">record = Counter()</span><br><span class="line"><span class="comment"># 先建立数组D的查找表</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(D)):</span><br><span class="line">    record[D[i]] += <span class="number">1</span></span><br><span class="line">res = <span class="number">0</span> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(B)):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C)):</span><br><span class="line">            num_find = <span class="number">0</span>-A[i]-B[j]-C[k]</span><br><span class="line">            <span class="keyword">if</span> record.get(num_find) != <span class="literal">None</span>:</span><br><span class="line">                res += record(num_find)</span><br><span class="line"><span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>但是对于题目中给出的数据规模：N&lt;=500，如果N为500时，n^3的算法依然消耗很大，能否再进行优化呢？</p>
<p>根据之前的思路继续往前走，如果只遍历两个数组，那么就可以得到O(n^2)级别的算法，但是遍历两个数组，那么还剩下C和D两个数组，上面的值怎么放？</p>
<p>对于查找表问题而言，<strong>很多时候到底要查找什么</strong>，是解决的关键。对于C和D的数组，可以通过dict来记录其中和的个数，之后遍历结果在和中进行查找。代码如下：</p>
<h4 id="O-n-2-级代码"><a href="#O-n-2-级代码" class="headerlink" title="O(n^2)级代码"></a>O(n^2)级代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSumCount</span>(<span class="params">self, A: List[<span class="built_in">int</span>], B: List[<span class="built_in">int</span>], C: List[<span class="built_in">int</span>], D: List[<span class="built_in">int</span>]</span>) -&gt; int:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        record = Counter()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(A)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(B)):</span><br><span class="line">                record[A[i]+B[j]] += <span class="number">1</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(C)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(D)):</span><br><span class="line">                find_num = <span class="number">0</span> - C[i] - D[j]</span><br><span class="line">                <span class="keyword">if</span> record.get(find_num) != <span class="literal">None</span>:</span><br><span class="line">                    res += record[find_num]</span><br><span class="line">        <span class="keyword">return</span> res   </span><br></pre></td></tr></table></figure>
<p>再使用Pythonic的列表生成式和sum函数进行优化，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSumCount</span>(<span class="params">self, A: List[<span class="built_in">int</span>], B: List[<span class="built_in">int</span>], C: List[<span class="built_in">int</span>], D: List[<span class="built_in">int</span>]</span>) -&gt; int:</span></span><br><span class="line">        record = collections.Counter(a + b <span class="keyword">for</span> a <span class="keyword">in</span> A <span class="keyword">for</span> b <span class="keyword">in</span> B)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(record.get(- c - d, <span class="number">0</span>) <span class="keyword">for</span> c <span class="keyword">in</span> C <span class="keyword">for</span> d <span class="keyword">in</span> D)</span><br></pre></td></tr></table></figure></p>
<h2 id="LeetCode-49-Group-Anagrams"><a href="#LeetCode-49-Group-Anagrams" class="headerlink" title="LeetCode 49 Group Anagrams"></a>LeetCode 49 Group Anagrams</h2><h3 id="题目描述-12"><a href="#题目描述-12" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出一个字符串数组，将其中所有可以通过颠倒字符顺序产生相同结果的单词进行分组。</p>
<p>示例:</p>
<p>输入: [“eat”, “tea”, “tan”, “ate”, “nat”, “bat”],</p>
<p>输出:[[“ate”,”eat”,”tea”],[“nat”,”tan”],[“bat”]]</p>
<p>说明：</p>
<p>所有输入均为小写字母。<br>不考虑答案输出的顺序。</p>
<h3 id="分析实现-11"><a href="#分析实现-11" class="headerlink" title="分析实现"></a>分析实现</h3><p>在之前LeetCode 242的问题中，对字符串t和s来判断，判断t是否是s的字母异位词。当时的方法是通过构建t和s的字典，比较字典是否相同来判断是否为异位词。</p>
<p>在刚开始解决这个问题时，我也局限于了这个思路，以为是通过移动指针，来依次比较两个字符串是否对应的字典相等，进而确定异位词列表，再把异位词列表添加到结果集res中。于是有：</p>
<h4 id="错误思路"><a href="#错误思路" class="headerlink" title="错误思路"></a>错误思路</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = [<span class="string">&quot;eat&quot;</span>, <span class="string">&quot;tea&quot;</span>, <span class="string">&quot;tan&quot;</span>, <span class="string">&quot;ate&quot;</span>, <span class="string">&quot;nat&quot;</span>, <span class="string">&quot;bat&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">cum = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    l,r = i+<span class="number">1</span>,<span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    i_dict = Counter(nums[i])</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">if</span> nums[i] <span class="keyword">not</span> <span class="keyword">in</span> cum:</span><br><span class="line">        res.append(nums[i])</span><br><span class="line">    <span class="keyword">while</span> l &lt; r:</span><br><span class="line">        l_dict = Counter(nums[l])</span><br><span class="line">        r_dict = Counter(nums[r])</span><br><span class="line">        <span class="keyword">if</span> i_dict == l_dict <span class="keyword">and</span> l_dict == r_dict:</span><br><span class="line">            res.append(nums[l],nums[r])</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">            r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> i_dict == l_dict:</span><br><span class="line">            res.append(nums[l])</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> i_dict == r_dict:</span><br><span class="line">            res.append(nums[r])</span><br><span class="line">            r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">    print(res)</span><br><span class="line">    cum.append(res)</span><br><span class="line">......................................</span><br></pre></td></tr></table></figure>
<p>这时发现长长绵绵考虑不完，而且还要注意指针的条件，怎样遍历才能遍历所有的情况且判断列表是否相互间包含。。。</p>
<p>于是立即开始反思是否哪块考虑错了?回顾第一开始的选择数据结构，在dict和list中，自己错误的选择了list来当作数据结构，进而用指针移动来判断元素的情况。而<strong>没有利用题目中不变的条件</strong>。</p>
<p>题目的意思，对异位词的进行分组，同异位词的分为一组，那么考虑对这一组内什么是相同的，且这个相同的也能作为不同组的判断条件。</p>
<p>不同组的判断条件，就可以用数据结构dict中的key来代表，那么什么相同的适合当作key呢？</p>
<p>这时回顾下下LeetCode 242，当时是因为异位字符串中包含的<strong>字符串的字母个数</strong>都是相同的，故把字母当作key来进行判断是否为异位词。</p>
<p>但是对于本题，把每个字符串的字母dict，再当作字符串数组的dict的key，显然不太合适，那么对于异位词，还有什么是相同的？</p>
<p>显然，如果将字符串统一排序，<strong>异位词排序后的字符串</strong>，显然都是相同的。那么就可以把其当作key，把遍历的数组中的异位词当作value，对字典进行赋值，进而遍历字典的value，得到结果list。</p>
<p>需要注意的细节是，<strong>字符串和list之间的转换</strong>：</p>
<ol>
<li>默认构造字典需为list的字典；</li>
<li>排序使用sorted()函数，而不用list.sort()方法，因为其不返回值；</li>
<li>通过’’.join(list)，将list转换为字符串；</li>
<li>通过str.split(‘,’)将字符串整个转换为list中的一项；</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">groupAnagrams</span>(<span class="params">self, strs: List[<span class="built_in">str</span>]</span>) -&gt; List[List[str]]:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        strs_dict = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">str</span> <span class="keyword">in</span> strs:</span><br><span class="line">            key = <span class="string">&#x27;&#x27;</span>.join(<span class="built_in">sorted</span>(<span class="built_in">list</span>(<span class="built_in">str</span>)))</span><br><span class="line">            strs_dict[key] += <span class="built_in">str</span>.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> strs_dict.values():</span><br><span class="line">            res.append(v)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>再将能用列表生成式替换的地方替换掉,代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">groupAnagrams</span>(<span class="params">self, strs: List[<span class="built_in">str</span>]</span>) -&gt; List[List[str]]:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        strs_dict = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">str</span> <span class="keyword">in</span> strs:</span><br><span class="line">            key = <span class="string">&#x27;&#x27;</span>.join(<span class="built_in">sorted</span>(<span class="built_in">list</span>(<span class="built_in">str</span>)))</span><br><span class="line">            strs_dict[key] += <span class="built_in">str</span>.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> [v <span class="keyword">for</span> v <span class="keyword">in</span> strs_dict.values()]</span><br></pre></td></tr></table></figure>
<h2 id="LeetCode-447-Number-of-Boomerangs"><a href="#LeetCode-447-Number-of-Boomerangs" class="headerlink" title="LeetCode 447 Number of Boomerangs"></a>LeetCode 447 Number of Boomerangs</h2><h3 id="题目描述-13"><a href="#题目描述-13" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出一个平面上的n个点，寻找存在多少个由这些点构成的三元组(i,j,k)，<strong>使得i,j两点的距离等于i,k两点的距离</strong>。</p>
<p>其中n最多为500,且所有的点坐标的范围在[-10000,10000]之间。</p>
<p>输入:</p>
<p>[[0,0],[1,0],[2,0]]</p>
<p>输出:</p>
<p>2</p>
<p>解释:</p>
<p>两个结果为： [[1,0],[0,0],[2,0]] 和 [[1,0],[2,0],[0,0]]</p>
<h3 id="分析实现-12"><a href="#分析实现-12" class="headerlink" title="分析实现"></a>分析实现</h3><h4 id="原始思路"><a href="#原始思路" class="headerlink" title="原始思路"></a>原始思路</h4><p>题目的要求是：使得i,j两点的距离等于i,k两点的距离，那么相当于是比较三个点之间距离的，那么开始的思路就是三层遍历，i从0到len，j从i+1到len，k从j+1到len，然后比较三个点的距离，相等则结果数加一。</p>
<p>显然这样的时间复杂度为O(n^3)，对于这道题目，能否用查找表的思路进行解决优化？</p>
<h4 id="查找表"><a href="#查找表" class="headerlink" title="查找表"></a>查找表</h4><p>之前的查找表问题，大多是通过<strong>构建一个查找表</strong>，而避免了在查找中再内层嵌套循环，从而降低了时间复杂度。那么可以考虑在这道题中，可以通过查找表进行代替哪两层循环。</p>
<p>当i,j两点距离等于i,k时，用查找表的思路，等价于：对距离key(i,j或i,k的距离)，其值value(个数)为2。</p>
<p>那么就可以做一个查找表，用来查找相同距离key的个数value是多少。遍历每一个节点i，扫描得到其他点到节点i的距离，在查找表中，对应的键就是距离的值，对应的值就是距离值得个数。</p>
<p>在拿到对于元素i的距离查找表后，接下来就是排列选择问题了：</p>
<ol>
<li>如果当距离为x的值有2个时，那么选择j,k的可能情况有：第一次选择有2种，第二次选择有1种，为2*1；</li>
<li>如果当距离为x的值有3个时，那么选择j,k的可能的情况有：第一次选择有3种，第二次选择有2种，为3*2;</li>
<li>那么当距离为x的值有n个时，选择j,k的可能情况有：第一次选择有n种，第二次选择有n-1种。</li>
</ol>
<h4 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h4><p>对于距离值的求算，按照欧式距离的方法进行求算的话，容易产生浮点数，可以将根号去掉，用差的平方和来进行比较距离。</p>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numberOfBoomerangs</span>(<span class="params">self, points: List[List[<span class="built_in">int</span>]]</span>) -&gt; int:</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> points:</span><br><span class="line">            record = Counter()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> points:</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    record[self.dis(i,j)] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> k,v <span class="keyword">in</span> record.items():</span><br><span class="line">                res += v*(v<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dis</span>(<span class="params">self,point1,point2</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (point1[<span class="number">0</span>]-point2[<span class="number">0</span>]) ** <span class="number">2</span> + (point1[<span class="number">1</span>]-point2[<span class="number">1</span>]) ** <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>对实现的代码进行优化：</p>
<ol>
<li>将for循环遍历改为列表生成式;</li>
<li>对sum+=的操作，考虑使用sum函数。</li>
<li>对不同的函数使用闭包的方式内嵌；</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numberOfBoomerangs</span>(<span class="params">self, points: List[List[<span class="built_in">int</span>]]</span>) -&gt; int:</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x1, y1</span>):</span></span><br><span class="line">            <span class="comment"># 对一个i下j,k的距离值求和</span></span><br><span class="line">            d = Counter((x2 - x1) ** <span class="number">2</span> + (y2 - y1) ** <span class="number">2</span> <span class="keyword">for</span> x2, y2 <span class="keyword">in</span> points)</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">sum</span>(t * (t<span class="number">-1</span>) <span class="keyword">for</span> t <span class="keyword">in</span> d.values())</span><br><span class="line">        <span class="comment"># 对每个i的距离进行求和</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(f(x1, y1) <span class="keyword">for</span> x1, y1 <span class="keyword">in</span> points)</span><br></pre></td></tr></table></figure>
<h2 id="LeetCode-149-Max-Points-on-a-Line"><a href="#LeetCode-149-Max-Points-on-a-Line" class="headerlink" title="LeetCode 149 Max Points on a Line"></a>LeetCode 149 Max Points on a Line</h2><h3 id="题目描述-14"><a href="#题目描述-14" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个二维平面，平面上有 n 个点，求最多有多少个点在同一条直线上。</p>
<p>示例 1:</p>
<p>输入: [[1,1],[2,2],[3,3]]</p>
<p>输出: 3</p>
<p>示例 2:</p>
<p>输入: [[1,1],[3,2],[5,3],[4,1],[2,3],[1,4]]</p>
<p>输出: 4</p>
<h3 id="分析实现-13"><a href="#分析实现-13" class="headerlink" title="分析实现"></a>分析实现</h3><p>本道题目的要求是：看有多少个点在同一条直线上，那么判断点是否在一条直线上，其实就等价于判断i,j两点的斜率是否等于i,k两点的斜率。</p>
<p>回顾上道447题目中的要求：使得i,j两点的距离等于i,k两点的距离，那么在这里，直接考虑使用查找表实现，即<strong>查找相同斜率key的个数value是多少</strong>。</p>
<p>在上个问题中，i和j，j和i算是两种不同的情况，但是这道题目中，这是属于相同的两个点，<br>因此在对遍历每个i,查找与i相同斜率的点时，不能再对结果数res++，而应该取查找表中的最大值。如果有两个斜率相同时，返回的应该是3个点，故返回的是结果数+1。</p>
<p>查找表实现套路如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxPoints</span>(<span class="params">self,points</span>):</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(points)):</span><br><span class="line">            record = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(points)):</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    record[self.get_Slope(points,i,j)] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> record.values():</span><br><span class="line">                res = <span class="built_in">max</span>(res, v)</span><br><span class="line">        <span class="keyword">return</span> res + <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_Slope</span>(<span class="params">self,points,i,j</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (points[i][<span class="number">0</span>] - points[j][<span class="number">0</span>]) / (points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>但是这样会出现一个问题，即斜率的求算中，有时会出现直线为垂直的情况，故需要对返回的结果进行判断，如果分母为0，则返回inf，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_Slope</span>(<span class="params">self,points,i,j</span>):</span></span><br><span class="line">    <span class="keyword">if</span> points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> (points[i][<span class="number">0</span>] - points[j][<span class="number">0</span>]) / (points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>再次提交，发现对于空列表的测试用例会判断错误，于是对边界情况进行判断，如果初始长度小于等于1,则直接返回len：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(points) &lt;= <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(points)</span><br></pre></td></tr></table></figure></p>
<p>再次提交，对于相同元素的测试用例会出现错误，回想刚才的过程，当有相同元素时，题目的要求是算作两个不同的点，但是在程序运行时，会将其考虑为相同的点，return回了inf。但在实际运行时，需要对相同元素的情况单独考虑。</p>
<p>于是可以设定samepoint值，遍历时判断，如果相同时，same值++,最后取v+same的值作为结果数。</p>
<p>考虑到如果全是相同值，那么这时dict中的record为空，也要将same值当作结果数返回，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxPoints</span>(<span class="params">self,points</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(points) &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(points)</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(points)):</span><br><span class="line">            record = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">            samepoint = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(points)):</span><br><span class="line">                <span class="keyword">if</span> points[i][<span class="number">0</span>] == points[j][<span class="number">0</span>] <span class="keyword">and</span> points[i][<span class="number">1</span>] == points[j][<span class="number">1</span>]:</span><br><span class="line">                    samepoint += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    record[self.get_Slope(points,i,j)] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> record.values():</span><br><span class="line">                res = <span class="built_in">max</span>(res, v+samepoint)</span><br><span class="line">            res = <span class="built_in">max</span>(res, samepoint)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_Slope</span>(<span class="params">self,points,i,j</span>):</span></span><br><span class="line">        <span class="keyword">if</span> points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (points[i][<span class="number">0</span>] - points[j][<span class="number">0</span>]) / (points[i][<span class="number">1</span>] - points[j][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>时间复杂度为O(n^2)，空间复杂度为O(n)</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>遍历时多用索引，而不要直接用值进行遍历；</p>
<h1 id="滑动数组"><a href="#滑动数组" class="headerlink" title="滑动数组"></a>滑动数组</h1><h2 id="LeetCode-219-Contains-Dupliccate-Ⅱ"><a href="#LeetCode-219-Contains-Dupliccate-Ⅱ" class="headerlink" title="LeetCode 219 Contains Dupliccate Ⅱ"></a>LeetCode 219 Contains Dupliccate Ⅱ</h2><h3 id="题目描述-15"><a href="#题目描述-15" class="headerlink" title="题目描述"></a>题目描述</h3><p>给出一个整形数组nums和一个整数k，是否存在索引i和j，使得nums[i]==nums[j]，且i和J之间的差不超过k。</p>
<p>示例1:</p>
<p>输入: nums = [1,2,3,1], k = 3</p>
<p>输出: true</p>
<p>示例 2:</p>
<p>输入: nums = [1,2,3,1,2,3], k = 2</p>
<p>输出: false</p>
<h3 id="分析实现-14"><a href="#分析实现-14" class="headerlink" title="分析实现"></a>分析实现</h3><p>翻译下这个题目：在这个数组中，如果有两个元素索引i和j，它们对应的元素是相等的，且索引j-i是小于等于k，那么就返回True，否则返回False。</p>
<p>因为对于这道题目可以用暴力解法双层循环，即：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure></p>
<p>故这道题目可以考虑使用滑动数组来解决：</p>
<p>固定滑动数组的长度为K+1，当这个滑动数组内如果能找到两个元素的值相等，就可以保证两个元素的索引的差是小于等于k的。如果当前的滑动数组中没有元素相同，就右移滑动数组的右边界r,同时将左边界l右移。查看r++的元素是否在l右移过后的数组里，如果不在就将其添加数组，在的话返回true表示两元素相等。</p>
<p>因为滑动数组中的元素是不同的，考虑用set作为数据结构：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyDuplicate</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        record = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[i] <span class="keyword">in</span> record:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            record.add(nums[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(record) == k+<span class="number">1</span>:</span><br><span class="line">                record.remove(nums[i-k])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>时间复杂度为O(n)，空间复杂度为O(n)</p>
<h2 id="LeetCode-220-Contains-Dupliccate-Ⅲ"><a href="#LeetCode-220-Contains-Dupliccate-Ⅲ" class="headerlink" title="LeetCode 220 Contains Dupliccate Ⅲ"></a>LeetCode 220 Contains Dupliccate Ⅲ</h2><h3 id="题目描述-16"><a href="#题目描述-16" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个整数数组，判断数组中是否有两个不同的索引 i 和 j，使得nums [i] 和nums [j]的差的绝对值最大为 t，并且 i 和 j 之间的差的绝对值最大为 ķ。</p>
<p>示例 1:</p>
<p>输入: nums = [1,2,3,1], k = 3, t = 0</p>
<p>输出: true</p>
<p>示例 2:</p>
<p>输入: nums = [1,0,1,1], k = 1, t = 2</p>
<p>输出: true</p>
<p>示例 3:</p>
<p>输入: nums = [1,5,9,1,5,9], k = 2, t = 3</p>
<p>输出: false</p>
<h3 id="分析实现-15"><a href="#分析实现-15" class="headerlink" title="分析实现"></a>分析实现</h3><p>相比较上一个问题，这个问题多了一个限定条件，条件不仅索引差限定k，数值差也限定为了t。</p>
<p>将索引的差值固定，于是问题和上道一样，同样转化为了固定长度K+1的滑动窗口内，是否存在两个值的差距不超过 t，考虑使用<strong>滑动窗口</strong>的思想来解决。</p>
<p>在遍历的过程中，目的是要在“已经出现、但还未滑出滑动窗口”的所有数中查找，是否有一个数与滑动数组中的数的<strong>差的绝对值</strong>最大为 t。对于差的绝对值最大为t，实际上等价于所要找的这个元素v的范围是在v-t到v+t之间，即查找“滑动数组”中的元素有没有[v-t，v+t]范围内的数存在。</p>
<p>因为只需证明是否存在即可，这时判断的逻辑是：如果在滑动数组<strong>查找比v-t大的最小的元素</strong>,如果这个元素小于等于v+t,即可以证明存在[v-t,v+t]。</p>
<p>那么实现过程其实和上题是一致的，只是上题中的判断条件是<strong>在查找表中找到和nums[i]相同的元素</strong>，而这题中的判断条件是<strong>查找比v-t大的最小的元素，判断其小于等于v+t</strong>，下面是实现的框架：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyDuplicate</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        record = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> 查找的比v-t大的最小的元素 &lt;= v+t:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            record.add(nums[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(record) == k+<span class="number">1</span>:</span><br><span class="line">                record.remove(nums[i-k])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>接下来考虑，如何查找比v-t大的最小的元素呢？</p>
<p>【注：C++中有lower_bound(v-t)的实现，py需要自己写函数】</p>
<p>当然首先考虑可以通过O(n)的解法来完成，如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span>(<span class="params">self,array,v</span>):</span></span><br><span class="line">    array = <span class="built_in">list</span>(array)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(array)):</span><br><span class="line">        <span class="keyword">if</span> array[i] &gt;= v:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><br>但是滑动数组作为set，是有序的数组。对于有序的数组，应该第一反应就是<strong>二分查找</strong>，于是考虑二分查找实现，查找比v-t大的最小的元素：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span>(<span class="params">self, nums, target</span>):</span></span><br><span class="line">    low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> low&lt;high:</span><br><span class="line">        mid = <span class="built_in">int</span>((low+high)/<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid</span><br><span class="line">    <span class="keyword">return</span> low <span class="keyword">if</span> nums[low] &gt;= target <span class="keyword">else</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><br>整体代码实现如下，时间复杂度为O(nlogn),空间复杂度为O(n):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyAlmostDuplicate</span>(<span class="params">self, nums, k, t</span>) -&gt; bool:</span></span><br><span class="line">        record = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(record) != <span class="number">0</span>:</span><br><span class="line">                rec = <span class="built_in">list</span>(record)</span><br><span class="line">                find_index = self.lower_bound(rec,nums[i]-t)</span><br><span class="line">                <span class="keyword">if</span> find_index != <span class="number">-1</span> <span class="keyword">and</span> rec[find_index] &lt;= nums[i] + t:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            record.add(nums[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(record) == k + <span class="number">1</span>:</span><br><span class="line">                record.remove(nums[i - k])</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span>(<span class="params">self, nums, target</span>):</span></span><br><span class="line">        low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> low&lt;high:</span><br><span class="line">            mid = <span class="built_in">int</span>((low+high)/<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">                low = mid+<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid</span><br><span class="line">        <span class="keyword">return</span> low <span class="keyword">if</span> nums[low] &gt;= target <span class="keyword">else</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<p>当然。。。在和小伙伴一起刷的时候，这样写的O(n^2)的结果会比上面要高，讨论的原因应该是上面的步骤存在着大量set和list的转换导致，对于py，仍旧是考虑算法思想实现为主，下面是O(n^2)的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyAlmostDuplicate</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], k: <span class="built_in">int</span>, t: <span class="built_in">int</span></span>) -&gt; bool:</span></span><br><span class="line">        <span class="keyword">if</span> t == <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(nums) == <span class="built_in">len</span>(<span class="built_in">set</span>(nums)):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,k+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> i+j &gt;= <span class="built_in">len</span>(nums): <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(nums[i+j]-nums[i]) &lt;= t: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="小套路："><a href="#小套路：" class="headerlink" title="小套路："></a>小套路：</h3><p>二分查找实现，查找比v-t大的最小的元素：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span>(<span class="params">self, nums, target</span>):</span></span><br><span class="line">    low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> low&lt;high:</span><br><span class="line">        mid = <span class="built_in">int</span>((low+high)/<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">            low = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = mid</span><br><span class="line">    <span class="keyword">return</span> low <span class="keyword">if</span> nums[low] &gt;= target <span class="keyword">else</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<p>二分查找实现，查找比v-t大的最小的元素：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">upper_bound</span>(<span class="params">nums, target</span>):</span></span><br><span class="line">    low, high = <span class="number">0</span>, <span class="built_in">len</span>(nums)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> low&lt;high:</span><br><span class="line">        mid=(low+high)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid]&lt;=target:</span><br><span class="line">            low = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:<span class="comment">#&gt;</span></span><br><span class="line">            high = mid</span><br><span class="line">            pos = high</span><br><span class="line">    <span class="keyword">if</span> nums[low]&gt;target:</span><br><span class="line">        pos = low</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure></p>
<h1 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h1><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>查找在算法题中是很常见的，但是怎么最大化查找的效率和写出bugfree的代码才是难的部分。一般查找方法有顺序查找、二分查找和双指针，推荐一开始可以直接用顺序查找，如果遇到TLE的情况再考虑剩下的两种，毕竟AC是最重要的。</p>
<p>一般二分查找的对象是有序或者由有序部分变化的（可能暂时理解不了，看例题即可），但还存在一种可以运用的地方是按值二分查找，之后会介绍。</p>
<h3 id="代码模板"><a href="#代码模板" class="headerlink" title="代码模板"></a>代码模板</h3><p>总体来说二分查找是比较简单的算法，网上看到的写法也很多，掌握一种就可以了。<br>以下是我的写法，参考C++标准库里<algorithm>的写法。这种写法比较好的点在于：</p>
<ul>
<li>1.即使区间为空、答案不存在、有重复元素、搜索开/闭区间的上/下界也同样适用</li>
<li>2.+-1 的位置调整只出现了一次，而且最后返回lo还是hi都是对的，无需纠结</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">firstBadVersion</span>(<span class="params">self, arr</span>):</span></span><br><span class="line">        <span class="comment"># 第一点</span></span><br><span class="line">        lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(arr)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">            <span class="comment"># 第二点</span></span><br><span class="line">            mid = (lo+hi) // <span class="number">2</span></span><br><span class="line">            <span class="comment"># 第三点</span></span><br><span class="line">            <span class="keyword">if</span> f(x):</span><br><span class="line">                lo = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hi = mid</span><br><span class="line">        <span class="keyword">return</span> lo</span><br></pre></td></tr></table></figure>
<p><strong>解释</strong>：</p>
<ul>
<li>第一点：lo和hi分别对应搜索的上界和下界，但不一定为0和arr最后一个元素的下标。</li>
<li>第二点：因为Python没有溢出，int型不够了会自动改成long int型，所以无需担心。如果再苛求一点，可以把这一行改成<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mid = lo + (hi-lo) // <span class="number">2</span></span><br><span class="line"><span class="comment"># 之所以 //2 这部分不用位运算 &gt;&gt; 1 是因为会自动优化，效率不会提升</span></span><br></pre></td></tr></table></figure></li>
<li>第三点：<br>比较重要的就是这个f(x)，在带入模板的情况下，写对函数就完了。</li>
</ul>
<p>那么我们一步一步地揭开二分查找的神秘面纱，首先来一道简单的题。</p>
<h3 id="LeetCode-35-Search-Insert-Position"><a href="#LeetCode-35-Search-Insert-Position" class="headerlink" title="LeetCode 35. Search Insert Position"></a>LeetCode 35. Search Insert Position</h3><p>给定排序数组和目标值，如果找到目标，则返回索引。如果不是，则返回按顺序插入索引的位置的索引。 您可以假设数组中没有重复项。</p>
<p><strong>Example</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Example 1:</span><br><span class="line">Input: [1,3,5,6], 5</span><br><span class="line">Output: 2</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line">Input: [1,3,5,6], 2</span><br><span class="line">Output: 1</span><br><span class="line"></span><br><span class="line">Example 3:</span><br><span class="line">Input: [1,3,5,6], 7</span><br><span class="line">Output: 4</span><br><span class="line"></span><br><span class="line">Example 4:</span><br><span class="line">Input: [1,3,5,6], 0</span><br><span class="line">Output: 0</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong> 这里要注意的点是 high 要设置为 len(nums) 的原因是像第三个例子会超出数组的最大值，所以要让 lo 能到 这个下标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">searchInsert</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; int:</span>        </span><br><span class="line">        lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">            mid = (lo + hi) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt; target:</span><br><span class="line">                lo = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hi = mid</span><br><span class="line">        <span class="keyword">return</span> lo</span><br></pre></td></tr></table></figure>
<h3 id="LeetCode540-Single-Element-in-a-Sorted-Array"><a href="#LeetCode540-Single-Element-in-a-Sorted-Array" class="headerlink" title="LeetCode540. Single Element in a Sorted Array"></a>LeetCode540. Single Element in a Sorted Array</h3><p>您将获得一个仅由整数组成的排序数组，其中每个元素精确出现两次，但一个元素仅出现一次。 找到只出现一次的单个元素。</p>
<p><strong>Example</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: [1,1,2,3,3,4,4,8,8]</span><br><span class="line">Output: 2</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: [3,3,7,7,10,11,11]</span><br><span class="line">Output: 10</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong> 异或的巧妙应用！如果mid是偶数，那么和1异或的话，那么得到的是mid+1，如果mid是奇数，得到的是mid-1。如果相等的话，那么唯一的元素还在这之后，往后找就可以了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">singleNonDuplicate</span>(<span class="params">self, nums</span>):</span></span><br><span class="line">        lo, hi = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">            mid = (lo + hi) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == nums[mid ^ <span class="number">1</span>]:</span><br><span class="line">                lo = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hi = mid</span><br><span class="line">        <span class="keyword">return</span> nums[lo]</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>是不是还挺简单哈哈，那我们来道HARD难度的题!</strong></p>
<hr>
<h3 id="LeetCode-410-Split-Array-Largest-Sum"><a href="#LeetCode-410-Split-Array-Largest-Sum" class="headerlink" title="LeetCode 410. Split Array Largest Sum"></a>LeetCode 410. Split Array Largest Sum</h3><p>给定一个由非负整数和整数m组成的数组，您可以将该数组拆分为m个非空连续子数组。编写算法以最小化这m个子数组中的最大和。</p>
<p><strong>Example</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input:</span><br><span class="line">nums &#x3D; [7,2,5,10,8]</span><br><span class="line">m &#x3D; 2</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">18</span><br><span class="line"></span><br><span class="line">Explanation:</span><br><span class="line">There are four ways to split nums into two subarrays.</span><br><span class="line">The best way is to split it into [7,2,5] and [10,8],</span><br><span class="line">where the largest sum among the two subarrays is only 18.</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>这其实就是二分查找里的按值二分了，可以看出这里的元素就无序了。但是我们的目标是找到一个合适的最小和，换个角度理解我们要找的值在最小值max(nums)和sum(nums)内，而这两个值中间是连续的。是不是有点难理解，那么看代码吧</li>
<li>辅助函数的作用是判断当前的“最小和”的情况下，区间数是多少，来和m判断</li>
<li>这里的下界是数组的最大值是因为如果比最大值小那么一个区间就装不下，数组的上界是数组和因为区间最少是一个，没必要扩大搜索的范围</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">splitArray</span>(<span class="params">self, nums: List[<span class="built_in">int</span>], m: <span class="built_in">int</span></span>) -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span>(<span class="params">mid</span>):</span></span><br><span class="line">            res = tmp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">                <span class="keyword">if</span> tmp + num &lt;= mid:</span><br><span class="line">                    tmp += num</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res += <span class="number">1</span></span><br><span class="line">                    tmp = num</span><br><span class="line">            <span class="keyword">return</span> res + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        lo, hi = <span class="built_in">max</span>(nums), <span class="built_in">sum</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> lo &lt; hi:</span><br><span class="line">            mid = (lo + hi) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> helper(mid) &gt; m:</span><br><span class="line">                lo = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hi = mid</span><br><span class="line">        <span class="keyword">return</span> lo</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>查找表</tag>
      </tags>
  </entry>
  <entry>
    <title>tfidf_bm25</title>
    <url>/2022/03/18/tfidf-bm25/</url>
    <content><![CDATA[<p>tfidf_bm25<br><a id="more"></a><br><a href="https://my.oschina.net/stanleysun/blog/1617727">Ref</a></p>
<h1 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h1><ul>
<li>TF Score ＝ 某个词在文档中出现的次数 ／ 文档的长度</li>
<li>IDF = log(N/n)</li>
<li>Lucence中的TF-IDF: simlarity = log(numDocs / (docFreq + 1)) <em> sqrt(tf) </em> (1/sqrt(length))<ul>
<li>numDocs:索引中文档数量，对应前文中的N。lucence不是(也不可能)把整个互联网的文档作为基数，而是把索引中的文档总数作为基数。</li>
<li>docFreq: 包含关键字的文档数量，对应前文中的n。</li>
<li>tf: 关键字在文档中出现的次数。</li>
<li>length: 文档的长度。</li>
</ul>
</li>
<li>上面的公式在Lucence系统里做计算时会被拆分成三个部分：<ul>
<li>IDF Score = log(numDocs / (docFreq + 1))</li>
<li>TF Score = sqrt(tf)</li>
<li>fieldNorms = 1/sqrt(length)</li>
</ul>
</li>
<li>fieldNorms 是对文本长度的归一化(Normalization)。所以，上面公式也可以表示成: simlarity = IDF score <em> TF score </em> fieldNorms</li>
</ul>
<h1 id="BM25"><a href="#BM25" class="headerlink" title="BM25"></a>BM25</h1><ul>
<li>BM: best match</li>
<li>与TF-IDF的比较：<ul>
<li>1）BM25 = IDF <em> TF </em> QF</li>
<li>2）QF为查询词在query中的权重</li>
<li>3）TF做了一些改善，包括：对TF的值做了限定，限制在0~k+1；引入了平均文档长度，并将文档长度/平均文档长度引入TF公式，使得文档长度越大，TF Score随TF的增加越快增长到上界</li>
</ul>
</li>
</ul>
<h2 id="TF"><a href="#TF" class="headerlink" title="TF"></a>TF</h2><ul>
<li>传统 TF Score = sqrt(tf)</li>
<li>BM25的 TF Score = ((k + 1) * tf) / (k + tf)</li>
<li>下面是两种计算方法中，词频对TF Score影响的走势图。从图中可以看到，当tf增加时，TF Score跟着增加，但是BM25的TF Score会被限制在0~k+1之间。它可以无限逼近k+1，但永远无法触达它。这在业务上可以理解为某一个因素的影响强度不能是无限的，而是有个最大值，这也符合我们对文本相关性逻辑的理解。<img src="/2022/03/18/tfidf-bm25/bm251.png" class="" title="bm251"></li>
<li>文档长度：BM25还引入了平均文档长度的概念，单个文档长度对相关性的影响力与它和平均长度的比值有关系。BM25的TF公式里，除了k外，引入另外两个参数：L和b。L是文档长度与平均长度的比值。如果文档长度是平均长度的2倍，则L＝2。b是一个常数，它的作用是规定L对评分的影响有多大。加了L和b的公式变为：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 即文档长度越大，TF Score值越小</span><br><span class="line">TF Score &#x3D; ((k + 1) * tf) &#x2F; (k * (1.0 - b + b * L) + tf) </span><br></pre></td></tr></table></figure></li>
<li>下面是不同L的条件下，词频对TFScore影响的走势图：<img src="/2022/03/18/tfidf-bm25/bm252.png" class="" title="bm252"></li>
<li>从图上可以看到，文档越短，它逼近上限的速度越快，反之则越慢。这是可以理解的，对于只有几个词的内容，比如文章“标题”，只需要匹配很少的几个词，就可以确定相关性。而对于大篇幅的内容，比如一本书的内容，需要匹配很多词才能知道它的重点是讲什么。上文说到，参数b的作用是设定L对评分的影响有多大。如果把b设置为0，则L完全失去对评分的影响力。b的值越大，L对总评分的影响力越大。</li>
</ul>
<h2 id="IDF"><a href="#IDF" class="headerlink" title="IDF"></a>IDF</h2><script type="math/tex; mode=display">
\sum_{i: q_{i}=d_{i}=1} \log \frac{\left(r_{i}+0.5\right) /\left(R-r_{i}+0.5\right)}{\left(n_{i}-r_{i}+0.5\right) /\left((N-R)-\left(n_{i}-r_{i}\right)+0.5\right)}</script><img src="/2022/03/18/tfidf-bm25/bm253.png" class="" title="bm253">
<h2 id="查询词权重"><a href="#查询词权重" class="headerlink" title="查询词权重"></a>查询词权重</h2><ul>
<li>qf是查询词在用户查询中的频率，但一般用户查询都比较短，qf通常是1，K2是经验参数。<script type="math/tex; mode=display">\frac{\left(k_{2}+1\right) qf_{i}}{k_{2}+qf_{}}</script></li>
<li>此时，相似度最终的完整公式为<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i \in Q} \log \frac{\left(r_{i}+0.5\right) /\left(R-r_{i}+0.5\right)}{\left(n_{i}-r_{i}+0.5\right) /\left(N-n_{i}-R+r_{i}+0.5\right)} \cdot \frac{\left(k_{1}+1\right) f_{i}}{k+f_{i}} \cdot \frac{\left(k_{2}+1\right) q}{k_{2}+q f i} \\
& \downarrow \\
k &=k_{1}\left((1-b)+b \cdot \frac{d l}{a v d_1}\right)
\end{aligned}</script></li>
</ul>
]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>Tf-idf</tag>
        <tag>BM25</tag>
      </tags>
  </entry>
  <entry>
    <title>Python</title>
    <url>/2020/12/10/python/</url>
    <content><![CDATA[<p>Python doc &amp; 整理的部分问题，忘了从哪里来的，侵删<br><a id="more"></a></p>
<h1 id="Python语言特性"><a href="#Python语言特性" class="headerlink" title="Python语言特性"></a>Python语言特性</h1><h2 id="1-Python的函数参数传递"><a href="#1-Python的函数参数传递" class="headerlink" title="1 Python的函数参数传递"></a>1 Python的函数参数传递</h2><p>看两个例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">a</span>):</span></span><br><span class="line">    a = <span class="number">2</span></span><br><span class="line">fun(a)</span><br><span class="line"><span class="built_in">print</span> a  <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">a</span>):</span></span><br><span class="line">    a.append(<span class="number">1</span>)</span><br><span class="line">fun(a)</span><br><span class="line"><span class="built_in">print</span> a  <span class="comment"># [1]</span></span><br></pre></td></tr></table></figure>
<p>所有的变量都可以理解是内存中一个对象的“引用”，或者，也可以看似c中void*的感觉。</p>
<p>通过<code>id</code>来看引用<code>a</code>的内存地址可以比较理解：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">a</span>):</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;func_in&quot;</span>,<span class="built_in">id</span>(a)   <span class="comment"># func_in 41322472</span></span><br><span class="line">    a = <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;re-point&quot;</span>,<span class="built_in">id</span>(a), <span class="built_in">id</span>(<span class="number">2</span>)   <span class="comment"># re-point 41322448 41322448</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;func_out&quot;</span>,<span class="built_in">id</span>(a), <span class="built_in">id</span>(<span class="number">1</span>)  <span class="comment"># func_out 41322472 41322472</span></span><br><span class="line">fun(a)</span><br><span class="line"><span class="built_in">print</span> a  <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<p>注：具体的值在不同电脑上运行时可能不同。</p>
<p>可以看到，在执行完<code>a = 2</code>之后，<code>a</code>引用中保存的值，即内存地址发生变化，由原来<code>1</code>对象的所在的地址变成了<code>2</code>这个实体对象的内存地址。</p>
<p>而第2个例子<code>a</code>引用保存的内存值就不会发生变化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">a</span>):</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;func_in&quot;</span>,<span class="built_in">id</span>(a)  <span class="comment"># func_in 53629256</span></span><br><span class="line">    a.append(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;func_out&quot;</span>,<span class="built_in">id</span>(a)     <span class="comment"># func_out 53629256</span></span><br><span class="line">fun(a)</span><br><span class="line"><span class="built_in">print</span> a  <span class="comment"># [1]</span></span><br></pre></td></tr></table></figure>
<p>这里记住的是类型是属于对象的，而不是变量。而对象有两种,“可更改”（mutable）与“不可更改”（immutable）对象。在python中，strings, tuples, 和numbers是不可更改的对象，而 list, dict, set 等则是可以修改的对象。(这就是这个问题的重点)</p>
<p>当一个引用传递给函数的时候,函数自动复制一份引用,这个函数里的引用和外边的引用没有半毛关系了.所以第一个例子里函数把引用指向了一个不可变对象,当函数返回的时候,外面的引用没半毛感觉.而第二个例子就不一样了,函数内的引用指向的是可变对象,对它的操作就和定位了指针地址一样,在内存里进行修改.</p>
<p>如果还不明白的话,这里有更好的解释: <a href="http://stackoverflow.com/questions/986006/how-do-i-pass-a-variable-by-reference">http://stackoverflow.com/questions/986006/how-do-i-pass-a-variable-by-reference</a></p>
<h2 id="2-Python中的元类-metaclass"><a href="#2-Python中的元类-metaclass" class="headerlink" title="2 Python中的元类(metaclass)"></a>2 Python中的元类(metaclass)</h2><p>这个非常的不常用,但是像ORM这种复杂的结构还是会需要的,详情请看:<a href="http://stackoverflow.com/questions/100003/what-is-a-metaclass-in-python">http://stackoverflow.com/questions/100003/what-is-a-metaclass-in-python</a></p>
<h2 id="3-staticmethod和-classmethod"><a href="#3-staticmethod和-classmethod" class="headerlink" title="3 @staticmethod和@classmethod"></a>3 @staticmethod和@classmethod</h2><p>Python其实有3个方法,即静态方法(staticmethod),类方法(classmethod)和实例方法,如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">&quot;executing foo(%s)&quot;</span>%(x)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;executing foo(%s,%s)&quot;</span>%(self,x)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">class_foo</span>(<span class="params">cls,x</span>):</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;executing class_foo(%s,%s)&quot;</span>%(cls,x)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">static_foo</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;executing static_foo(%s)&quot;</span>%x</span><br><span class="line"></span><br><span class="line">a=A()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里先理解下函数参数里面的self和cls.这个self和cls是对类或者实例的绑定,对于一般的函数来说我们可以这么调用<code>foo(x)</code>,这个函数就是最常用的,它的工作跟任何东西(类,实例)无关.对于实例方法,我们知道在类里每次定义方法的时候都需要绑定这个实例,就是<code>foo(self, x)</code>,为什么要这么做呢?因为实例方法的调用离不开实例,我们需要把实例自己传给函数,调用的时候是这样的<code>a.foo(x)</code>(其实是<code>foo(a, x)</code>).类方法一样,只不过它传递的是类而不是实例,<code>A.class_foo(x)</code>.注意这里的self和cls可以替换别的参数,但是python的约定是这俩,还是不要改的好.</p>
<p>对于静态方法其实和普通的方法一样,不需要对谁进行绑定,唯一的区别是调用的时候需要使用<code>a.static_foo(x)</code>或者<code>A.static_foo(x)</code>来调用.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">\</th>
<th style="text-align:left">实例方法</th>
<th style="text-align:left">类方法</th>
<th style="text-align:left">静态方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">a = A()</td>
<td style="text-align:left">a.foo(x)</td>
<td style="text-align:left">a.class_foo(x)</td>
<td style="text-align:left">a.static_foo(x)</td>
</tr>
<tr>
<td style="text-align:left">A</td>
<td style="text-align:left">不可用</td>
<td style="text-align:left">A.class_foo(x)</td>
<td style="text-align:left">A.static_foo(x)</td>
</tr>
</tbody>
</table>
</div>
<p>更多关于这个问题:</p>
<ol>
<li><a href="http://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python">http://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python</a></li>
<li><a href="https://realpython.com/blog/python/instance-class-and-static-methods-demystified/">https://realpython.com/blog/python/instance-class-and-static-methods-demystified/</a><h2 id="4-类变量和实例变量"><a href="#4-类变量和实例变量" class="headerlink" title="4 类变量和实例变量"></a>4 类变量和实例变量</h2></li>
</ol>
<p><strong>类变量：</strong></p>
<blockquote>
<p>​    是可在类的所有实例之间共享的值（也就是说，它们不是单独分配给每个实例的）。例如下例中，num_of_instance 就是类变量，用于跟踪存在着多少个Test 的实例。</p>
</blockquote>
<p><strong>实例变量：</strong></p>
<blockquote>
<p>实例化之后，每个实例单独拥有的变量。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>(<span class="params"><span class="built_in">object</span></span>):</span>  </span><br><span class="line">    num_of_instance = <span class="number">0</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span>  </span><br><span class="line">        self.name = name  </span><br><span class="line">        Test.num_of_instance += <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  </span><br><span class="line">    <span class="built_in">print</span> Test.num_of_instance   <span class="comment"># 0</span></span><br><span class="line">    t1 = Test(<span class="string">&#x27;jack&#x27;</span>)  </span><br><span class="line">    <span class="built_in">print</span> Test.num_of_instance   <span class="comment"># 1</span></span><br><span class="line">    t2 = Test(<span class="string">&#x27;lucy&#x27;</span>)  </span><br><span class="line">    <span class="built_in">print</span> t1.name , t1.num_of_instance  <span class="comment"># jack 2</span></span><br><span class="line">    <span class="built_in">print</span> t2.name , t2.num_of_instance  <span class="comment"># lucy 2</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>补充的例子</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span></span><br><span class="line">    name=<span class="string">&quot;aaa&quot;</span></span><br><span class="line"></span><br><span class="line">p1=Person()</span><br><span class="line">p2=Person()</span><br><span class="line">p1.name=<span class="string">&quot;bbb&quot;</span></span><br><span class="line"><span class="built_in">print</span> p1.name  <span class="comment"># bbb</span></span><br><span class="line"><span class="built_in">print</span> p2.name  <span class="comment"># aaa</span></span><br><span class="line"><span class="built_in">print</span> Person.name  <span class="comment"># aaa</span></span><br></pre></td></tr></table></figure>
<p>这里<code>p1.name=&quot;bbb&quot;</code>是实例调用了类变量,这其实和上面第一个问题一样,就是函数传参的问题,<code>p1.name</code>一开始是指向的类变量<code>name=&quot;aaa&quot;</code>,但是在实例的作用域里把类变量的引用改变了,就变成了一个实例变量,self.name不再引用Person的类变量name了.</p>
<p>可以看看下面的例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span></span><br><span class="line">    name=[]</span><br><span class="line"></span><br><span class="line">p1=Person()</span><br><span class="line">p2=Person()</span><br><span class="line">p1.name.append(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span> p1.name  <span class="comment"># [1]</span></span><br><span class="line"><span class="built_in">print</span> p2.name  <span class="comment"># [1]</span></span><br><span class="line"><span class="built_in">print</span> Person.name  <span class="comment"># [1]</span></span><br></pre></td></tr></table></figure>
<p>参考:<a href="http://stackoverflow.com/questions/6470428/catch-multiple-exceptions-in-one-line-except-block">http://stackoverflow.com/questions/6470428/catch-multiple-exceptions-in-one-line-except-block</a></p>
<h2 id="5-Python自省"><a href="#5-Python自省" class="headerlink" title="5 Python自省"></a>5 Python自省</h2><p>这个也是python彪悍的特性.</p>
<p>自省就是面向对象的语言所写的程序在运行时,所能知道对象的类型.简单一句就是运行时能够获得对象的类型.比如type(),dir(),getattr(),hasattr(),isinstance().</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b = &#123;<span class="string">&#x27;a&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;b&#x27;</span>:<span class="number">2</span>,<span class="string">&#x27;c&#x27;</span>:<span class="number">3</span>&#125;</span><br><span class="line">c = <span class="literal">True</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(a),<span class="built_in">type</span>(b),<span class="built_in">type</span>(c) <span class="comment"># &lt;type &#x27;list&#x27;&gt; &lt;type &#x27;dict&#x27;&gt; &lt;type &#x27;bool&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">isinstance</span>(a,<span class="built_in">list</span>)  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<h2 id="6-字典推导式"><a href="#6-字典推导式" class="headerlink" title="6 字典推导式"></a>6 字典推导式</h2><p>可能你见过列表推导时,却没有见过字典推导式,在2.7中才加入的:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = &#123;key: value <span class="keyword">for</span> (key, value) <span class="keyword">in</span> iterable&#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-Python中单下划线和双下划线"><a href="#7-Python中单下划线和双下划线" class="headerlink" title="7 Python中单下划线和双下划线"></a>7 Python中单下划线和双下划线</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>():</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="meta">... </span>            self.__superprivate = <span class="string">&quot;Hello&quot;</span></span><br><span class="line"><span class="meta">... </span>            self._semiprivate = <span class="string">&quot;, world!&quot;</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mc = MyClass()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> mc.__superprivate</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">AttributeError: myClass instance has no attribute <span class="string">&#x27;__superprivate&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> mc._semiprivate</span><br><span class="line">, world!</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> mc.__dict__</span><br><span class="line">&#123;<span class="string">&#x27;_MyClass__superprivate&#x27;</span>: <span class="string">&#x27;Hello&#x27;</span>, <span class="string">&#x27;_semiprivate&#x27;</span>: <span class="string">&#x27;, world!&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p><code>__foo__</code>:一种约定,Python 的“魔术”对象,用来区别其他用户自定义的命名,以防冲突，就是例如<code>__init__()</code>,<code>__del__()</code>,<code>__call__()</code>这些特殊方法</p>
<p><code>_foo</code>:一种约定,用来指定变量私有.被常用于模块中，在一个模块中以单下划线开头的变量和函数被默认当作内部函数，如果使用 from a_module import /* 导入时，这部分变量和函数不会被导入。不过值得注意的是，如果使用 import a_module 这样导入模块，仍然可以用 a_module._some_var 这样的形式访问到这样的对象。</p>
<p><code>__foo</code>:这个有真正的意义:解析器用<code>_classname__foo</code>来代替这个名字,以区别和其他类相同的命名,它无法直接像公有成员一样随便访问,通过对象名._类名<strong>xxx这样的方式可以访问.Test 类里有一成员 </strong>x，那么 dir(Test) 时会看到 _Test<strong>x 而非 </strong>x。这是为了避免该成员的名称与子类中的名称冲突。但要注意这要求该名称末尾没有下划线。</p>
<p>详情见:<a href="http://stackoverflow.com/questions/1301346/the-meaning-of-a-single-and-a-double-underscore-before-an-object-name-in-python">http://stackoverflow.com/questions/1301346/the-meaning-of-a-single-and-a-double-underscore-before-an-object-name-in-python</a></p>
<p>或者: <a href="http://www.zhihu.com/question/19754941">http://www.zhihu.com/question/19754941</a></p>
<h2 id="8-字符串格式化-和-format"><a href="#8-字符串格式化-和-format" class="headerlink" title="8 字符串格式化:%和.format"></a>8 字符串格式化:%和.format</h2><p>.format在许多方面看起来更便利.对于<code>%</code>最烦人的是它无法同时传递一个变量和元组.你可能会想下面的代码不会有什么问题:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;hi there %s&quot; % name</span><br></pre></td></tr></table></figure>
<p>但是,如果name恰好是(1,2,3),它将会抛出一个TypeError异常.为了保证它总是正确的,你必须这样做:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;hi there %s&quot; % (name,)   # 提供一个单元素的数组而不是一个参数</span><br></pre></td></tr></table></figure>
<p>但是有点丑..format就没有这些问题.你给的第二个问题也是这样,.format好看多了.</p>
<p>你为什么不用它?</p>
<ul>
<li>不知道它(在读这个之前)</li>
<li>为了和Python2.5兼容(譬如logging库建议使用<code>%</code>(<a href="https://github.com/taizilongxu/interview_python/issues/4">issue #4</a>))</li>
</ul>
<p><a href="http://stackoverflow.com/questions/5082452/python-string-formatting-vs-format">http://stackoverflow.com/questions/5082452/python-string-formatting-vs-format</a></p>
<h2 id="9-迭代器和生成器"><a href="#9-迭代器和生成器" class="headerlink" title="9 迭代器和生成器"></a>9 迭代器和生成器</h2><ul>
<li>迭代器：凡是可以用for … in …的。可以随时读取你想要的元素，但是迭代器需要保存所有元素</li>
<li>生成器：一种特殊的迭代器，Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly. </li>
<li>yield: 返回的是生成器，而不是一个值。每调用一次，执行yield及之前的结果一次。</li>
</ul>
<p>这个是stackoverflow里python排名第一的问题,值得一看: <a href="http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python">http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python</a></p>
<p>这是中文版: <a href="http://taizilongxu.gitbooks.io/stackoverflow-about-python/content/1/README.html">http://taizilongxu.gitbooks.io/stackoverflow-about-python/content/1/README.html</a></p>
<p>这里有个关于生成器的创建问题面试官有考：<br>问：  将列表生成式中[]改成() 之后数据结构是否改变？<br>答案：是，从列表变为生成器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>L = [x*x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>L</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>g = (x*x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>g</span><br><span class="line">&lt;generator <span class="built_in">object</span> &lt;genexpr&gt; at <span class="number">0x0000028F8B774200</span>&gt;</span><br></pre></td></tr></table></figure>
<p>通过列表生成式，可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含百万元素的列表，不仅是占用很大的内存空间，如：我们只需要访问前面的几个元素，后面大部分元素所占的空间都是浪费的。因此，没有必要创建完整的列表（节省大量内存空间）。在Python中，我们可以采用生成器：边循环，边计算的机制—&gt;generator</p>
<h2 id="10-args-and-kwargs"><a href="#10-args-and-kwargs" class="headerlink" title="10 *args and **kwargs"></a>10 <code>*args</code> and <code>**kwargs</code></h2><p>用<code>*args</code>和<code>**kwargs</code>只是为了方便并没有强制使用它们.</p>
<p>当你不确定你的函数里将要传递多少参数时你可以用<code>*args</code>.例如,它可以传递任意数量的参数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">print_everything</span>(<span class="params">*args</span>):</span></span><br><span class="line">        <span class="keyword">for</span> count, thing <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span> <span class="string">&#x27;&#123;0&#125;. &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(count, thing)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print_everything(<span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;banana&#x27;</span>, <span class="string">&#x27;cabbage&#x27;</span>)</span><br><span class="line"><span class="number">0.</span> apple</span><br><span class="line"><span class="number">1.</span> banana</span><br><span class="line"><span class="number">2.</span> cabbage</span><br></pre></td></tr></table></figure>
<p>相似的,<code>**kwargs</code>允许你使用没有事先定义的参数名:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">table_things</span>(<span class="params">**kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> name, value <span class="keyword">in</span> kwargs.items():</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span> <span class="string">&#x27;&#123;0&#125; = &#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(name, value)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>table_things(apple = <span class="string">&#x27;fruit&#x27;</span>, cabbage = <span class="string">&#x27;vegetable&#x27;</span>)</span><br><span class="line">cabbage = vegetable</span><br><span class="line">apple = fruit</span><br></pre></td></tr></table></figure>
<p>你也可以混着用.命名参数首先获得参数值然后所有的其他参数都传递给<code>*args</code>和<code>**kwargs</code>.命名参数在列表的最前端.例如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def table_things(titlestring, **kwargs)</span><br></pre></td></tr></table></figure>
<p><code>*args</code>和<code>**kwargs</code>可以同时在函数的定义中,但是<code>*args</code>必须在<code>**kwargs</code>前面.</p>
<p>当调用函数时你也可以用<code>*</code>和<code>**</code>语法.例如:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">print_three_things</span>(<span class="params">a, b, c</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span> <span class="string">&#x27;a = &#123;0&#125;, b = &#123;1&#125;, c = &#123;2&#125;&#x27;</span>.<span class="built_in">format</span>(a,b,c)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mylist = [<span class="string">&#x27;aardvark&#x27;</span>, <span class="string">&#x27;baboon&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print_three_things(*mylist)</span><br><span class="line"></span><br><span class="line">a = aardvark, b = baboon, c = cat</span><br></pre></td></tr></table></figure>
<p>就像你看到的一样,它可以传递列表(或者元组)的每一项并把它们解包.注意必须与它们在函数里的参数相吻合.当然,你也可以在函数定义或者函数调用时用*.</p>
<p><a href="http://stackoverflow.com/questions/3394835/args-and-kwargs">http://stackoverflow.com/questions/3394835/args-and-kwargs</a></p>
<h2 id="11-面向切面编程AOP和装饰器"><a href="#11-面向切面编程AOP和装饰器" class="headerlink" title="11 面向切面编程AOP和装饰器"></a>11 面向切面编程AOP和装饰器</h2><ul>
<li>装饰器：函数可以接受函数作为参数，函数的返回值也可以是一个函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def measure_time(func):</span><br><span class="line">    def inner(*args, **kwargs):</span><br><span class="line">        s &#x3D; time.time()</span><br><span class="line">        result &#x3D; func(*args, **kwargs)</span><br><span class="line">        e &#x3D; time.time()</span><br><span class="line">        logger.info(&#39;&#123;&#125; costs &#123;&#125;&#39;.format(func.__name__, e - s))</span><br><span class="line">        return result</span><br><span class="line">    return inner</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from functools import wraps</span><br><span class="line"></span><br><span class="line">def makebold(fn):</span><br><span class="line">    @wraps(fn)</span><br><span class="line">    def wrapped(*args, **kwargs):</span><br><span class="line">        return &quot;&lt;b&gt;&quot; + fn(*args, **kwargs) + &quot;&lt;&#x2F;b&gt;&quot;</span><br><span class="line">    return wrapped</span><br></pre></td></tr></table></figure>
<p>某个函数的装饰器即：把某个函数作为参数传入装饰器，并执行装饰器里的内容</p>
<p>这个AOP一听起来有点懵,同学面阿里的时候就被问懵了…</p>
<p>装饰器是一个很著名的设计模式，经常被用于有切面需求的场景，较为经典的有插入日志、性能测试、事务处理等。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。概括的讲，<strong>装饰器的作用就是为已经存在的对象添加额外的功能。</strong></p>
<p>这个问题比较大,推荐: <a href="http://stackoverflow.com/questions/739654/how-can-i-make-a-chain-of-function-decorators-in-python">http://stackoverflow.com/questions/739654/how-can-i-make-a-chain-of-function-decorators-in-python</a></p>
<p>中文: <a href="http://taizilongxu.gitbooks.io/stackoverflow-about-python/content/3/README.html">http://taizilongxu.gitbooks.io/stackoverflow-about-python/content/3/README.html</a></p>
<h2 id="12-鸭子类型"><a href="#12-鸭子类型" class="headerlink" title="12 鸭子类型"></a>12 鸭子类型</h2><p>“当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。”</p>
<p>我们并不关心对象是什么类型，到底是不是鸭子，只关心行为。</p>
<p>比如在python中，有很多file-like的东西，比如StringIO,GzipFile,socket。它们有很多相同的方法，我们把它们当作文件使用。</p>
<p>又比如list.extend()方法中,我们并不关心它的参数是不是list,只要它是可迭代的,所以它的参数可以是list/tuple/dict/字符串/生成器等.</p>
<p>鸭子类型在动态语言中经常使用，非常灵活，使得python不想java那样专门去弄一大堆的设计模式。</p>
<h2 id="13-Python中重载-python无函数重载功能"><a href="#13-Python中重载-python无函数重载功能" class="headerlink" title="13 Python中重载 python无函数重载功能"></a>13 Python中重载 python无函数重载功能</h2><p>引自知乎:<a href="http://www.zhihu.com/question/20053359">http://www.zhihu.com/question/20053359</a></p>
<p>函数重载主要是为了解决两个问题。</p>
<ol>
<li>可变参数类型。</li>
<li>可变参数个数。</li>
</ol>
<p>另外，一个基本的设计原则是，仅仅当两个函数除了参数类型和参数个数不同以外，其功能是完全相同的，此时才使用函数重载，如果两个函数的功能其实不同，那么不应当使用重载，而应当使用一个名字不同的函数。</p>
<p>好吧，那么对于情况 1 ，函数功能相同，但是参数类型不同，python 如何处理？答案是根本不需要处理，因为 python 可以接受任何类型的参数，如果函数的功能相同，那么不同的参数类型在 python 中很可能是相同的代码，没有必要做成两个不同函数。</p>
<p>那么对于情况 2 ，函数功能相同，但参数个数不同，python 如何处理？大家知道，答案就是缺省参数。对那些缺少的参数设定为缺省参数即可解决问题。因为你假设函数功能相同，那么那些缺少的参数终归是需要用的。</p>
<p>好了，鉴于情况 1 跟 情况 2 都有了解决方案，python 自然就不需要函数重载了。</p>
<h2 id="14-新式类和旧式类"><a href="#14-新式类和旧式类" class="headerlink" title="14 新式类和旧式类"></a>14 新式类和旧式类</h2><p>这个面试官问了,我说了老半天,不知道他问的真正意图是什么.</p>
<p><a href="http://stackoverflow.com/questions/54867/what-is-the-difference-between-old-style-and-new-style-classes-in-python">stackoverflow</a></p>
<p>这篇文章很好的介绍了新式类的特性: <a href="http://www.cnblogs.com/btchenguang/archive/2012/09/17/2689146.html">http://www.cnblogs.com/btchenguang/archive/2012/09/17/2689146.html</a></p>
<p>新式类很早在2.2就出现了,所以旧式类完全是兼容的问题,Python3里的类全部都是新式类.这里有一个MRO问题可以了解下(新式类继承是根据C3算法,旧式类是深度优先),<Python核心编程>里讲的也很多.</p>
<blockquote>
<p>一个旧式类的深度优先的例子</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo1</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;A&quot;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="params">A</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo2</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>(<span class="params">A</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo1</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;C&quot;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span>(<span class="params">B, C</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">d = D()</span><br><span class="line">d.foo1()</span><br><span class="line"></span><br><span class="line"><span class="comment"># A</span></span><br></pre></td></tr></table></figure>
<p><strong>按照经典类的查找顺序<code>从左到右深度优先</code>的规则，在访问<code>d.foo1()</code>的时候,D这个类是没有的..那么往上查找,先找到B,里面没有,深度优先,访问A,找到了foo1(),所以这时候调用的是A的foo1()，从而导致C重写的foo1()被绕过</strong></p>
<h2 id="15-new-和-init-的区别"><a href="#15-new-和-init-的区别" class="headerlink" title="15 __new__和__init__的区别"></a>15 <code>__new__</code>和<code>__init__</code>的区别</h2><p>这个<code>__new__</code>确实很少见到,先做了解吧.</p>
<ol>
<li><code>__new__</code>是一个静态方法,而<code>__init__</code>是一个实例方法.</li>
<li><code>__new__</code>方法会返回一个创建的实例,而<code>__init__</code>什么都不返回.</li>
<li>只有在<code>__new__</code>返回一个cls的实例时后面的<code>__init__</code>才能被调用.</li>
<li>当创建一个新实例时调用<code>__new__</code>,初始化一个实例时用<code>__init__</code>.</li>
</ol>
<p><a href="http://stackoverflow.com/questions/674304/pythons-use-of-new-and-init">stackoverflow</a></p>
<p>ps: <code>__metaclass__</code>是创建类时起作用.所以我们可以分别使用<code>__metaclass__</code>,<code>__new__</code>和<code>__init__</code>来分别在类创建,实例创建和实例初始化的时候做一些小手脚.</p>
<h2 id="16-单例模式"><a href="#16-单例模式" class="headerlink" title="16 单例模式"></a>16 单例模式</h2><blockquote>
<p>​    单例模式是一种常用的软件设计模式。在它的核心结构中只包含一个被称为单例类的特殊类。通过单例模式可以保证系统中一个类只有一个实例而且该实例易于外界访问，从而方便对实例个数的控制并节约系统资源。如果希望在系统中某个类的对象只能存在一个，单例模式是最好的解决方案。</p>
<p><code>__new__()</code>在<code>__init__()</code>之前被调用，用于生成实例对象。利用这个方法和类的属性的特点可以实现设计模式的单例模式。单例模式是指创建唯一对象，单例模式设计的类只能实例<br><strong>这个绝对常考啊.绝对要记住1~2个方法,当时面试官是让手写的.</strong></p>
</blockquote>
<h3 id="1-使用-new-方法"><a href="#1-使用-new-方法" class="headerlink" title="1 使用__new__方法"></a>1 使用<code>__new__</code>方法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Singleton</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, *args, **kw</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(cls, <span class="string">&#x27;_instance&#x27;</span>):</span><br><span class="line">            orig = <span class="built_in">super</span>(Singleton, cls)</span><br><span class="line">            cls._instance = orig.__new__(cls, *args, **kw)</span><br><span class="line">        <span class="keyword">return</span> cls._instance</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>(<span class="params">Singleton</span>):</span></span><br><span class="line">    a = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="2-共享属性"><a href="#2-共享属性" class="headerlink" title="2 共享属性"></a>2 共享属性</h3><p>创建实例时把所有实例的<code>__dict__</code>指向同一个字典,这样它们具有相同的属性和方法.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Borg</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    _state = &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, *args, **kw</span>):</span></span><br><span class="line">        ob = <span class="built_in">super</span>(Borg, cls).__new__(cls, *args, **kw)</span><br><span class="line">        ob.__dict__ = cls._state</span><br><span class="line">        <span class="keyword">return</span> ob</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass2</span>(<span class="params">Borg</span>):</span></span><br><span class="line">    a = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="3-装饰器版本"><a href="#3-装饰器版本" class="headerlink" title="3 装饰器版本"></a>3 装饰器版本</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">singleton</span>(<span class="params">cls</span>):</span></span><br><span class="line">    instances = &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getinstance</span>(<span class="params">*args, **kw</span>):</span></span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">not</span> <span class="keyword">in</span> instances:</span><br><span class="line">            instances[cls] = cls(*args, **kw)</span><br><span class="line">        <span class="keyword">return</span> instances[cls]</span><br><span class="line">    <span class="keyword">return</span> getinstance</span><br><span class="line"></span><br><span class="line"><span class="meta">@singleton</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<h3 id="4-import方法"><a href="#4-import方法" class="headerlink" title="4 import方法"></a>4 import方法</h3><p>作为python的模块是天然的单例模式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mysingleton.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">My_Singleton</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">my_singleton = My_Singleton()</span><br><span class="line"></span><br><span class="line"><span class="comment"># to use</span></span><br><span class="line"><span class="keyword">from</span> mysingleton <span class="keyword">import</span> my_singleton</span><br><span class="line"></span><br><span class="line">my_singleton.foo()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong><a href="http://python.jobbole.com/87294/">单例模式伯乐在线详细解释</a></strong></p>
<h2 id="17-Python中的作用域"><a href="#17-Python中的作用域" class="headerlink" title="17 Python中的作用域"></a>17 Python中的作用域</h2><p>Python 中，一个变量的作用域总是由在代码中被赋值的地方所决定的。</p>
<p>当 Python 遇到一个变量的话他会按照这样的顺序进行搜索：</p>
<p>本地作用域（Local）→当前作用域被嵌入的本地作用域（Enclosing locals）→全局/模块作用域（Global）→内置作用域（Built-in）</p>
<h2 id="18-GIL线程全局锁"><a href="#18-GIL线程全局锁" class="headerlink" title="18 GIL线程全局锁"></a>18 GIL线程全局锁</h2><p>线程全局锁(Global Interpreter Lock),即Python为了保证线程安全而采取的独立线程运行的限制,说白了就是一个核只能在同一时间运行一个线程.<strong>对于io密集型任务，python的多线程起到作用，但对于cpu密集型任务，python的多线程几乎占不到任何优势，还有可能因为争夺资源而变慢。</strong></p>
<p>见<a href="http://www.oschina.net/translate/pythons-hardest-problem">Python 最难的问题</a></p>
<p>解决办法就是多进程和下面的协程(协程也只是单CPU,但是能减小切换代价提升性能).</p>
<h2 id="19-进程、线程、协程"><a href="#19-进程、线程、协程" class="headerlink" title="19 进程、线程、协程"></a>19 进程、线程、协程</h2><p><a href="https://www.cnblogs.com/lxmhhy/p/6041001.html">https://www.cnblogs.com/lxmhhy/p/6041001.html</a></p>
<ul>
<li>进程：<br>进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位。每个进程都有自己的独立内存空间，不同进程通过进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。</li>
<li>线程：<br>线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。</li>
<li>协程：<br>协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。</li>
</ul>
<p>GIL 是python的全局解释器锁，同一进程中假如有多个线程运行，一个线程在运行python程序的时候会霸占python解释器（加了一把锁即GIL），使该进程内的其他线程无法运行，等该线程运行完后其他线程才能运行。如果线程运行过程中遇到耗时操作，则解释器锁解开，使其他线程运行。所以在多线程中，线程的运行仍是有先后顺序的，并不是同时进行。</p>
<p>多进程中因为每个进程都能被系统分配资源，相当于每个进程有了一个python解释器，所以多进程可以实现多个进程的同时运行，缺点是进程系统资源开销大</p>
<p>简单点说协程是进程和线程的升级版,进程和线程都面临着内核态和用户态的切换问题而耗费许多切换时间,而协程就是用户自己控制切换的时机,不再需要陷入系统的内核态.</p>
<p>Python里最常见的yield就是协程的思想!可以查看第九个问题.</p>
<h2 id="20-闭包"><a href="#20-闭包" class="headerlink" title="20 闭包"></a>20 闭包</h2><p>闭包(closure)是函数式编程的重要的语法结构。闭包也是一种组织代码的结构，它同样提高了代码的可重复使用性。</p>
<p>当一个内嵌函数引用其外部作作用域的变量,我们就会得到一个闭包. 总结一下,创建一个闭包必须满足以下几点:</p>
<ol>
<li>必须有一个内嵌函数</li>
<li>内嵌函数必须引用外部函数中的变量</li>
<li>外部函数的返回值必须是内嵌函数</li>
</ol>
<p>感觉闭包还是有难度的,几句话是说不明白的,还是查查相关资料.</p>
<p>重点是函数运行后并不会被撤销,就像16题的instance字典一样,当函数运行完后,instance并不被销毁,而是继续留在内存空间里.这个功能类似类里的类变量,只不过迁移到了函数上.</p>
<p>闭包就像个空心球一样,你知道外面和里面,但你不知道中间是什么样.</p>
<h2 id="21-lambda函数"><a href="#21-lambda函数" class="headerlink" title="21 lambda函数"></a>21 lambda函数</h2><p>其实就是一个匿名函数,为什么叫lambda?因为和后面的函数式编程有关.</p>
<p>推荐: <a href="http://www.zhihu.com/question/20125256">知乎</a></p>
<h2 id="22-Python函数式编程"><a href="#22-Python函数式编程" class="headerlink" title="22 Python函数式编程"></a>22 Python函数式编程</h2><p>这个需要适当的了解一下吧,毕竟函数式编程在Python中也做了引用.</p>
<p>推荐: <a href="http://coolshell.cn/articles/10822.html">酷壳</a></p>
<p>python中函数式编程支持:</p>
<p>filter 函数的功能相当于过滤器。调用一个布尔函数<code>bool_func</code>来迭代遍历每个seq中的元素；返回一个使<code>bool_seq</code>返回值为true的元素的序列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line">&gt;&gt;&gt;b = <span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x &gt; <span class="number">5</span>, a)</span><br><span class="line">&gt;&gt;&gt;<span class="built_in">print</span> b</span><br><span class="line">&gt;&gt;&gt;[<span class="number">6</span>,<span class="number">7</span>]</span><br></pre></td></tr></table></figure>
<p>map函数是对一个序列的每个项依次执行函数，下面是对一个序列每个项都乘以2：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="built_in">map</span>(<span class="keyword">lambda</span> x:x*<span class="number">2</span>,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(a)</span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>
<p>reduce函数是对一个序列的每个项迭代调用函数，下面是求3的阶乘：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(<span class="keyword">lambda</span> x,y:x*y,<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">4</span>))</span><br><span class="line"><span class="number">6</span></span><br></pre></td></tr></table></figure>
<h2 id="23-Python里的拷贝"><a href="#23-Python里的拷贝" class="headerlink" title="23 Python里的拷贝"></a>23 Python里的拷贝</h2><p>引用和copy(),deepcopy()的区别</p>
<ul>
<li>复制不可变数据类型：引用=copy=deepcopy。不管copy还是deepcopy,都是同一个地址当浅复制的值是不可变对象（数值，字符串，元组）时和=“赋值”的情况一样，对象的id值与浅复制原来的值相同。</li>
<li>复制的值是可变对象（列表和字典）：浅拷贝copy有两种情况：</li>
</ul>
<p>第一种情况：复制的 对象中无 复杂 子对象，原来值的改变并不会影响浅复制的值，同时浅复制的值改变也并不会影响原来的值。原来值的id值与浅复制原来的值不同。</p>
<p>第二种情况：复制的对象中有 复杂 子对象 （例如列表中的一个子元素是一个列表）， 改变原来的值 中的复杂子对象的值 ，会影响浅复制的值。</p>
<p>深拷贝deepcopy：完全复制独立，包括内层列表和字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]]  <span class="comment">#原始对象</span></span><br><span class="line"></span><br><span class="line">b = a  <span class="comment">#赋值，传对象的引用</span></span><br><span class="line">c = copy.copy(a)  <span class="comment">#对象拷贝，浅拷贝</span></span><br><span class="line">d = copy.deepcopy(a)  <span class="comment">#对象拷贝，深拷贝</span></span><br><span class="line"></span><br><span class="line">a.append(<span class="number">5</span>)  <span class="comment">#修改对象a</span></span><br><span class="line">a[<span class="number">4</span>].append(<span class="string">&#x27;c&#x27;</span>)  <span class="comment">#修改对象a中的[&#x27;a&#x27;, &#x27;b&#x27;]数组对象</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;a = &#x27;</span>, a</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;b = &#x27;</span>, b</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;c = &#x27;</span>, c</span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;d = &#x27;</span>, d</span><br><span class="line"></span><br><span class="line">输出结果：</span><br><span class="line">a =  [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>], <span class="number">5</span>]</span><br><span class="line">b =  [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>], <span class="number">5</span>]</span><br><span class="line">c =  [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]]</span><br><span class="line">d =  [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<h2 id="24-Python垃圾回收机制"><a href="#24-Python垃圾回收机制" class="headerlink" title="24 Python垃圾回收机制"></a>24 Python垃圾回收机制</h2><p>Python GC主要使用引用计数（reference counting）来跟踪和回收垃圾。在引用计数的基础上，通过“标记-清除”（mark and sweep）解决容器对象可能产生的循环引用问题，通过“分代回收”（generation collection）以空间换时间的方法提高垃圾回收效率。</p>
<h3 id="1-引用计数"><a href="#1-引用计数" class="headerlink" title="1 引用计数"></a>1 引用计数</h3><p>PyObject是每个对象必有的内容，其中<code>ob_refcnt</code>就是做为引用计数。当一个对象有新的引用时，它的<code>ob_refcnt</code>就会增加，当引用它的对象被删除，它的<code>ob_refcnt</code>就会减少.引用计数为0时，该对象生命就结束了。</p>
<p>优点:</p>
<ol>
<li>简单</li>
<li>实时性</li>
</ol>
<p>缺点:</p>
<ol>
<li>维护引用计数消耗资源</li>
<li>循环引用</li>
</ol>
<h3 id="2-标记-清除机制"><a href="#2-标记-清除机制" class="headerlink" title="2 标记-清除机制"></a>2 标记-清除机制</h3><p>基本思路是先按需分配，等到没有空闲内存的时候从寄存器和程序栈上的引用出发，遍历以对象为节点、以引用为边构成的图，把所有可以访问到的对象打上标记，然后清扫一遍内存空间，把所有没标记的对象释放。</p>
<h3 id="3-分代技术"><a href="#3-分代技术" class="headerlink" title="3 分代技术"></a>3 分代技术</h3><p>分代回收的整体思想是：将系统中的所有内存块根据其存活时间划分为不同的集合，每个集合就成为一个“代”，垃圾收集频率随着“代”的存活时间的增大而减小，存活时间通常利用经过几次垃圾回收来度量。</p>
<p>Python默认定义了三代对象集合，索引数越大，对象存活时间越长。</p>
<p>举例：<br>当某些内存块M经过了3次垃圾收集的清洗之后还存活时，我们就将内存块M划到一个集合A中去，而新分配的内存都划分到集合B中去。当垃圾收集开始工作时，大多数情况都只对集合B进行垃圾回收，而对集合A进行垃圾回收要隔相当长一段时间后才进行，这就使得垃圾收集机制需要处理的内存少了，效率自然就提高了。在这个过程中，集合B中的某些内存块由于存活时间长而会被转移到集合A中，当然，集合A中实际上也存在一些垃圾，这些垃圾的回收会因为这种分代的机制而被延迟。</p>
<h2 id="25-Python的List"><a href="#25-Python的List" class="headerlink" title="25 Python的List"></a>25 Python的List</h2><p>推荐: <a href="http://www.jianshu.com/p/J4U6rR">http://www.jianshu.com/p/J4U6rR</a></p>
<h2 id="26-Python的is"><a href="#26-Python的is" class="headerlink" title="26 Python的is"></a>26 Python的is</h2><p><strong>is是对比地址,==是对比值</strong></p>
<h2 id="27-read-readline和readlines"><a href="#27-read-readline和readlines" class="headerlink" title="27 read,readline和readlines"></a>27 read,readline和readlines</h2><ul>
<li>read        读取整个文件</li>
<li>readline    读取下一行,使用生成器方法</li>
<li>readlines   读取整个文件到一个迭代器以供我们遍历</li>
</ul>
<h2 id="28-Python2和3的区别"><a href="#28-Python2和3的区别" class="headerlink" title="28 Python2和3的区别"></a>28 Python2和3的区别</h2><p>推荐：<a href="http://chenqx.github.io/2014/11/10/Key-differences-between-Python-2-7-x-and-Python-3-x/">Python 2.7.x 与 Python 3.x 的主要差异</a></p>
<ul>
<li>print函数</li>
<li>xrange被取消，用range代替. python2中range返回列表，python3中返回生成器</li>
<li>除法</li>
<li>next方法在python3中被取消，只使用next函数</li>
</ul>
<h2 id="29-super-init"><a href="#29-super-init" class="headerlink" title="29 super init"></a>29 super init</h2><p>super() lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven’t already.</p>
<p>Note that the syntax changed in Python 3.0: you can just say super().<code>__init__</code>() instead of super(ChildB, self).<code>__init__</code>() which IMO is quite a bit nicer.</p>
<p><a href="http://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods">http://stackoverflow.com/questions/576169/understanding-python-super-with-init-methods</a></p>
<p><a href="http://blog.csdn.net/mrlevo520/article/details/51712440">Python2.7中的super方法浅见</a></p>
<h2 id="30-range-and-xrange"><a href="#30-range-and-xrange" class="headerlink" title="30 range and xrange"></a>30 range and xrange</h2><p>都在循环时使用，xrange内存性能更好。<br>for i in range(0, 20):<br>for i in xrange(0, 20):<br>What is the difference between range and xrange functions in Python 2.X?<br> range creates a list, so if you do range(1, 10000000) it creates a list in memory with 9999999 elements.<br> xrange is a sequence object that evaluates lazily.</p>
<p><a href="http://stackoverflow.com/questions/94935/what-is-the-difference-between-range-and-xrange-functions-in-python-2-x">http://stackoverflow.com/questions/94935/what-is-the-difference-between-range-and-xrange-functions-in-python-2-x</a></p>
<h2 id="31-列出python中可变数据类型和不可变数据类型，并简述原理"><a href="#31-列出python中可变数据类型和不可变数据类型，并简述原理" class="headerlink" title="31 列出python中可变数据类型和不可变数据类型，并简述原理"></a>31 列出python中可变数据类型和不可变数据类型，并简述原理</h2><p>不可变数据类型：数值型、字符串型string和元组tuple</p>
<p>不允许变量的值发生变化，如果改变了变量的值，相当于是新建了一个对象，而对于相同的值的对象，在内存中则只有一个对象（一个地址），如下图用id()方法可以打印对象的id</p>
<p>可变数据类型：列表list、set和字典dict；</p>
<p>允许变量的值发生变化，即如果对变量进行append、+=等这种操作后，只是改变了变量的值，而不会新建一个对象，变量引用的对象的地址也不会变化，不过对于相同的值的不同对象，在内存中则会存在不同的对象，即每个对象都有自己的地址，相当于内存中对于同值的对象保存了多份，这里不存在引用计数，是实实在在的对象。</p>
<h2 id="更多：https-zhuanlan-zhihu-com-p-54430650"><a href="#更多：https-zhuanlan-zhihu-com-p-54430650" class="headerlink" title="更多：https://zhuanlan.zhihu.com/p/54430650"></a>更多：<a href="https://zhuanlan.zhihu.com/p/54430650">https://zhuanlan.zhihu.com/p/54430650</a></h2><h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><h2 id="1-select-poll和epoll"><a href="#1-select-poll和epoll" class="headerlink" title="1 select,poll和epoll"></a>1 select,poll和epoll</h2><p>其实所有的I/O都是轮询的方法,只不过实现的层面不同罢了.</p>
<p>这个问题可能有点深入了,但相信能回答出这个问题是对I/O多路复用有很好的了解了.其中tornado使用的就是epoll的.</p>
<p><a href="http://www.cnblogs.com/Anker/p/3265058.html">selec,poll和epoll区别总结</a></p>
<p>基本上select有3个缺点:</p>
<ol>
<li>连接数受限</li>
<li>查找配对速度慢</li>
<li>数据由内核拷贝到用户态</li>
</ol>
<p>poll改善了第一个缺点</p>
<p>epoll改了三个缺点.</p>
<p>关于epoll的: <a href="http://www.cnblogs.com/my_life/articles/3968782.html">http://www.cnblogs.com/my_life/articles/3968782.html</a></p>
<h2 id="2-调度算法"><a href="#2-调度算法" class="headerlink" title="2 调度算法"></a>2 调度算法</h2><ol>
<li>先来先服务(FCFS, First Come First Serve)</li>
<li>短作业优先(SJF, Shortest Job First)</li>
<li>最高优先权调度(Priority Scheduling)</li>
<li>时间片轮转(RR, Round Robin)</li>
<li>多级反馈队列调度(multilevel feedback queue scheduling)</li>
</ol>
<p>常见的调度算法总结:<a href="http://www.jianshu.com/p/6edf8174c1eb">http://www.jianshu.com/p/6edf8174c1eb</a></p>
<p>实时调度算法:</p>
<ol>
<li>最早截至时间优先 EDF</li>
<li>最低松弛度优先 LLF</li>
</ol>
<h2 id="3-死锁"><a href="#3-死锁" class="headerlink" title="3 死锁"></a>3 死锁</h2><p>原因:</p>
<ol>
<li>竞争资源</li>
<li>程序推进顺序不当</li>
</ol>
<p>必要条件:</p>
<ol>
<li>互斥条件</li>
<li>请求和保持条件</li>
<li>不剥夺条件</li>
<li>环路等待条件</li>
</ol>
<p>处理死锁基本方法:</p>
<ol>
<li>预防死锁(摒弃除1以外的条件)</li>
<li>避免死锁(银行家算法)</li>
<li>检测死锁(资源分配图)</li>
<li>解除死锁<ol>
<li>剥夺资源</li>
<li>撤销进程</li>
</ol>
</li>
</ol>
<p>死锁概念处理策略详细介绍:<a href="https://wizardforcel.gitbooks.io/wangdaokaoyan-os/content/10.html">https://wizardforcel.gitbooks.io/wangdaokaoyan-os/content/10.html</a></p>
<h2 id="4-程序编译与链接"><a href="#4-程序编译与链接" class="headerlink" title="4 程序编译与链接"></a>4 程序编译与链接</h2><p>推荐: <a href="http://www.ruanyifeng.com/blog/2014/11/compiler.html">http://www.ruanyifeng.com/blog/2014/11/compiler.html</a></p>
<p>Bulid过程可以分解为4个步骤:预处理(Prepressing), 编译(Compilation)、汇编(Assembly)、链接(Linking)</p>
<p>以c语言为例:</p>
<h3 id="1-预处理"><a href="#1-预处理" class="headerlink" title="1 预处理"></a>1 预处理</h3><p>预编译过程主要处理那些源文件中的以“#”开始的预编译指令，主要处理规则有：</p>
<ol>
<li>将所有的“#define”删除，并展开所用的宏定义</li>
<li>处理所有条件预编译指令，比如“#if”、“#ifdef”、 “#elif”、“#endif”</li>
<li>处理“#include”预编译指令，将被包含的文件插入到该编译指令的位置，注：此过程是递归进行的</li>
<li>删除所有注释</li>
<li>添加行号和文件名标识，以便于编译时编译器产生调试用的行号信息以及用于编译时产生编译错误或警告时可显示行号</li>
<li>保留所有的#pragma编译器指令。</li>
</ol>
<h3 id="2-编译"><a href="#2-编译" class="headerlink" title="2 编译"></a>2 编译</h3><p>编译过程就是把预处理完的文件进行一系列的词法分析、语法分析、语义分析及优化后生成相应的汇编代码文件。这个过程是整个程序构建的核心部分。</p>
<h3 id="3-汇编"><a href="#3-汇编" class="headerlink" title="3 汇编"></a>3 汇编</h3><p>汇编器是将汇编代码转化成机器可以执行的指令，每一条汇编语句几乎都是一条机器指令。经过编译、链接、汇编输出的文件成为目标文件(Object File)</p>
<h3 id="4-链接"><a href="#4-链接" class="headerlink" title="4 链接"></a>4 链接</h3><p>链接的主要内容就是把各个模块之间相互引用的部分处理好，使各个模块可以正确的拼接。<br>链接的主要过程包块 地址和空间的分配（Address and Storage Allocation）、符号决议(Symbol Resolution)和重定位(Relocation)等步骤。</p>
<h2 id="5-静态链接和动态链接"><a href="#5-静态链接和动态链接" class="headerlink" title="5 静态链接和动态链接"></a>5 静态链接和动态链接</h2><p>静态链接方法：静态链接的时候，载入代码就会把程序会用到的动态代码或动态代码的地址确定下来<br>静态库的链接可以使用静态链接，动态链接库也可以使用这种方法链接导入库</p>
<p>动态链接方法：使用这种方式的程序并不在一开始就完成动态链接，而是直到真正调用动态库代码时，载入程序才计算(被调用的那部分)动态代码的逻辑地址，然后等到某个时候，程序又需要调用另外某块动态代码时，载入程序又去计算这部分代码的逻辑地址，所以，这种方式使程序初始化时间较短，但运行期间的性能比不上静态链接的程序</p>
<h2 id="6-虚拟内存技术"><a href="#6-虚拟内存技术" class="headerlink" title="6 虚拟内存技术"></a>6 虚拟内存技术</h2><p>虚拟存储器是指具有请求调入功能和置换功能,能从逻辑上对内存容量加以扩充的一种存储系统.</p>
<h2 id="7-分页和分段"><a href="#7-分页和分段" class="headerlink" title="7 分页和分段"></a>7 分页和分段</h2><p>分页: 用户程序的地址空间被划分成若干固定大小的区域，称为“页”，相应地，内存空间分成若干个物理块，页和块的大小相等。可将用户程序的任一页放在内存的任一块中，实现了离散分配。</p>
<p>分段: 将用户程序地址空间分成若干个大小不等的段，每段可以定义一组相对完整的逻辑信息。存储分配时，以段为单位，段与段在内存中可以不相邻接，也实现了离散分配。</p>
<h3 id="分页与分段的主要区别"><a href="#分页与分段的主要区别" class="headerlink" title="分页与分段的主要区别"></a>分页与分段的主要区别</h3><ol>
<li>页是信息的物理单位,分页是为了实现非连续分配,以便解决内存碎片问题,或者说分页是由于系统管理的需要.段是信息的逻辑单位,它含有一组意义相对完整的信息,分段的目的是为了更好地实现共享,满足用户的需要.</li>
<li>页的大小固定,由系统确定,将逻辑地址划分为页号和页内地址是由机器硬件实现的.而段的长度却不固定,决定于用户所编写的程序,通常由编译程序在对源程序进行编译时根据信息的性质来划分.</li>
<li>分页的作业地址空间是一维的.分段的地址空间是二维的.</li>
</ol>
<h2 id="8-页面置换算法"><a href="#8-页面置换算法" class="headerlink" title="8 页面置换算法"></a>8 页面置换算法</h2><ol>
<li>最佳置换算法OPT:不可能实现</li>
<li>先进先出FIFO</li>
<li>最近最久未使用算法LRU:最近一段时间里最久没有使用过的页面予以置换.</li>
<li>clock算法</li>
</ol>
<h2 id="9-边沿触发和水平触发"><a href="#9-边沿触发和水平触发" class="headerlink" title="9 边沿触发和水平触发"></a>9 边沿触发和水平触发</h2><p>边缘触发是指每当状态变化时发生一个 io 事件，条件触发是只要满足条件就发生一个 io 事件</p>
<h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><h2 id="1-事务"><a href="#1-事务" class="headerlink" title="1 事务"></a>1 事务</h2><p>数据库事务(Database Transaction) ，是指作为单个逻辑工作单元执行的一系列操作，要么完全地执行，要么完全地不执行。<br>彻底理解数据库事务: <a href="http://www.hollischuang.com/archives/898">http://www.hollischuang.com/archives/898</a></p>
<h2 id="2-数据库索引"><a href="#2-数据库索引" class="headerlink" title="2 数据库索引"></a>2 数据库索引</h2><p>推荐: <a href="http://tech.meituan.com/mysql-index.html">http://tech.meituan.com/mysql-index.html</a></p>
<p><a href="http://blog.codinglabs.org/articles/theory-of-mysql-index.html">MySQL索引背后的数据结构及算法原理</a></p>
<p>聚集索引,非聚集索引,B-Tree,B+Tree,最左前缀原理</p>
<h2 id="3-Redis原理"><a href="#3-Redis原理" class="headerlink" title="3 Redis原理"></a>3 Redis原理</h2><h3 id="Redis是什么？"><a href="#Redis是什么？" class="headerlink" title="Redis是什么？"></a>Redis是什么？</h3><ol>
<li>是一个完全开源免费的key-value内存数据库 </li>
<li>通常被认为是一个数据结构服务器，主要是因为其有着丰富的数据结构 strings、map、 list、sets、 sorted sets</li>
</ol>
<h3 id="Redis数据库"><a href="#Redis数据库" class="headerlink" title="Redis数据库"></a>Redis数据库</h3><blockquote>
<p>​    通常局限点来说，Redis也以消息队列的形式存在，作为内嵌的List存在，满足实时的高并发需求。在使用缓存的时候，redis比memcached具有更多的优势，并且支持更多的数据类型，把redis当作一个中间存储系统，用来处理高并发的数据库操作</p>
</blockquote>
<ul>
<li>速度快：使用标准C写，所有数据都在内存中完成，读写速度分别达到10万/20万 </li>
<li>持久化：对数据的更新采用Copy-on-write技术，可以异步地保存到磁盘上，主要有两种策略，一是根据时间，更新次数的快照（save 300 10 ）二是基于语句追加方式(Append-only file，aof) </li>
<li>自动操作：对不同数据类型的操作都是自动的，很安全 </li>
<li>快速的主—从复制，官方提供了一个数据，Slave在21秒即完成了对Amazon网站10G key set的复制。 </li>
<li>Sharding技术： 很容易将数据分布到多个Redis实例中，数据库的扩展是个永恒的话题，在关系型数据库中，主要是以添加硬件、以分区为主要技术形式的纵向扩展解决了很多的应用场景，但随着web2.0、移动互联网、云计算等应用的兴起，这种扩展模式已经不太适合了，所以近年来，像采用主从配置、数据库复制形式的，Sharding这种技术把负载分布到多个特理节点上去的横向扩展方式用处越来越多。</li>
</ul>
<h3 id="Redis缺点"><a href="#Redis缺点" class="headerlink" title="Redis缺点"></a>Redis缺点</h3><ul>
<li>是数据库容量受到物理内存的限制,不能用作海量数据的高性能读写,因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。</li>
<li>Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。</li>
</ul>
<h2 id="4-乐观锁和悲观锁"><a href="#4-乐观锁和悲观锁" class="headerlink" title="4 乐观锁和悲观锁"></a>4 乐观锁和悲观锁</h2><p>悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作</p>
<p>乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。</p>
<p>乐观锁与悲观锁的具体区别: <a href="http://www.cnblogs.com/Bob-FD/p/3352216.html">http://www.cnblogs.com/Bob-FD/p/3352216.html</a></p>
<h2 id="5-MVCC"><a href="#5-MVCC" class="headerlink" title="5 MVCC"></a>5 MVCC</h2><blockquote>
<p>​    全称是Multi-Version Concurrent Control，即多版本并发控制，在MVCC协议下，每个读操作会看到一个一致性的snapshot，并且可以实现非阻塞的读。MVCC允许数据具有多个版本，这个版本可以是时间戳或者是全局递增的事务ID，在同一个时间点，不同的事务看到的数据是不同的。</p>
</blockquote>
<h3 id="MySQL的innodb引擎是如何实现MVCC的"><a href="#MySQL的innodb引擎是如何实现MVCC的" class="headerlink" title="MySQL的innodb引擎是如何实现MVCC的"></a><a href="http://lib.csdn.net/base/mysql">MySQL</a>的innodb引擎是如何实现MVCC的</h3><p>innodb会为每一行添加两个字段，分别表示该行<strong>创建的版本</strong>和<strong>删除的版本</strong>，填入的是事务的版本号，这个版本号随着事务的创建不断递增。在repeated read的隔离级别（<a href="http://blog.csdn.net/chosen0ne/article/details/10036775">事务的隔离级别请看这篇文章</a>）下，具体各种数据库操作的实现：</p>
<ul>
<li>select：满足以下两个条件innodb会返回该行数据：<ul>
<li>该行的创建版本号小于等于当前版本号，用于保证在select操作之前所有的操作已经执行落地。</li>
<li>该行的删除版本号大于当前版本或者为空。删除版本号大于当前版本意味着有一个并发事务将该行删除了。</li>
</ul>
</li>
<li>insert：将新插入的行的创建版本号设置为当前系统的版本号。</li>
<li>delete：将要删除的行的删除版本号设置为当前系统的版本号。</li>
<li>update：不执行原地update，而是转换成insert + delete。将旧行的删除版本号设置为当前版本号，并将新行insert同时设置创建版本号为当前版本号。</li>
</ul>
<p>其中，写操作（insert、delete和update）执行时，需要将系统版本号递增。</p>
<p>​    由于旧数据并不真正的删除，所以必须对这些数据进行清理，innodb会开启一个后台线程执行清理工作，具体的规则是将删除版本号小于当前系统版本的行删除，这个过程叫做purge。</p>
<p>通过MVCC很好的实现了事务的隔离性，可以达到repeated read级别，要实现serializable还必须加锁。</p>
<blockquote>
<p> 参考：<a href="http://blog.csdn.net/chosen0ne/article/details/18093187">MVCC浅析</a></p>
</blockquote>
<h2 id="6-MyISAM和InnoDB"><a href="#6-MyISAM和InnoDB" class="headerlink" title="6 MyISAM和InnoDB"></a>6 MyISAM和InnoDB</h2><p>MyISAM 适合于一些需要大量查询的应用，但其对于有大量写操作并不是很好。甚至你只是需要update一个字段，整个表都会被锁起来，而别的进程，就算是读进程都无法操作直到读操作完成。另外，MyISAM 对于 SELECT COUNT(*) 这类的计算是超快无比的。</p>
<p>InnoDB 的趋势会是一个非常复杂的存储引擎，对于一些小的应用，它会比 MyISAM 还慢。他是它支持“行锁” ，于是在写操作比较多的时候，会更优秀。并且，他还支持更多的高级应用，比如：事务。</p>
<p>mysql 数据库引擎: <a href="http://www.cnblogs.com/0201zcr/p/5296843.html">http://www.cnblogs.com/0201zcr/p/5296843.html</a><br>MySQL存储引擎－－MyISAM与InnoDB区别: <a href="https://segmentfault.com/a/1190000008227211">https://segmentfault.com/a/1190000008227211</a></p>
<h1 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h1><h2 id="1-三次握手"><a href="#1-三次握手" class="headerlink" title="1 三次握手"></a>1 三次握手</h2><ol>
<li>客户端通过向服务器端发送一个SYN来创建一个主动打开，作为三次握手的一部分。客户端把这段连接的序号设定为随机数 A。</li>
<li>服务器端应当为一个合法的SYN回送一个SYN/ACK。ACK 的确认码应为 A+1，SYN/ACK 包本身又有一个随机序号 B。</li>
<li>最后，客户端再发送一个ACK。当服务端受到这个ACK的时候，就完成了三路握手，并进入了连接创建状态。此时包序号被设定为收到的确认号 A+1，而响应则为 B+1。</li>
</ol>
<h2 id="2-四次挥手"><a href="#2-四次挥手" class="headerlink" title="2 四次挥手"></a>2 四次挥手</h2><p><em>注意: 中断连接端可以是客户端，也可以是服务器端. 下面仅以客户端断开连接举例, 反之亦然.</em></p>
<ol>
<li>客户端发送一个数据分段, 其中的 FIN 标记设置为1. 客户端进入 FIN-WAIT 状态. 该状态下客户端只接收数据, 不再发送数据.</li>
<li>服务器接收到带有 FIN = 1 的数据分段, 发送带有 ACK = 1 的剩余数据分段, 确认收到客户端发来的 FIN 信息.</li>
<li>服务器等到所有数据传输结束, 向客户端发送一个带有 FIN = 1 的数据分段, 并进入 CLOSE-WAIT 状态, 等待客户端发来带有 ACK = 1 的确认报文.</li>
<li>客户端收到服务器发来带有 FIN = 1 的报文, 返回 ACK = 1 的报文确认, 为了防止服务器端未收到需要重发, 进入 TIME-WAIT 状态. 服务器接收到报文后关闭连接. 客户端等待 2MSL 后未收到回复, 则认为服务器成功关闭, 客户端关闭连接.</li>
</ol>
<p>图解: <a href="http://blog.csdn.net/whuslei/article/details/6667471">http://blog.csdn.net/whuslei/article/details/6667471</a></p>
<h2 id="3-ARP协议"><a href="#3-ARP协议" class="headerlink" title="3 ARP协议"></a>3 ARP协议</h2><p>地址解析协议(Address Resolution Protocol)，其基本功能为透过目标设备的IP地址，查询目标的MAC地址，以保证通信的顺利进行。它是IPv4网络层必不可少的协议，不过在IPv6中已不再适用，并被邻居发现协议（NDP）所替代。</p>
<h2 id="4-urllib和urllib2的区别"><a href="#4-urllib和urllib2的区别" class="headerlink" title="4 urllib和urllib2的区别"></a>4 urllib和urllib2的区别</h2><p>这个面试官确实问过,当时答的urllib2可以Post而urllib不可以.</p>
<ol>
<li>urllib提供urlencode方法用来GET查询字符串的产生，而urllib2没有。这是为何urllib常和urllib2一起使用的原因。</li>
<li>urllib2可以接受一个Request类的实例来设置URL请求的headers，urllib仅可以接受URL。这意味着，你不可以伪装你的User Agent字符串等。</li>
</ol>
<h2 id="5-Post和Get"><a href="#5-Post和Get" class="headerlink" title="5 Post和Get"></a>5 Post和Get</h2><p><a href="http://www.cnblogs.com/nankezhishi/archive/2012/06/09/getandpost.html">GET和POST有什么区别？及为什么网上的多数答案都是错的</a><br><a href="https://www.zhihu.com/question/31640769?rf=37401322">知乎回答</a></p>
<p>get: <a href="http://tools.ietf.org/html/rfc2616#section-9.3">RFC 2616 - Hypertext Transfer Protocol — HTTP/1.1</a><br>post: <a href="http://tools.ietf.org/html/rfc2616#section-9.5">RFC 2616 - Hypertext Transfer Protocol — HTTP/1.1</a></p>
<h2 id="6-Cookie和Session"><a href="#6-Cookie和Session" class="headerlink" title="6 Cookie和Session"></a>6 Cookie和Session</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">Cookie</th>
<th style="text-align:left">Session</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">储存位置</td>
<td style="text-align:left">客户端</td>
<td style="text-align:left">服务器端</td>
</tr>
<tr>
<td style="text-align:left">目的</td>
<td style="text-align:left">跟踪会话，也可以保存用户偏好设置或者保存用户名密码等</td>
<td style="text-align:left">跟踪会话</td>
</tr>
<tr>
<td style="text-align:left">安全性</td>
<td style="text-align:left">不安全</td>
<td style="text-align:left">安全</td>
</tr>
</tbody>
</table>
</div>
<p>session技术是要使用到cookie的，之所以出现session技术，主要是为了安全。</p>
<h2 id="7-apache和nginx的区别"><a href="#7-apache和nginx的区别" class="headerlink" title="7 apache和nginx的区别"></a>7 apache和nginx的区别</h2><p>nginx 相对 apache 的优点：</p>
<ul>
<li>轻量级，同样起web 服务，比apache 占用更少的内存及资源</li>
<li>抗并发，nginx 处理请求是异步非阻塞的，支持更多的并发连接，而apache 则是阻塞型的，在高并发下nginx 能保持低资源低消耗高性能</li>
<li>配置简洁</li>
<li>高度模块化的设计，编写模块相对简单</li>
<li>社区活跃</li>
</ul>
<p>apache 相对nginx 的优点：</p>
<ul>
<li>rewrite ，比nginx 的rewrite 强大</li>
<li>模块超多，基本想到的都可以找到</li>
<li>少bug ，nginx 的bug 相对较多</li>
<li>超稳定</li>
</ul>
<h2 id="8-网站用户密码保存"><a href="#8-网站用户密码保存" class="headerlink" title="8 网站用户密码保存"></a>8 网站用户密码保存</h2><ol>
<li>明文保存</li>
<li>明文hash后保存,如md5</li>
<li>MD5+Salt方式,这个salt可以随机</li>
<li>知乎使用了Bcrypy(好像)加密</li>
</ol>
<h2 id="9-HTTP和HTTPS"><a href="#9-HTTP和HTTPS" class="headerlink" title="9 HTTP和HTTPS"></a>9 HTTP和HTTPS</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">状态码</th>
<th style="text-align:left">定义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1xx 报告</td>
<td style="text-align:left">接收到请求，继续进程</td>
</tr>
<tr>
<td style="text-align:left">2xx 成功</td>
<td style="text-align:left">步骤成功接收，被理解，并被接受</td>
</tr>
<tr>
<td style="text-align:left">3xx 重定向</td>
<td style="text-align:left">为了完成请求,必须采取进一步措施</td>
</tr>
<tr>
<td style="text-align:left">4xx 客户端出错</td>
<td style="text-align:left">请求包括错的顺序或不能完成</td>
</tr>
<tr>
<td style="text-align:left">5xx 服务器出错</td>
<td style="text-align:left">服务器无法完成显然有效的请求</td>
</tr>
</tbody>
</table>
</div>
<p>403: Forbidden<br>404: Not Found</p>
<p>HTTPS握手,对称加密,非对称加密,TLS/SSL,RSA</p>
<h2 id="10-XSRF和XSS"><a href="#10-XSRF和XSS" class="headerlink" title="10 XSRF和XSS"></a>10 XSRF和XSS</h2><ul>
<li>CSRF(Cross-site request forgery)跨站请求伪造</li>
<li>XSS(Cross Site Scripting)跨站脚本攻击</li>
</ul>
<p>CSRF重点在请求,XSS重点在脚本</p>
<h2 id="11-幂等-Idempotence"><a href="#11-幂等-Idempotence" class="headerlink" title="11 幂等 Idempotence"></a>11 幂等 Idempotence</h2><p>HTTP方法的幂等性是指一次和多次请求某一个资源应该具有同样的<strong>副作用</strong>。(注意是副作用)</p>
<p><code>GET http://www.bank.com/account/123456</code>，不会改变资源的状态，不论调用一次还是N次都没有副作用。请注意，这里强调的是一次和N次具有相同的副作用，而不是每次GET的结果相同。<code>GET http://www.news.com/latest-news</code>这个HTTP请求可能会每次得到不同的结果，但它本身并没有产生任何副作用，因而是满足幂等性的。</p>
<p>DELETE方法用于删除资源，有副作用，但它应该满足幂等性。比如：<code>DELETE http://www.forum.com/article/4231</code>，调用一次和N次对系统产生的副作用是相同的，即删掉id为4231的帖子；因此，调用者可以多次调用或刷新页面而不必担心引起错误。</p>
<p>POST所对应的URI并非创建的资源本身，而是资源的接收者。比如：<code>POST http://www.forum.com/articles</code>的语义是在<code>http://www.forum.com/articles</code>下创建一篇帖子，HTTP响应中应包含帖子的创建状态以及帖子的URI。两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性。</p>
<p>PUT所对应的URI是要创建或更新的资源本身。比如：<code>PUT http://www.forum/articles/4231</code>的语义是创建或更新ID为4231的帖子。对同一URI进行多次PUT的副作用和一次PUT是相同的；因此，PUT方法具有幂等性。</p>
<h2 id="12-RESTful架构-SOAP-RPC"><a href="#12-RESTful架构-SOAP-RPC" class="headerlink" title="12 RESTful架构(SOAP,RPC)"></a>12 RESTful架构(SOAP,RPC)</h2><p>推荐: <a href="http://www.ruanyifeng.com/blog/2011/09/restful.html">http://www.ruanyifeng.com/blog/2011/09/restful.html</a></p>
<h2 id="13-SOAP"><a href="#13-SOAP" class="headerlink" title="13 SOAP"></a>13 SOAP</h2><p>SOAP（原为Simple Object Access Protocol的首字母缩写，即简单对象访问协议）是交换数据的一种协议规范，使用在计算机网络Web服务（web service）中，交换带结构信息。SOAP为了简化网页服务器（Web Server）从XML数据库中提取数据时，节省去格式化页面时间，以及不同应用程序之间按照HTTP通信协议，遵从XML格式执行资料互换，使其抽象于语言实现、平台和硬件。</p>
<h2 id="14-RPC"><a href="#14-RPC" class="headerlink" title="14 RPC"></a>14 RPC</h2><p>RPC（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。</p>
<p>总结:服务提供的两大流派.传统意义以方法调用为导向通称RPC。为了企业SOA,若干厂商联合推出webservice,制定了wsdl接口定义,传输soap.当互联网时代,臃肿SOA被简化为http+xml/json.但是简化出现各种混乱。以资源为导向,任何操作无非是对资源的增删改查，于是统一的REST出现了.</p>
<p>进化的顺序: RPC -&gt; SOAP -&gt; RESTful</p>
<h2 id="15-CGI和WSGI"><a href="#15-CGI和WSGI" class="headerlink" title="15 CGI和WSGI"></a>15 CGI和WSGI</h2><p>CGI是通用网关接口，是连接web服务器和应用程序的接口，用户通过CGI来获取动态数据或文件等。<br>CGI程序是一个独立的程序，它可以用几乎所有语言来写，包括perl，c，lua，python等等。</p>
<p>WSGI, Web Server Gateway Interface，是Python应用程序或框架和Web服务器之间的一种接口，WSGI的其中一个目的就是让用户可以用统一的语言(Python)编写前后端。</p>
<p>官方说明：<a href="https://www.python.org/dev/peps/pep-3333/">PEP-3333</a></p>
<h2 id="16-中间人攻击"><a href="#16-中间人攻击" class="headerlink" title="16 中间人攻击"></a>16 中间人攻击</h2><p>在GFW里屡见不鲜的,呵呵.</p>
<p>中间人攻击（Man-in-the-middle attack，通常缩写为MITM）是指攻击者与通讯的两端分别创建独立的联系，并交换其所收到的数据，使通讯的两端认为他们正在通过一个私密的连接与对方直接对话，但事实上整个会话都被攻击者完全控制。</p>
<h2 id="17-c10k问题"><a href="#17-c10k问题" class="headerlink" title="17 c10k问题"></a>17 c10k问题</h2><p>所谓c10k问题，指的是服务器同时支持成千上万个客户端的问题，也就是concurrent 10 000 connection（这也是c10k这个名字的由来）。<br>推荐: <a href="https://my.oschina.net/xianggao/blog/664275">https://my.oschina.net/xianggao/blog/664275</a></p>
<h2 id="18-socket"><a href="#18-socket" class="headerlink" title="18 socket"></a>18 socket</h2><p>推荐: <a href="http://www.360doc.com/content/11/0609/15/5482098_122692444.shtml">http://www.360doc.com/content/11/0609/15/5482098_122692444.shtml</a></p>
<p>Socket=Ip address+ TCP/UDP + port</p>
<h2 id="19-浏览器缓存"><a href="#19-浏览器缓存" class="headerlink" title="19 浏览器缓存"></a>19 浏览器缓存</h2><p>推荐: <a href="http://www.cnblogs.com/skynet/archive/2012/11/28/2792503.html">http://www.cnblogs.com/skynet/archive/2012/11/28/2792503.html</a></p>
<p>304 Not Modified</p>
<h2 id="20-HTTP1-0和HTTP1-1"><a href="#20-HTTP1-0和HTTP1-1" class="headerlink" title="20 HTTP1.0和HTTP1.1"></a>20 HTTP1.0和HTTP1.1</h2><p>推荐: <a href="http://blog.csdn.net/elifefly/article/details/3964766">http://blog.csdn.net/elifefly/article/details/3964766</a></p>
<ol>
<li>请求头Host字段,一个服务器多个网站</li>
<li>长链接</li>
<li>文件断点续传</li>
<li>身份认证,状态管理,Cache缓存</li>
</ol>
<p>HTTP请求8种方法介绍<br>HTTP/1.1协议中共定义了8种HTTP请求方法，HTTP请求方法也被叫做“请求动作”，不同的方法规定了不同的操作指定的资源方式。服务端也会根据不同的请求方法做不同的响应。</p>
<p>GET</p>
<p>GET请求会显示请求指定的资源。一般来说GET方法应该只用于数据的读取，而不应当用于会产生副作用的非幂等的操作中。</p>
<p>GET会方法请求指定的页面信息，并返回响应主体，GET被认为是不安全的方法，因为GET方法会被网络蜘蛛等任意的访问。</p>
<p>HEAD</p>
<p>HEAD方法与GET方法一样，都是向服务器发出指定资源的请求。但是，服务器在响应HEAD请求时不会回传资源的内容部分，即：响应主体。这样，我们可以不传输全部内容的情况下，就可以获取服务器的响应头信息。HEAD方法常被用于客户端查看服务器的性能。</p>
<p>POST</p>
<p>POST请求会 向指定资源提交数据，请求服务器进行处理，如：表单数据提交、文件上传等，请求数据会被包含在请求体中。POST方法是非幂等的方法，因为这个请求可能会创建新的资源或/和修改现有资源。</p>
<p>PUT</p>
<p>PUT请求会身向指定资源位置上传其最新内容，PUT方法是幂等的方法。通过该方法客户端可以将指定资源的最新数据传送给服务器取代指定的资源的内容。</p>
<p>DELETE</p>
<p>DELETE请求用于请求服务器删除所请求URI（统一资源标识符，Uniform Resource Identifier）所标识的资源。DELETE请求后指定资源会被删除，DELETE方法也是幂等的。</p>
<p>CONNECT</p>
<p>CONNECT方法是HTTP/1.1协议预留的，能够将连接改为管道方式的代理服务器。通常用于SSL加密服务器的链接与非加密的HTTP代理服务器的通信。</p>
<p>OPTIONS</p>
<p>OPTIONS请求与HEAD类似，一般也是用于客户端查看服务器的性能。 这个方法会请求服务器返回该资源所支持的所有HTTP请求方法，该方法会用’*’来代替资源名称，向服务器发送OPTIONS请求，可以测试服务器功能是否正常。JavaScript的XMLHttpRequest对象进行CORS跨域资源共享时，就是使用OPTIONS方法发送嗅探请求，以判断是否有对指定资源的访问权限。 允许</p>
<p>TRACE</p>
<p>TRACE请求服务器回显其收到的请求信息，该方法主要用于HTTP请求的测试或诊断。</p>
<p>HTTP/1.1之后增加的方法</p>
<p>在HTTP/1.1标准制定之后，又陆续扩展了一些方法。其中使用中较多的是 PATCH 方法：</p>
<p>PATCH</p>
<p>PATCH方法出现的较晚，它在2010年的RFC 5789标准中被定义。PATCH请求与PUT请求类似，同样用于资源的更新。二者有以下两点不同：</p>
<p>但PATCH一般用于资源的部分更新，而PUT一般用于资源的整体更新。<br>当资源不存在时，PATCH会创建一个新的资源，而PUT只会对已在资源进行更新。</p>
<h2 id="21-Ajax"><a href="#21-Ajax" class="headerlink" title="21 Ajax"></a>21 Ajax</h2><p>AJAX,Asynchronous JavaScript and XML（异步的 JavaScript 和 XML）, 是与在不重新加载整个页面的情况下，与服务器交换数据并更新部分网页的技术。</p>
<h1 id="NIX"><a href="#NIX" class="headerlink" title="*NIX"></a>*NIX</h1><h2 id="unix进程间通信方式-IPC"><a href="#unix进程间通信方式-IPC" class="headerlink" title="unix进程间通信方式(IPC)"></a>unix进程间通信方式(IPC)</h2><ol>
<li>管道（Pipe）：管道可用于具有亲缘关系进程间的通信，允许一个进程和另一个与它有共同祖先的进程之间进行通信。</li>
<li>命名管道（named pipe）：命名管道克服了管道没有名字的限制，因此，除具有管道所具有的功能外，它还允许无亲缘关系进程间的通信。命名管道在文件系统中有对应的文件名。命名管道通过命令mkfifo或系统调用mkfifo来创建。</li>
<li>信号（Signal）：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身；linux除了支持Unix早期信号语义函数sigal外，还支持语义符合Posix.1标准的信号函数sigaction（实际上，该函数是基于BSD的，BSD为了实现可靠信号机制，又能够统一对外接口，用sigaction函数重新实现了signal函数）。</li>
<li>消息（Message）队列：消息队列是消息的链接表，包括Posix消息队列system V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺</li>
<li>共享内存：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥。</li>
<li>内存映射（mapped memory）：内存映射允许任何多个进程间通信，每一个使用该机制的进程通过把一个共享的文件映射到自己的进程地址空间来实现它。</li>
<li>信号量（semaphore）：主要作为进程间以及同一进程不同线程之间的同步手段。</li>
<li>套接口（Socket）：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。起初是由Unix系统的BSD分支开发出来的，但现在一般可以移植到其它类Unix系统上：Linux和System V的变种都支持套接字。</li>
</ol>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="1-红黑树"><a href="#1-红黑树" class="headerlink" title="1 红黑树"></a>1 红黑树</h2><p>红黑树与AVL的比较：</p>
<p>AVL是严格平衡树，因此在增加或者删除节点的时候，根据不同情况，旋转的次数比红黑树要多；</p>
<p>红黑是用非严格的平衡来换取增删节点时候旋转次数的降低；</p>
<p>所以简单说，如果你的应用中，搜索的次数远远大于插入和删除，那么选择AVL，如果搜索，插入删除次数几乎差不多，应该选择RB。</p>
<p>红黑树详解: <a href="https://xieguanglei.github.io/blog/post/red-black-tree.html">https://xieguanglei.github.io/blog/post/red-black-tree.html</a></p>
<p>教你透彻了解红黑树: <a href="https://github.com/julycoding/The-Art-Of-Programming-By-July/blob/master/ebook/zh/03.01.md">https://github.com/julycoding/The-Art-Of-Programming-By-July/blob/master/ebook/zh/03.01.md</a></p>
<h1 id="编程题"><a href="#编程题" class="headerlink" title="编程题"></a>编程题</h1><h2 id="1-台阶问题-斐波那契"><a href="#1-台阶问题-斐波那契" class="headerlink" title="1 台阶问题/斐波那契"></a>1 台阶问题/斐波那契</h2><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fib = <span class="keyword">lambda</span> n: n <span class="keyword">if</span> n &lt;= <span class="number">2</span> <span class="keyword">else</span> fib(n - <span class="number">1</span>) + fib(n - <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>第二种记忆方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">memo</span>(<span class="params">func</span>):</span></span><br><span class="line">    cache = &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrap</span>(<span class="params">*args</span>):</span></span><br><span class="line">        <span class="keyword">if</span> args <span class="keyword">not</span> <span class="keyword">in</span> cache:</span><br><span class="line">            cache[args] = func(*args)</span><br><span class="line">        <span class="keyword">return</span> cache[args]</span><br><span class="line">    <span class="keyword">return</span> wrap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@memo</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="keyword">if</span> i &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> fib(i<span class="number">-1</span>) + fib(i<span class="number">-2</span>)</span><br></pre></td></tr></table></figure>
<p>第三种方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span>(<span class="params">n</span>):</span></span><br><span class="line">    a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> xrange(n):</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">    <span class="keyword">return</span> b</span><br></pre></td></tr></table></figure>
<h2 id="2-变态台阶问题"><a href="#2-变态台阶问题" class="headerlink" title="2 变态台阶问题"></a>2 变态台阶问题</h2><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fib = <span class="keyword">lambda</span> n: n <span class="keyword">if</span> n &lt; <span class="number">2</span> <span class="keyword">else</span> <span class="number">2</span> * fib(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="3-矩形覆盖"><a href="#3-矩形覆盖" class="headerlink" title="3 矩形覆盖"></a>3 矩形覆盖</h2><p>我们可以用<code>2*1</code>的小矩形横着或者竖着去覆盖更大的矩形。请问用n个<code>2*1</code>的小矩形无重叠地覆盖一个<code>2*n</code>的大矩形，总共有多少种方法？</p>
<blockquote>
<p>第<code>2*n</code>个矩形的覆盖方法等于第<code>2*(n-1)</code>加上第<code>2*(n-2)</code>的方法。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = <span class="keyword">lambda</span> n: <span class="number">1</span> <span class="keyword">if</span> n &lt; <span class="number">2</span> <span class="keyword">else</span> f(n - <span class="number">1</span>) + f(n - <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="4-杨氏矩阵查找"><a href="#4-杨氏矩阵查找" class="headerlink" title="4 杨氏矩阵查找"></a>4 杨氏矩阵查找</h2><p>在一个m行n列二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p>
<p>使用Step-wise线性搜索。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">l, r, c</span>):</span></span><br><span class="line">    <span class="keyword">return</span> l[r][c]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find</span>(<span class="params">l, x</span>):</span></span><br><span class="line">    m = <span class="built_in">len</span>(l) - <span class="number">1</span></span><br><span class="line">    n = <span class="built_in">len</span>(l[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    r = <span class="number">0</span></span><br><span class="line">    c = n</span><br><span class="line">    <span class="keyword">while</span> c &gt;= <span class="number">0</span> <span class="keyword">and</span> r &lt;= m:</span><br><span class="line">        value = get_value(l, r, c)</span><br><span class="line">        <span class="keyword">if</span> value == x:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> value &gt; x:</span><br><span class="line">            c = c - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> value &lt; x:</span><br><span class="line">            r = r + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="5-去除列表中的重复元素"><a href="#5-去除列表中的重复元素" class="headerlink" title="5 去除列表中的重复元素"></a>5 去除列表中的重复元素</h2><p>用集合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">list</span>(<span class="built_in">set</span>(l))</span><br></pre></td></tr></table></figure>
<p>用字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l1 = [<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;a&#x27;</span>]</span><br><span class="line">l2 = &#123;&#125;.fromkeys(l1).keys()</span><br><span class="line"><span class="built_in">print</span> l2</span><br></pre></td></tr></table></figure>
<p>用字典并保持顺序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l1 = [<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;a&#x27;</span>]</span><br><span class="line">l2 = <span class="built_in">list</span>(<span class="built_in">set</span>(l1))</span><br><span class="line">l2.sort(key=l1.index)</span><br><span class="line"><span class="built_in">print</span> l2</span><br></pre></td></tr></table></figure>
<p>列表推导式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l1 = [<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;a&#x27;</span>]</span><br><span class="line">l2 = []</span><br><span class="line">[l2.append(i) <span class="keyword">for</span> i <span class="keyword">in</span> l1 <span class="keyword">if</span> <span class="keyword">not</span> i <span class="keyword">in</span> l2]</span><br></pre></td></tr></table></figure>
<p>sorted排序并且用列表推导式.</p>
<p>l = [‘b’,’c’,’d’,’b’,’c’,’a’,’a’]<br>[single.append(i) for i in sorted(l) if i not in single]<br>print single</p>
<h2 id="6-链表成对调换"><a href="#6-链表成对调换" class="headerlink" title="6 链表成对调换"></a>6 链表成对调换</h2><p><code>1-&gt;2-&gt;3-&gt;4</code>转换成<code>2-&gt;1-&gt;4-&gt;3</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.val = x</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># @param a ListNode</span></span><br><span class="line">    <span class="comment"># @return a ListNode</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">swapPairs</span>(<span class="params">self, head</span>):</span></span><br><span class="line">        <span class="keyword">if</span> head != <span class="literal">None</span> <span class="keyword">and</span> head.<span class="built_in">next</span> != <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">next</span> = head.<span class="built_in">next</span></span><br><span class="line">            head.<span class="built_in">next</span> = self.swapPairs(<span class="built_in">next</span>.<span class="built_in">next</span>)</span><br><span class="line">            <span class="built_in">next</span>.<span class="built_in">next</span> = head</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> head</span><br></pre></td></tr></table></figure>
<h2 id="7-创建字典的方法"><a href="#7-创建字典的方法" class="headerlink" title="7 创建字典的方法"></a>7 创建字典的方法</h2><h3 id="1-直接创建"><a href="#1-直接创建" class="headerlink" title="1 直接创建"></a>1 直接创建</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">dict</span> = &#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;earth&#x27;</span>, <span class="string">&#x27;port&#x27;</span>:<span class="string">&#x27;80&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-工厂方法"><a href="#2-工厂方法" class="headerlink" title="2 工厂方法"></a>2 工厂方法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">items=[(<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;earth&#x27;</span>),(<span class="string">&#x27;port&#x27;</span>,<span class="string">&#x27;80&#x27;</span>)]</span><br><span class="line">dict2=<span class="built_in">dict</span>(items)</span><br><span class="line">dict1=<span class="built_in">dict</span>(([<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;earth&#x27;</span>],[<span class="string">&#x27;port&#x27;</span>,<span class="string">&#x27;80&#x27;</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="3-fromkeys-方法"><a href="#3-fromkeys-方法" class="headerlink" title="3 fromkeys()方法"></a>3 fromkeys()方法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1=&#123;&#125;.fromkeys((<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;y&#x27;</span>),<span class="number">-1</span>)</span><br><span class="line"><span class="built_in">dict</span>=&#123;<span class="string">&#x27;x&#x27;</span>:<span class="number">-1</span>,<span class="string">&#x27;y&#x27;</span>:<span class="number">-1</span>&#125;</span><br><span class="line">dict2=&#123;&#125;.fromkeys((<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;y&#x27;</span>))</span><br><span class="line">dict2=&#123;<span class="string">&#x27;x&#x27;</span>:<span class="literal">None</span>, <span class="string">&#x27;y&#x27;</span>:<span class="literal">None</span>&#125;</span><br></pre></td></tr></table></figure>
<h2 id="8-合并两个有序列表"><a href="#8-合并两个有序列表" class="headerlink" title="8 合并两个有序列表"></a>8 合并两个有序列表</h2><p>知乎远程面试要求编程</p>
<blockquote>
<p> 尾递归</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_recursion_merge_sort2</span>(<span class="params">l1, l2, tmp</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(l1) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">len</span>(l2) == <span class="number">0</span>:</span><br><span class="line">        tmp.extend(l1)</span><br><span class="line">        tmp.extend(l2)</span><br><span class="line">        <span class="keyword">return</span> tmp</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> l1[<span class="number">0</span>] &lt; l2[<span class="number">0</span>]:</span><br><span class="line">            tmp.append(l1[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">del</span> l1[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tmp.append(l2[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">del</span> l2[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> _recursion_merge_sort2(l1, l2, tmp)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recursion_merge_sort2</span>(<span class="params">l1, l2</span>):</span></span><br><span class="line">    <span class="keyword">return</span> _recursion_merge_sort2(l1, l2, [])</span><br></pre></td></tr></table></figure>
<blockquote>
<p> 循环算法</p>
</blockquote>
<p>思路：</p>
<p>定义一个新的空列表</p>
<p>比较两个列表的首个元素</p>
<p>小的就插入到新列表里</p>
<p>把已经插入新列表的元素从旧列表删除</p>
<p>直到两个旧列表有一个为空</p>
<p>再把旧列表加到新列表后面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def loop_merge_sort(l1, l2):</span><br><span class="line">    tmp &#x3D; []</span><br><span class="line">    while len(l1) &gt; 0 and len(l2) &gt; 0:</span><br><span class="line">        if l1[0] &lt; l2[0]:</span><br><span class="line">            tmp.append(l1[0])</span><br><span class="line">            del l1[0]</span><br><span class="line">        else:</span><br><span class="line">            tmp.append(l2[0])</span><br><span class="line">            del l2[0]</span><br><span class="line">    tmp.extend(l1)</span><br><span class="line">    tmp.extend(l2)</span><br><span class="line">    return tmp</span><br></pre></td></tr></table></figure>
<blockquote>
<p>pop弹出</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>]</span><br><span class="line">b = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sortedlist</span>(<span class="params">a,b</span>):</span></span><br><span class="line">    c = []</span><br><span class="line">    <span class="keyword">while</span> a <span class="keyword">and</span> b:</span><br><span class="line">        <span class="keyword">if</span> a[<span class="number">0</span>] &gt;= b[<span class="number">0</span>]:</span><br><span class="line">            c.append(b.pop(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c.append(a.pop(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">while</span> a:</span><br><span class="line">        c.append(a.pop(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">while</span> b:</span><br><span class="line">        c.append(b.pop(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"><span class="built_in">print</span> merge_sortedlist(a,b)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h2 id="9-交叉链表求交点"><a href="#9-交叉链表求交点" class="headerlink" title="9 交叉链表求交点"></a>9 交叉链表求交点</h2><blockquote>
<p>其实思想可以按照从尾开始比较两个链表，如果相交，则从尾开始必然一致，只要从尾开始比较，直至不一致的地方即为交叉点，如图所示</p>
</blockquote>
<p><img src="http://hi.csdn.net/attachment/201106/28/0_1309244136MWLP.gif" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用a,b两个list来模拟链表，可以看出交叉点是 7这个节点</span></span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">1</span>,<span class="number">5</span>]</span><br><span class="line">b = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">1</span>,<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">min</span>(<span class="built_in">len</span>(a),<span class="built_in">len</span>(b))):</span><br><span class="line">    <span class="keyword">if</span> i==<span class="number">1</span> <span class="keyword">and</span> (a[<span class="number">-1</span>] != b[<span class="number">-1</span>]):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;No&quot;</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> a[-i] != b[-i]:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">&quot;交叉节点：&quot;</span>,a[-i+<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>另外一种比较正规的方法，构造链表类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.val = x</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">node</span>(<span class="params">l1, l2</span>):</span></span><br><span class="line">    length1, lenth2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="comment"># 求两个链表长度</span></span><br><span class="line">    <span class="keyword">while</span> l1.<span class="built_in">next</span>:</span><br><span class="line">        l1 = l1.<span class="built_in">next</span></span><br><span class="line">        length1 += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> l2.<span class="built_in">next</span>:</span><br><span class="line">        l2 = l2.<span class="built_in">next</span></span><br><span class="line">        length2 += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 长的链表先走</span></span><br><span class="line">    <span class="keyword">if</span> length1 &gt; lenth2:</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(length1 - length2):</span><br><span class="line">            l1 = l1.<span class="built_in">next</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(length2 - length1):</span><br><span class="line">            l2 = l2.<span class="built_in">next</span></span><br><span class="line">    <span class="keyword">while</span> l1 <span class="keyword">and</span> l2:</span><br><span class="line">        <span class="keyword">if</span> l1.<span class="built_in">next</span> == l2.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">return</span> l1.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l1 = l1.<span class="built_in">next</span></span><br><span class="line">            l2 = l2.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>
<p>修改了一下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.val = x</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">node</span>(<span class="params">l1, l2</span>):</span></span><br><span class="line">    length1, length2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="comment"># 求两个链表长度</span></span><br><span class="line">    <span class="keyword">while</span> l1.<span class="built_in">next</span>:</span><br><span class="line">        l1 = l1.<span class="built_in">next</span><span class="comment">#尾节点</span></span><br><span class="line">        length1 += <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> l2.<span class="built_in">next</span>:</span><br><span class="line">        l2 = l2.<span class="built_in">next</span><span class="comment">#尾节点</span></span><br><span class="line">        length2 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#如果相交</span></span><br><span class="line">    <span class="keyword">if</span> l1.<span class="built_in">next</span> == l2.<span class="built_in">next</span>:</span><br><span class="line">        <span class="comment"># 长的链表先走</span></span><br><span class="line">        <span class="keyword">if</span> length1 &gt; length2:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(length1 - length2):</span><br><span class="line">                l1 = l1.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">return</span> l1<span class="comment">#返回交点</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(length2 - length1):</span><br><span class="line">                l2 = l2.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">return</span> l2<span class="comment">#返回交点</span></span><br><span class="line">    <span class="comment"># 如果不相交</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<p>思路: <a href="http://humaoli.blog.163.com/blog/static/13346651820141125102125995/">http://humaoli.blog.163.com/blog/static/13346651820141125102125995/</a></p>
<h2 id="10-二分查找"><a href="#10-二分查找" class="headerlink" title="10 二分查找"></a>10 二分查找</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span>(<span class="params"><span class="built_in">list</span>, item</span>):</span></span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = <span class="built_in">len</span>(<span class="built_in">list</span>) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> low &lt;= high:</span><br><span class="line">        mid = (high - low) / <span class="number">2</span> + low    <span class="comment"># 避免(high + low) / 2溢出</span></span><br><span class="line">        guess = <span class="built_in">list</span>[mid]</span><br><span class="line">        <span class="keyword">if</span> guess &gt; item:</span><br><span class="line">            high = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> guess &lt; item:</span><br><span class="line">            low = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">mylist = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"><span class="built_in">print</span> binary_search(mylist, <span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>参考: <a href="http://blog.csdn.net/u013205877/article/details/76411718">http://blog.csdn.net/u013205877/article/details/76411718</a></p>
<h2 id="11-快排"><a href="#11-快排" class="headerlink" title="11 快排"></a>11 快排</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quicksort</span>(<span class="params"><span class="built_in">list</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">list</span>)&lt;<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        midpivot = <span class="built_in">list</span>[<span class="number">0</span>]</span><br><span class="line">        lessbeforemidpivot = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>[<span class="number">1</span>:] <span class="keyword">if</span> i&lt;=midpivot]</span><br><span class="line">        biggerafterpivot = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>[<span class="number">1</span>:] <span class="keyword">if</span> i &gt; midpivot]</span><br><span class="line">        finallylist = quicksort(lessbeforemidpivot)+[midpivot]+quicksort(biggerafterpivot)</span><br><span class="line">        <span class="keyword">return</span> finallylist</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> quicksort([<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p> 更多排序问题可见：<a href="http://blog.csdn.net/mrlevo520/article/details/77829204">数据结构与算法-排序篇-Python描述</a></p>
</blockquote>
<h2 id="12-找零问题"><a href="#12-找零问题" class="headerlink" title="12 找零问题"></a>12 找零问题</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#values是硬币的面值values = [ 25, 21, 10, 5, 1]</span></span><br><span class="line"><span class="comment">#valuesCounts   钱币对应的种类数</span></span><br><span class="line"><span class="comment">#money  找出来的总钱数</span></span><br><span class="line"><span class="comment">#coinsUsed   对应于目前钱币总数i所使用的硬币数目</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coinChange</span>(<span class="params">values,valuesCounts,money,coinsUsed</span>):</span></span><br><span class="line">    <span class="comment">#遍历出从1到money所有的钱数可能</span></span><br><span class="line">    <span class="keyword">for</span> cents <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,money+<span class="number">1</span>):</span><br><span class="line">        minCoins = cents</span><br><span class="line">        <span class="comment">#把所有的硬币面值遍历出来和钱数做对比</span></span><br><span class="line">        <span class="keyword">for</span> kind <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,valuesCounts):</span><br><span class="line">            <span class="keyword">if</span> (values[kind] &lt;= cents):</span><br><span class="line">                temp = coinsUsed[cents - values[kind]] +<span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> (temp &lt; minCoins):</span><br><span class="line">                    minCoins = temp</span><br><span class="line">        coinsUsed[cents] = minCoins</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;面值:&#123;0&#125;的最少硬币使用数为:&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(cents, coinsUsed[cents]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>思路: <a href="http://blog.csdn.net/wdxin1322/article/details/9501163">http://blog.csdn.net/wdxin1322/article/details/9501163</a></p>
<p>方法: <a href="http://www.cnblogs.com/ChenxofHit/archive/2011/03/18/1988431.html">http://www.cnblogs.com/ChenxofHit/archive/2011/03/18/1988431.html</a></p>
<h2 id="13-广度遍历和深度遍历二叉树"><a href="#13-广度遍历和深度遍历二叉树" class="headerlink" title="13 广度遍历和深度遍历二叉树"></a>13 广度遍历和深度遍历二叉树</h2><p>给定一个数组，构建二叉树，并且按层次打印这个二叉树</p>
<h2 id="14-二叉树节点"><a href="#14-二叉树节点" class="headerlink" title="14 二叉树节点"></a>14 二叉树节点</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data, left=<span class="literal">None</span>, right=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.data = data</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line"></span><br><span class="line">tree = Node(<span class="number">1</span>, Node(<span class="number">3</span>, Node(<span class="number">7</span>, Node(<span class="number">0</span>)), Node(<span class="number">6</span>)), Node(<span class="number">2</span>, Node(<span class="number">5</span>), Node(<span class="number">4</span>)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="15-层次遍历"><a href="#15-层次遍历" class="headerlink" title="15 层次遍历"></a>15 层次遍历</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span>(<span class="params">root</span>):</span></span><br><span class="line">    row = [root]</span><br><span class="line">    <span class="keyword">while</span> row:</span><br><span class="line">        print(row)</span><br><span class="line">        row = [kid <span class="keyword">for</span> item <span class="keyword">in</span> row <span class="keyword">for</span> kid <span class="keyword">in</span> (item.left, item.right) <span class="keyword">if</span> kid]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="16-深度遍历"><a href="#16-深度遍历" class="headerlink" title="16 深度遍历"></a>16 深度遍历</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deep</span>(<span class="params">root</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="built_in">print</span> root.data</span><br><span class="line">    deep(root.left)</span><br><span class="line">    deep(root.right)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    lookup(tree)</span><br><span class="line">    deep(tree)</span><br></pre></td></tr></table></figure>
<h2 id="17-前中后序遍历"><a href="#17-前中后序遍历" class="headerlink" title="17 前中后序遍历"></a>17 前中后序遍历</h2><p>深度遍历改变顺序就OK了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#二叉树的遍历</span></span><br><span class="line"><span class="comment">#简单的二叉树节点类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,value,left,right</span>):</span></span><br><span class="line">        self.value = value</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line"></span><br><span class="line"><span class="comment">#中序遍历:遍历左子树,访问当前节点,遍历右子树</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mid_travelsal</span>(<span class="params">root</span>):</span></span><br><span class="line">    <span class="keyword">if</span> root.left <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mid_travelsal(root.left)</span><br><span class="line">    <span class="comment">#访问当前节点</span></span><br><span class="line">    print(root.value)</span><br><span class="line">    <span class="keyword">if</span> root.right <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mid_travelsal(root.right)</span><br><span class="line"></span><br><span class="line"><span class="comment">#前序遍历:访问当前节点,遍历左子树,遍历右子树</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pre_travelsal</span>(<span class="params">root</span>):</span></span><br><span class="line">    <span class="built_in">print</span> (root.value)</span><br><span class="line">    <span class="keyword">if</span> root.left <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        pre_travelsal(root.left)</span><br><span class="line">    <span class="keyword">if</span> root.right <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        pre_travelsal(root.right)</span><br><span class="line"></span><br><span class="line"><span class="comment">#后续遍历:遍历左子树,遍历右子树,访问当前节点</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_trvelsal</span>(<span class="params">root</span>):</span></span><br><span class="line">    <span class="keyword">if</span> root.left <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        post_trvelsal(root.left)</span><br><span class="line">    <span class="keyword">if</span> root.right <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        post_trvelsal(root.right)</span><br><span class="line">    <span class="built_in">print</span> (root.value)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="18-求最大树深"><a href="#18-求最大树深" class="headerlink" title="18 求最大树深"></a>18 求最大树深</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span>(<span class="params">root</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(maxDepth(root.left), maxDepth(root.right)) + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="19-求两棵树是否相同"><a href="#19-求两棵树是否相同" class="headerlink" title="19 求两棵树是否相同"></a>19 求两棵树是否相同</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isSameTree</span>(<span class="params">p, q</span>):</span></span><br><span class="line">    <span class="keyword">if</span> p == <span class="literal">None</span> <span class="keyword">and</span> q == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">elif</span> p <span class="keyword">and</span> q :</span><br><span class="line">        <span class="keyword">return</span> p.val == q.val <span class="keyword">and</span> isSameTree(p.left,q.left) <span class="keyword">and</span> isSameTree(p.right,q.right)</span><br><span class="line">    <span class="keyword">else</span> :</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="20-前序中序求后序"><a href="#20-前序中序求后序" class="headerlink" title="20 前序中序求后序"></a>20 前序中序求后序</h2><p>推荐: <a href="http://blog.csdn.net/hinyunsin/article/details/6315502">http://blog.csdn.net/hinyunsin/article/details/6315502</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rebuild</span>(<span class="params">pre, center</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pre:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    cur = Node(pre[<span class="number">0</span>])</span><br><span class="line">    index = center.index(pre[<span class="number">0</span>])</span><br><span class="line">    cur.left = rebuild(pre[<span class="number">1</span>:index + <span class="number">1</span>], center[:index])</span><br><span class="line">    cur.right = rebuild(pre[index + <span class="number">1</span>:], center[index + <span class="number">1</span>:])</span><br><span class="line">    <span class="keyword">return</span> cur</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deep</span>(<span class="params">root</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    deep(root.left)</span><br><span class="line">    deep(root.right)</span><br><span class="line">    <span class="built_in">print</span> root.data</span><br></pre></td></tr></table></figure>
<h2 id="21-单链表逆置"><a href="#21-单链表逆置" class="headerlink" title="21 单链表逆置"></a>21 单链表逆置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data=<span class="literal">None</span>, <span class="built_in">next</span>=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.data = data</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="built_in">next</span></span><br><span class="line"></span><br><span class="line">link = Node(<span class="number">1</span>, Node(<span class="number">2</span>, Node(<span class="number">3</span>, Node(<span class="number">4</span>, Node(<span class="number">5</span>, Node(<span class="number">6</span>, Node(<span class="number">7</span>, Node(<span class="number">8</span>, Node(<span class="number">9</span>)))))))))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rev</span>(<span class="params">link</span>):</span></span><br><span class="line">    pre = link</span><br><span class="line">    cur = link.<span class="built_in">next</span></span><br><span class="line">    pre.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">while</span> cur:</span><br><span class="line">        tmp = cur.<span class="built_in">next</span></span><br><span class="line">        cur.<span class="built_in">next</span> = pre</span><br><span class="line">        pre = cur</span><br><span class="line">        cur = tmp</span><br><span class="line">    <span class="keyword">return</span> pre</span><br><span class="line"></span><br><span class="line">root = rev(link)</span><br><span class="line"><span class="keyword">while</span> root:</span><br><span class="line">    <span class="built_in">print</span> root.data</span><br><span class="line">    root = root.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>
<p>思路: <a href="http://blog.csdn.net/feliciafay/article/details/6841115">http://blog.csdn.net/feliciafay/article/details/6841115</a></p>
<p>方法: <a href="http://www.xuebuyuan.com/2066385.html?mobile=1">http://www.xuebuyuan.com/2066385.html?mobile=1</a></p>
<h2 id="22-两个字符串是否是变位词"><a href="#22-两个字符串是否是变位词" class="headerlink" title="22 两个字符串是否是变位词"></a>22 两个字符串是否是变位词</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Anagram</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    @:param s1: The first string</span></span><br><span class="line"><span class="string">    @:param s2: The second string</span></span><br><span class="line"><span class="string">    @:return true or false</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Solution1</span>(<span class="params">s1,s2</span>):</span></span><br><span class="line">        alist = <span class="built_in">list</span>(s2)</span><br><span class="line"></span><br><span class="line">        pos1 = <span class="number">0</span></span><br><span class="line">        stillOK = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> pos1 &lt; <span class="built_in">len</span>(s1) <span class="keyword">and</span> stillOK:</span><br><span class="line">            pos2 = <span class="number">0</span></span><br><span class="line">            found = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">while</span> pos2 &lt; <span class="built_in">len</span>(alist) <span class="keyword">and</span> <span class="keyword">not</span> found:</span><br><span class="line">                <span class="keyword">if</span> s1[pos1] == alist[pos2]:</span><br><span class="line">                    found = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    pos2 = pos2 + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> found:</span><br><span class="line">                alist[pos2] = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stillOK = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">            pos1 = pos1 + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> stillOK</span><br><span class="line"></span><br><span class="line">    print(Solution1(<span class="string">&#x27;abcd&#x27;</span>,<span class="string">&#x27;dcba&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Solution2</span>(<span class="params">s1,s2</span>):</span></span><br><span class="line">        alist1 = <span class="built_in">list</span>(s1)</span><br><span class="line">        alist2 = <span class="built_in">list</span>(s2)</span><br><span class="line"></span><br><span class="line">        alist1.sort()</span><br><span class="line">        alist2.sort()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        pos = <span class="number">0</span></span><br><span class="line">        matches = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> pos &lt; <span class="built_in">len</span>(s1) <span class="keyword">and</span> matches:</span><br><span class="line">            <span class="keyword">if</span> alist1[pos] == alist2[pos]:</span><br><span class="line">                pos = pos + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                matches = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> matches</span><br><span class="line"></span><br><span class="line">    print(Solution2(<span class="string">&#x27;abcde&#x27;</span>,<span class="string">&#x27;edcbg&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Solution3</span>(<span class="params">s1,s2</span>):</span></span><br><span class="line">        c1 = [<span class="number">0</span>]*<span class="number">26</span></span><br><span class="line">        c2 = [<span class="number">0</span>]*<span class="number">26</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s1)):</span><br><span class="line">            pos = <span class="built_in">ord</span>(s1[i])-<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">            c1[pos] = c1[pos] + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s2)):</span><br><span class="line">            pos = <span class="built_in">ord</span>(s2[i])-<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">            c2[pos] = c2[pos] + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        stillOK = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">while</span> j&lt;<span class="number">26</span> <span class="keyword">and</span> stillOK:</span><br><span class="line">            <span class="keyword">if</span> c1[j] == c2[j]:</span><br><span class="line">                j = j + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stillOK = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> stillOK</span><br><span class="line"></span><br><span class="line">    print(Solution3(<span class="string">&#x27;apple&#x27;</span>,<span class="string">&#x27;pleap&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h2 id="23-动态规划问题"><a href="#23-动态规划问题" class="headerlink" title="23 动态规划问题"></a>23 动态规划问题</h2><blockquote>
<p> 可参考：<a href="http://blog.csdn.net/mrlevo520/article/details/75676160">动态规划(DP)的整理-Python描述</a></p>
</blockquote>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>计组-原码、反码、补码、移码、浮点数、校验码</title>
    <url>/2022/04/05/ComputerOrganizationandDesign/</url>
    <content><![CDATA[<p><a href="https://piaodazhu.github.io/15-floating-point-number-basic/">ref</a></p>
<h1 id="码制-原码、反码、补码、移码"><a href="#码制-原码、反码、补码、移码" class="headerlink" title="码制-原码、反码、补码、移码"></a>码制-原码、反码、补码、移码</h1><ul>
<li>原码：符号位+数值位。直观，但是运算较为困难，<strong>加法和减法需要采取不同的硬件实现方式；0有+0和-0两种表达方式</strong>。</li>
<li>反码：正数的反码：原码，负数的反码：原码的数值位取反。<strong>0有+0和-0两种表达方式</strong></li>
<li>补码：正数的补码：原码，负数的补码：反码数值位+1。<strong>能将减法运算转换为加法运算，使得加减法能采用同一套硬件实现；0的表示唯一</strong></li>
<li>移码：补码的符号位取反（<strong>不区分正负数</strong>）。<strong>和补码一样，方便运算；且整体大小单调递增，用于表示浮点数的阶码；0的表示唯一</strong></li>
</ul>
<a id="more"></a>
<h2 id="表示范围"><a href="#表示范围" class="headerlink" title="表示范围"></a>表示范围</h2><ul>
<li>原码和反码一样，补码和移码一样。后者比前者在负数上多表示一个数，原因是前者的0有两种表达方式。</li>
<li>从0开始，因此最大值都要减一</li>
<li>小数是对整数都除以2^(n-1)<img src="/2022/04/05/ComputerOrganizationandDesign/float1.png" class="" title="float1">
</li>
</ul>
<h2 id="规格化"><a href="#规格化" class="headerlink" title="规格化"></a>规格化</h2><ul>
<li>对于规格化浮点数小数点后第一个值是固定的（正数：1，负数：原码1，补码0）</li>
<li>目的：为了提高精度需要使尾数的有效位数尽可能占满可用的位数。这种措施称为浮点数的规格化。</li>
<li><a href="https://blog.csdn.net/qq_43855740/article/details/104721619">ref</a></li>
</ul>
<h1 id="浮点数"><a href="#浮点数" class="headerlink" title="浮点数"></a>浮点数</h1><ul>
<li>浮点数在计算机中的表示<ul>
<li>浮点数表示时，尾数使用纯小数(有的是0.xxx的小数，有的是1.xxx的小数)</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">|&lt;-----j--------&gt;|&lt;--------S--------&gt;|</span><br><span class="line">+----|-----------|----|--------------+</span><br><span class="line">| jf | j1j2...jm | Sf | S1S2...Sn    |</span><br><span class="line">+----|-----------|----|--------------+</span><br><span class="line"> 阶符     阶码     数符 |    尾数数值</span><br></pre></td></tr></table></figure>
<ul>
<li>例如-28.75，用1位阶符、5位阶码、1位数符、8位尾数，阶码和尾数都采用原码表示，则可以表示如下：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(-28.75)base10 </span><br><span class="line">&#x3D; (-110100.11)base2</span><br><span class="line">&#x3D; -1 * (0.11010011)base2 * 2^5</span><br><span class="line">阶符 jf &#x3D; 0， 阶码 j1j2j3j4j5 &#x3D; 00101 &#x3D; 5， 数符 Sf &#x3D; 1， 尾数 S1S2...S8 &#x3D; 11010011</span><br><span class="line">结果：</span><br><span class="line">    0 00101 1 11010011</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="表示范围-1"><a href="#表示范围-1" class="headerlink" title="表示范围"></a>表示范围</h2><ul>
<li>先确定阶码位和尾数位是用什么码制表示。尾数的最大最小值分别和阶码最大值相乘即可。</li>
<li>IEEE-754<ul>
<li>尾数的规格化方式：省略了开头的“1.”，即M表示的是1.M</li>
<li>阶码表示以2为底的阶数，用移码表示，但需要注意的是，这里的移码不能采取将补码的最高位取反的方式获得，这是因为把补码的最高位取反，相当于增加了1000…00的偏置量，而IEEE 754标准下的偏置量是0111…11，也就少了一个1。之所以这样设计，是为了保留阶码全0或者全1的情况，以用作特殊用途。</li>
<li>因此，以float型浮点数转化为十进制数的计算公式为：<script type="math/tex">N=(-1)^{S} \times 1.M \times 2^{J-127}</script></li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+------|-----------------------|----------------------+</span><br><span class="line">|   S  |     阶码J（含阶符）     |       尾数M          |</span><br><span class="line">+------|-----------------------|----------------------+</span><br><span class="line">  数符       （采用移码）        |     （采用原码） </span><br></pre></td></tr></table></figure>
<h2 id="浮点数运算"><a href="#浮点数运算" class="headerlink" title="浮点数运算"></a>浮点数运算</h2><ul>
<li><a href="https://piaodazhu.github.io/15-floating-point-number-basic/">ref</a></li>
</ul>
<h1 id="校验码"><a href="#校验码" class="headerlink" title="校验码"></a>校验码</h1><ul>
<li>码距/海明距离：任意两个码字之间最少变化的二进制位数</li>
</ul>
<h2 id="奇偶校验码"><a href="#奇偶校验码" class="headerlink" title="奇偶校验码"></a>奇偶校验码</h2><ul>
<li>分奇校验和偶校验</li>
<li>预留一位作为校验位，以奇校验为例，值为1则说明有奇数个1，反之有偶数个1</li>
<li>只能<strong>检错不能纠错、且只能检验出奇数个数据改变的场景</strong></li>
</ul>
<h2 id="海明校验码"><a href="#海明校验码" class="headerlink" title="海明校验码"></a>海明校验码</h2><ul>
<li>一种多重奇偶校验码。</li>
<li>实现原理：在有效信息位中加入几个校验位形成海明码，并把海明码的每一个二进制位分配到几个奇偶校验组中。当某一位出错后，就会引起有关的几个校验位的值发生变化。</li>
<li>特点：<strong>可以发现错误，定位错误位置，自动纠正错误。 可以检测双比特错误，但只能纠正单比特错误。</strong><blockquote>
<p>纠错理论：L-1=D+C且D≥C：编码最小码距L越大，其检测错误的位数D越大，纠正错误的位数C也越大，且纠错能力恒小于检错能力</p>
</blockquote>
</li>
<li>case<img src="/2022/04/05/ComputerOrganizationandDesign/hammingcode1.png" class="" title="hammingcode1">
<img src="/2022/04/05/ComputerOrganizationandDesign/hammingcode2.png" class="" title="hammingcode2">
</li>
</ul>
<p>hammingcode1.png</p>
<h2 id="循环冗余校验码（Cyclic-redundancy-check-crc）"><a href="#循环冗余校验码（Cyclic-redundancy-check-crc）" class="headerlink" title="循环冗余校验码（Cyclic redundancy check, crc）"></a>循环冗余校验码（Cyclic redundancy check, crc）</h2><ul>
<li>CRC码共N位：K位信息位后拼接R位校验码。K+R=N，也称(N,R)码。</li>
<li><strong>可以发现错误，定位错误位置，自动纠正错误。可检测/纠正出所有奇数位错，所有双比特的错和所有小于、等于校验位长度的突发错</strong></li>
<li><ol>
<li>发送端和接受端会有一个生成多项式G(x)约定,生成多项式G(x)的最高次幂为R。任意一个二进制数码都可用一个系数为0或1的多项式与之对应。比如：二进制数码 1101 对应的G(x)=1X^3+1X^2+0X^1+1X^0= X^3+X^2+1</li>
</ol>
</li>
<li><ol>
<li>在发送端，将要传送的K位二进制信息码左移R位，将它与生成多项式G(x)所对应的的二进制数码进行模2除法（按位异或），产生余数，生成一个R位检验码，并附在信息码后，构成一个新的二进制码（CRC）码，共K+R位。</li>
</ol>
</li>
<li>case<img src="/2022/04/05/ComputerOrganizationandDesign/crc.jpg" class="" title="crc">
</li>
</ul>
<h1 id="存储系统-amp-存储管理"><a href="#存储系统-amp-存储管理" class="headerlink" title="存储系统 &amp; 存储管理"></a>存储系统 &amp; 存储管理</h1><h2 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h2><ul>
<li>存储器层次结构的思想是上一层次的存储器是低一层次的高速缓存。</li>
<li>越接近cpu的存储器速度越快容量越小，越远离cpu的存储器速度越慢容量越大。</li>
<li>高速缓冲技术是利用程序访问的局部性原理，把程序中正在使用的部分存放在一个高速的、容量较小的Cache中，使CPU的访存操作大多数针对Cache进行，从而大大提高程序的执行速度。<img src="/2022/04/05/ComputerOrganizationandDesign/storage1.jpeg" class="" title="storage1">
</li>
</ul>
<h3 id="局部性原理"><a href="#局部性原理" class="headerlink" title="局部性原理"></a>局部性原理</h3><ul>
<li>空间局部性：一个存储位置被访问后，其周围的位置也倾向于被访问。</li>
<li>时间局部性：一个存储位置倾向于被多次访问（类似循环体中的变量）。</li>
<li>程序越满足局部性，其执行效率越高。例如GRU的缓存机制利用了时间局部性，prefetch利用了空间局部性的思想。</li>
</ul>
<h3 id="cache置换算法"><a href="#cache置换算法" class="headerlink" title="cache置换算法"></a>cache置换算法</h3><ul>
<li>随机算法：几乎没意义</li>
<li>先进先出：一定意义</li>
<li>近期最少使用 least recently used : lru 使用最广泛，把数据加入一个链表中，按访问时间排序，发生淘汰的时候，把访问时间最旧的淘汰掉。。利用hash表（实现O(1)的访问）和双向链表（实现O(1)的add/delete）。<ul>
<li>查询某个key，没有则返回-1，有则返回，并移动到链表头。</li>
<li>新增某个元素，新增在链表头，若链表满，delete链表尾的元素。</li>
<li>LRU对于循环出现的数据，缓存命中不高。例如1，1，1，2，2，2，3，4，1，1，1，2，2，2…..</li>
</ul>
</li>
<li>最不经常使用页置换 least frequently used : 把数据加入到链表中，按频次排序，一个数据被访问过，把它的频次+1，发生淘汰的时候，把频次低的淘汰掉。<ul>
<li>lfu 问题在于可能前期用的多，后期不用，就会占住内存不释放。LFU对于交替出现的数据，缓存命中不高，例如1，1，1，2，2，3，4，3，4，3，4，3，4，3，4，3，4……</li>
</ul>
</li>
</ul>
<h2 id="存储管理"><a href="#存储管理" class="headerlink" title="存储管理"></a>存储管理</h2>]]></content>
      <categories>
        <category>系统分析师</category>
      </categories>
      <tags>
        <tag>码制</tag>
        <tag>浮点数</tag>
      </tags>
  </entry>
  <entry>
    <title>软考-系统分析师</title>
    <url>/2022/03/19/%E8%BD%AF%E8%80%83-%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90%E5%B8%88/</url>
    <content><![CDATA[<p>软考<br><a id="more"></a></p>
<h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><h2 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h2><ul>
<li>在操作系统中进程是进行<strong>系统资源分配、调度和管理</strong>的最小单位。<ul>
<li>PCB（process control block）是进程存在的唯一标志， PCB 描述了进程的基本情况。其中的内容可分成为调度信息和执行信息两大部分。</li>
<li>进程的生命周期可以刻画为不同的状态，并分为三态模型和五态模型。</li>
</ul>
</li>
<li>线程是进程的活动成分，是<strong>处理器</strong>分配资源的最小单位，它可以<strong>共享进程的资源与地址空间</strong>，通过线程的活动，进程可以提供多种服务〈对服务器进程而言）或实行子任务并行（对用户进程而言）。每个进程创建时只有一个线程，根据需要在运行过程中创建更多的线程〈前者也可称主线程）。显然，只有主线程的进程才是传统意义下的进程。<ul>
<li>内核负责线程的调度，线程的优先级可以动态地改变。采用线程机制的最大优点是节省开销，传统的进程创建子进程的办法中，内存开销大，而且创建时间也长。</li>
<li>多线程实现的并行避免了进程问并行的缺点：创建线程的开销比创建进程要小，同一进程的线程共享进程的地址空间，所以线程切挟（处理器调度）比进程切换快。</li>
</ul>
</li>
</ul>
<h2 id="PV操作"><a href="#PV操作" class="headerlink" title="PV操作"></a>PV操作</h2><p><a href="https://www.cnblogs.com/guanghe/p/15246894.html">ref</a></p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul>
<li>PV操作是一种实现进程互斥与同步的有效方法。PV操作与信号量的处理相关。P（passeren）通过，理解为<strong>申请资源</strong>，V（vrijgeven）释放，理解为<strong>释放资源</strong>。</li>
<li>P操作的主要动作是：<ul>
<li>①S减1；（申请资源，资源量减一）</li>
<li>②若S减1后仍大于或等于0，则进程继续执行；</li>
<li>③若S减1后小于0，则该进程被阻塞后放入等待该信号量的等待队列中，然后转进程调度。</li>
</ul>
</li>
<li>V操作的主要动作是：<ul>
<li>①S加1；（释放资源，资源量加一）</li>
<li>②若相加后结果大于0，则进程继续执行；</li>
<li>③若相加后结果小于或等于0，则从该信号的等待队列中释放一个等待进程，<strong>然后再返回原进程（调用v的进程）继续执行或转进程调度</strong>。</li>
</ul>
</li>
<li>PV操作对于每一个进程来说，都只能进行一次，而且必须成对使用（申请资源用P，释放资源用V）。在PV原语执行期间不允许有中断发生。原语不能被中断执行，因为原语对变量的操作过程如果被打断，可能会去运行另一个对同一变量的操作过程，从而出现临界段问题。如果能够找到一种解决临界段问题的元方法，就可以实现对共享变量操作的原子性。</li>
<li>相关推论<ul>
<li>推论1：若信号量S为正值，则该值等于在阻塞进程之前对信号量S可施行的P操作数，亦即等于S所代表的实际还可以使用的物理资源数。</li>
<li>推论2：若信号量s为负值，则其绝对值等于登记排列在该信号量S等待队列之中的进程个数，亦即恰好等于对信号量S实施P操作而被阻塞并进入信号量S等待队列的进程数。</li>
<li>推论3：通常，P操作意味着请求一个资源，V操作意味着释放一个资源。在一定条件下，P操作代表阻塞进程操作，而V操作代表唤醒被阻塞进程的操作。</li>
</ul>
</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><h4 id="进程的同步与互斥"><a href="#进程的同步与互斥" class="headerlink" title="进程的同步与互斥"></a>进程的同步与互斥</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 进程同步</span><br><span class="line">(1)调用P操作测试消息是否到达</span><br><span class="line">　　任何进程调用P操作可测试到自己所期望的消息是否已经到达。若消息尚未产生，则S&#x3D;0，调用P(s)后，P(S)一定让调用者成为等待信号量S的状态，即调用者此时必定等待直到消息到达；若消息已经存在，则S≠0，调用P(S)后，进程不会成为等待状态而可继续执行，即进程测试到自己期望的消息已经存在。</span><br><span class="line">(2)调用V操作发送消息</span><br><span class="line">　　任何进程要向其他进程发送消息时可调用V操作。若调用V操作之前S&#x3D;0，表示消息尚未产生且无等待消息的进程，则调用V(S)后，V(s)执行S:&#x3D;S+1使S≠0，即意味着消息已存在；若调用V操作之前S&lt;0，表示消息未产生前已有进程在等待消息，则调用V(S)后将释放一个等待消息者，即表示该进程等待的消息已经到达，可以继续执行。</span><br><span class="line">　　在用PV操作实现同步时，一定要根据具体的问题来定义信号量和调用P操作或V操作。一个信号量与一个消息联系在一起，当有多个消息时必须定义多个信号量；测试不同的消息是否到达或发送不同的消息时，应对不同的信号量调用P操作或V操作。</span><br><span class="line"></span><br><span class="line">2. 进程互斥</span><br><span class="line">(1)设立一个互斥信号量S，表示临界区，其取值为1，0，-1，…其中，S&#x3D;1表示无并发进程进入S临界区；S&#x3D;0表示已有一个并发进程进入了S临界区；S等于负数表示已有一个并发进程进入S临界区，且有|S|个进程等待进入S临界区，S的初值为1。</span><br><span class="line">(2)用PV操作表示对S临界区的申请和释放。在进入临界区之前，通过P操作进行申请，在退出临界区之后，通过V操作释放。</span><br></pre></td></tr></table></figure>
<h4 id="生产者与消费者"><a href="#生产者与消费者" class="headerlink" title="生产者与消费者"></a>生产者与消费者</h4><p><a href="https://www.cnblogs.com/guanghe/p/15246894.html">ref</a></p>
<h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><ul>
<li>产生原因：多个进程的循环等待</li>
<li>必要条件：互斥条件，不可抢占条件，保持与等待条件（部分分配条件）、循环等待条件<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- 互斥条件：一个资源每次只能被一个进程使用。</span><br><span class="line">- 保持与等待条件：有一个进程已获得了一些资源，但因请求其他资源被阻塞肘，对已获得的资源保持不放。</span><br><span class="line">- 不可抢占条件：有些系统资源是不可抢占的，当某个进程已获得这种资源后，系统不能强行收回，只能由进程使用完时自己释放。</span><br><span class="line">- 循环等待条件：若干个进程形成环形链，每个都占用对方要申请的下一个资源。</span><br></pre></td></tr></table></figure></li>
<li>破坏任一条件，死锁不成立</li>
<li>解决策略<img src="/2022/03/19/%E8%BD%AF%E8%80%83-%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90%E5%B8%88/deadlock1.png" class="" title="deadlock1">
</li>
</ul>
]]></content>
      <categories>
        <category>系统分析师</category>
      </categories>
      <tags>
        <tag>软考系统分析师</tag>
      </tags>
  </entry>
</search>
