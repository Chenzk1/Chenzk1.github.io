---
title: 优化器
mathjax: true
date: 2022-01-01 19:29:23
tags:
  - ML
  - DL
  - 优化器
categories:
  - Learning
---


# GD&SGD&Mini-batch GD
$$
\theta=\theta-\eta \cdot \nabla_{\theta} J(\theta)
$$
<!-- more -->

- GD
  - 用了全量样本，凸函数可以到全局最小值，非凸函数可以到局部最小值
  - 慢，不能在线serving
- SGD
  - 快
  - 损失函数震荡
- Mini-batch GD
  - GD与SGD的结合：降低了参数更新时的方差，收敛较SGD更稳定
  - 速度较快：可以利用矩阵运算

两个问题
- 收敛能力
  - lr太大，容易震荡，甚至偏离极小值；lr太小，收敛慢
  - 非凸函数，容易陷于局部最小值/鞍点
- 自适应lr调节
  - SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。
Mini-batch gradient descent 不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）
对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是训练集全集带入即BGD，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动。）

# Momentum
- 加上惯性，尝试解决容易陷入局部最小值/鞍点的问题
  - 使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡
$ v_{t-1} $上加梯度，可以认为是对 $ v_{t-1} $ 的预判

$$ \begin{array}{l}
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\
\theta=\theta-v_{t}
\end{array} $$
- 由Polyak提出，也叫Polyak动量
Nesterov
- 优化Polyak动量，“预判我的预判”
$$ \begin{array}{ll}
v_{t} & =\beta v_{t-1}+\nabla_{\theta} J\left(\theta_{t}-\eta \beta v_{t-1}\right) \\
\theta_{t+1} & =\theta_{t}-\eta v_{t} \end{array} $$
- 从工程实现上，无法直接获得预判点的梯度，可得如下等价公式：
$$ \begin{array}{ll}
v_{t} & =\beta v_{t-1}+g_{t}+\beta\left(g_{t}-g_{t-1}\right) \\
\theta_{t+1} & =\theta_{t}-\eta v_{t}
\end{array} $$

# AdaGrad&RMSProp&Adam
## AdaGrad
$$\Large \begin{cases} 
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
v_t = v_{t-1} + g_t^2 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{v_t + \epsilon}} g_t
\end{cases}$$

## RMSPropV2
$$\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
v_t = \beta v_{t-1} + g_t^2 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{v_t + 1}} g_t
\end{cases}$$

## Adam
$$\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\gamma = \frac{\sqrt{1-\beta_2^{t}}}{1-\beta_1^t} \\
\theta_{t+1} = \theta_{t} - \eta \frac{\gamma}{\sqrt{v_t +  \epsilon}} m_t
\end{cases}$$

- AdaGrad: Adaptive Gradient
  - 学习率的自适应调节：学习率 除以 梯度平方累积项，更新次数越多（高频特征）的更新步长减小，稀疏特征步长增大
  - 后期学不动
- RMSProp：Root Mean Square Propogation，为了解决AdaGrad后期学不动的问题
  - 累积梯度时加上衰减系数，越早的梯度会逐渐衰减
- Adam：Adaptive Moment Optimization, 
  - RMSProp + Momentum 

**以下内容未解锁**

<!-- # AdaMom: An optional optimizer for Dense Module
$$\Large
\begin{cases}
g_t = \nabla_{\theta}J(\theta_t) + \alpha\theta_t \\
m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t = \beta_2 v_{t-1} + g_t^2 \\
c_t = \beta_2 c_{t-1} + 1 \\
\theta_{t+1} = \theta_{t} - \eta \frac{1}{\sqrt{\frac{v_t}{c_t} +  \epsilon}} m_t
\end{cases}$$
- Adam的基础上，对二阶动量求累计更新次数的平均
- Momentum
 Adagrad仅使用二阶动量，导致累计梯度平方和单调增长，在某些维度变量频繁更新之后，会导致累计梯度平方和极大，参数几乎不再更新，也就是我们常说的学习不动了。
- 在AdaMom中，我们单纯对二阶动量做累计更新次数的均值，缓解上述学习不动的情况。同时继承Adam算法中引入一阶动量的优势。其次，为了更适应在线学习的场景，我们去除了计算二阶动量时在gradient平方上的decay。在这些调整的基础上，可以将学习率设置更小，对学习稳定性和快速收敛带来优势。同时我们观察到adamom训练过程产生的gradient远小于其他优化器，印证了对学习的稳定性和长期更新的判断。但目前没有进行数学论证。
- 和Adam的不同在于早期学习率的差距
AdaMom可以看成Adam的生产环境改进版，前期降低了m_t的方差/自带了warmup，有效缓解了Dense部分在异步更新场景初期可能出现的梯度爆炸

# AdaNest: an adaptive dense optimizer with Nesterov acceleration

- 公式
  - 1: Nesterov的工程实现
  - 2: 保证所有系数和为1，类似配分函数
  - 3: 继承于AdaMom，Vt除以梯度更新次数，借此解决AdaGrad的梯度衰减问题 
- 保留Adam/AdaMom二阶动量特性的同时，使用更加激进的Nesterov动量替换Polyak动量，提升了效果
- 从引入预判的角度看，AdaNest因为使用了预测点的梯度，对梯度的估计更加准确，总体比Adam更激进
- Adam与RMSProp的折衷，相比RMSProp引入了动量，相比Adam赋予$$g_t$$ 更大的权重


# GroupLasso
- FTRL的vector版本，为了增强vector的稀疏性，以此来减小PS内存
Summary -->


