---
title: CS224WLec3-Node Embeddings
mathjax: true
date: 2021-12-15 17:55:00
tags:
  - cs224w
  - Graph
  - DeepWalk
  - Node2vec
categories: 
  - Lecture
  - cs224w
---

# Basic
- goal: ä¸ºnodeç”Ÿæˆä¸€ä¸ªembedding
- ä¸¤ä¸ªè¦ç´ ï¼š**encoder/ç›¸ä¼¼åº¦è®¡é‡æ–¹æ³•ï¼ˆencodeå‰åéƒ½éœ€è¦ï¼‰**
- framework: encoder ç”Ÿæˆembedding --> ç›¸ä¼¼åº¦è®¡é‡æ–¹æ³•å†³å®šembeddingå­¦ä¹ çš„å¥½å
- **unsupervised/self-supervised way** based on random walks
- task independent

<!-- more -->
# Method
- goal: åŸå§‹graphä¸­ç›¸ä¼¼çš„nodeè·å¾—çš„embeddingsä¹Ÿæ˜¯ç›¸ä¼¼çš„
- ç±»ä¼¼äºword2vec: ç›®æ ‡æ˜¯æ±‚node uçš„embedding $$\mathbf{z}_{u}$$,è€Œæ¨¡å‹çš„é¢„æµ‹ç›®æ ‡æ˜¯ï¼š$$P\left(v \mid \mathbf{z}_{u}\right)$$ï¼Œå³node vå‡ºç°åœ¨ä»¥node uå¼€å§‹çš„walkä¸Šçš„æ¦‚ç‡ã€‚
- å¦‚ä½•è·å¾—â€œå¥å­â€ï¼šrandom walk
- èŒƒå¼: 
  - encoderç”Ÿæˆnode embeddingï¼Œæœ¬èŠ‚çš„encoderä¸ºword2vecä¸­çš„æƒé‡çŸ©é˜µ: $$ \operatorname{ENC}(v)=\mathbf{z}_{v} $$
  - decoderå°†node embeddingæ˜ å°„å›åŸç©ºé—´ï¼Œè¿™é‡Œå­˜åœ¨éšå¼çš„decoderï¼Œembeddingç©ºé—´ä¸¤å‘é‡çš„ç‚¹ç§¯å¯ä»¥è¡¨ç¤ºåŸç©ºé—´u,vçš„ç›¸ä¼¼åº¦: $$ \operatorname{similarity}(u, v) \approx \mathbf{z}_{v}^{\mathrm{T}} \mathbf{z}_{u} $$
    - ç‚¹å‡»ç›¸ä¼¼åº¦ï¼šæœ€å°åŒ–ä¸¤å‘é‡çš„æ¨¡ä»¥åŠå¤¹è§’ä½™å¼¦çš„ä¹˜ç§¯

## Deep Walk
### Random Walk
- å‡ºå‘ç‚¹ï¼šå¦‚æœä¸€ä¸ªrandom walkä¸­åŒ…æ‹¬ä»uåˆ°vçš„è·¯å¾„ï¼Œé‚£uå’Œvæ˜¯ç›¸ä¼¼çš„/æœ‰ç›¸ä¼¼çš„é«˜ç»´çš„å¤šè·³ä¿¡æ¯
- æœ¬è´¨ï¼šDFS
- $ N_{\mathrm{R}}(u) $ä¸ºç­–ç•¥Rä¸‹ï¼Œä»uå‡ºå‘çš„walkä¸­ï¼Œå‡ºç°çš„æ‰€æœ‰nodes
$$ \max _{f} \sum_{u \in V} \log \mathrm{P}\left(N_{\mathrm{R}}(u) \mid \mathbf{z}_{u}\right) $$
--ã€‹
$$ \mathcal{L}=\sum_{u \in V} \sum_{v \in N_{R}(u)}-\log \left(P\left(v \mid \mathbf{z}_{u}\right)\right) $$
- åˆ©ç”¨softmaxæ±‚p
$$ P\left(v \mid \mathbf{z}_{u}\right)=\frac{\exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)}{\sum_{n \in V} \exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n}\right)} $$
- problem: softmaxåˆ†æ¯ ä»¥åŠ æœ€å¤–å±‚éƒ½éœ€è¦|V|æ¬¡éå† --ã€‹$$ \mathrm{O}\left(|\mathrm{V}|^{2}\right) $$çš„å¤æ‚åº¦ --ã€‹**ä¼˜åŒ–**

### Negative Sampling
- ä½¿ç”¨æ‰€æœ‰æ ·æœ¬åšnormalization --> åªé‡‡æ ·kä¸ªè´Ÿæ ·æœ¬åšnormalization
$$ P\left(v \mid \mathbf{z}_{u}\right)=\frac{\exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)}{\sum_{n \in V } \exp \left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n}\right)} \approx \log \left(\sigma\left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{v}\right)\right)-\sum_{i=1}^{k} \log \left(\sigma\left(\mathbf{z}_{u}^{\mathrm{T}} \mathbf{z}_{n_{i}}\right)\right), n_{i} \sim P_{V} $$
- kçš„é€‰æ‹©ï¼š
  - kè¶Šå¤§ï¼Œæ¨¡å‹è¶Šé²æ£’
  - kè¶Šå¤§ï¼Œå¯¹è´Ÿæ ·æœ¬è€ƒè™‘çš„è¶Šå¤š
  - 5~20é—´è¾ƒå¸¸è§
- è´Ÿæ ·æœ¬çš„é€‰æ‹©ï¼šå¯ä»¥é€‰æ‹©graphå†…ä»»æ„æ ·æœ¬ï¼Œä½†æ›´å‡†ç¡®çš„æ–¹æ³•æ˜¯é€‰æ‹©ä¸åœ¨walkä¸­çš„æ ·æœ¬

## Node2vec: better  random walk strategy
- ç®€å•çš„random walkä¼šé™åˆ¶walkä¸­çš„nodeç›¸ä¼¼åº¦ä¸graphä¸­nodeç›¸ä¼¼åº¦çš„ä¸€è‡´æ€§

### Biased 2nd-order random Walks
- trade off between local and global views of the network: BFS & DFS
- å½“å‰åœ¨w, ä¸Šä¸€æ­¥åœ¨sçš„walkï¼Œæœ‰ä¸‰ç§è¡Œèµ°æ–¹å‘
  - é€€åï¼šå›é€€åˆ°s
  - ä¿æŒï¼šèµ°åˆ°å’Œsè·ç¦»ä¸€è‡´çš„ä¸€ä¸ªèŠ‚ç‚¹
  - å‰è¿›ï¼šèµ°åˆ°è·ç¦»sæ›´è¿œçš„èŠ‚ç‚¹
{% asset_img node2vec1.png node2vec1 %}
- å®ç°ï¼šä¸¤ä¸ª**è¶…å‚**p/qï¼Œä»¥åŠâ€œ1â€æ¥ä»¥éå½’ä¸€åŒ–çš„æ–¹æ³•è¡¨ç¤ºä¸Šè¿°ä¸‰ç§æƒ…å†µçš„æ¦‚ç‡
{% asset_img node2vec2.png node2vec2 %}
- æµç¨‹
  - Compute random walk probabilities
  - Simulate ğ‘Ÿ random walks of length ğ‘™ starting from each node ğ‘¢
  - Optimize the node2vec objective using Stochastic Gradient Descent
- Linear-time complexity
- All 3 steps are individually parallelizable

## Embedding entire graphs
- approach1: add all node embeddings
- approach2: introduce a "virtual node" or "super node" to represent the graph and learning embedding for this graph
- approach3: anonymous walks embeddings

### Anonymous walk embeddings
- Anonymous walk: random walk --> å°†nodeè¡¨ç¤ºä¸ºè·ç¦»start nodeçš„å»é‡indexã€‚å› æ­¤ï¼Œç¡®å®šäº†walk lengthçš„æ—¶å€™ï¼Œå°±ç¡®å®šäº†anonymous walkä¸­indexçš„ä¸ªæ•°ã€‚
- æ–¹æ³•1ï¼šé•¿åº¦ä¸ºlçš„annoy walkå…±æœ‰nç§æƒ…å†µ --> åšmæ¬¡random walks --> ç»Ÿè®¡æ¯ç§æƒ…å†µçš„countï¼Œå¹¶å½¢æˆä¸€ä¸ªvector
- æ–¹æ³•2ï¼šç”¨Anonymous walksçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå­¦ä¹ å›¾çš„embedding
  - Learn to predict walks that co-occur in ğš«-size window (e.g., predict ğ‘¤3 given ğ‘¤1, ğ‘¤2 if Î” = 2)
  - objective:
  $$ \max _{z_{G}} \sum_{t=\Delta+1}^{T} \log P\left(w_{t} \mid w_{t-\Delta}, \ldots, w_{t-1}, \mathbf{z}_{G}\right) $$

{% asset_img annoywalks1.png annoywalks1 %}
{% asset_img annoywalks2.png annoywalks2 %}

## Pros & Cons
- å±äºshallow encodingï¼Œæœ‰å¦‚ä¸‹ä¼˜ç¼ºç‚¹ï¼š
  - éœ€è¦O(|V|)çš„å‚æ•°é‡ï¼ŒèŠ‚ç‚¹é—´çš„embeddingä¸å…±äº«ï¼Œæ¯ä¸ªnodeæœ‰ç‹¬ç«‹çš„embedding
  - trainingæ—¶æ²¡æœ‰çš„nodeï¼Œä¸ä¼šæœ‰embedding
  - æ²¡æœ‰åˆ©ç”¨åˆ°èŠ‚ç‚¹çš„ç‰¹å¾ï¼Œåªåˆ©ç”¨äº†graph structure
  
# Reference
- [ppt](http://web.stanford.edu/class/cs224w/slides/03-nodeemb.pdf)
